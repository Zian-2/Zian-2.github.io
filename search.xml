<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CS336：Assignment1-basics</title>
    <url>/2026/01/21/Assignment1-basics/</url>
    <content><![CDATA[<h1 id="1-作业总览"><a href="#1-作业总览" class="headerlink" title="1 作业总览"></a>1 作业总览</h1><p>你将构建训练标准 Transformer 语言模型（LM）所需的所有组件，并训练一些模型。</p>
<h3 id="你将实现："><a href="#你将实现：" class="headerlink" title="你将实现："></a>你将实现：</h3><p>1 BPE 分词器 (Byte-pair encoding tokenizer)：第 2 节<br>2 Transformer 语言模型 (LM)：第 3 节<br>3 交叉熵损失函数与 AdamW 优化器：第 4 节<br>4 训练循环：支持模型和优化器状态的序列化与加载（保存与读取）：第 5 节  </p>
<h3 id="你将完成如下任务："><a href="#你将完成如下任务：" class="headerlink" title="你将完成如下任务："></a>你将完成如下任务：</h3><p>1 在 TinyStories 数据集上训练一个 BPE 分词器。<br>2 对数据集运行训练好的分词器，将其转换为整数 ID 序列。<br>3 在 TinyStories 数据集上训练 Transformer 语言模型。<br>4 使用训练好的模型生成样本并评估困惑度 (Perplexity)。<br>5 在 OpenWebText 数据集上训练模型，并将达到的困惑度结果提交到排行榜。  </p>
<h3 id="允许使用的工具："><a href="#允许使用的工具：" class="headerlink" title="允许使用的工具："></a>允许使用的工具：</h3><p>课程希望你从0开始搭组件，所以不得使用<code>torch.nn</code>、<code>torch.nn.functional</code> 或 <code>torch.optim</code> 中的任何定义，除了：<br><span id="more"></span><br>1 <code>torch.nn.Parameter</code><br>2 <code>torch.nn</code> 中的容器类（<code>Module</code>, <code>ModuleList</code>, <code>Sequential</code> 等）<br>3 <code>torch.optim.Optimizer</code> 基类   </p>
<h3 id="关于-AI-工具："><a href="#关于-AI-工具：" class="headerlink" title="关于 AI 工具："></a>关于 AI 工具：</h3><p>1 允许使用大语言模型进行低级的编程问题或提问高级的概念问题的咨询，但严禁直接使用 AI 来完成作业题目。<br>2 建议在完成作业时关闭 IDE 中的 AI 自动补全（如 Cursor Tab, GitHub CoPilot）。</p>
<h3 id="数据集获取："><a href="#数据集获取：" class="headerlink" title="数据集获取："></a>数据集获取：</h3><p>本次作业使用两个预处理过的数据集：TinyStories 和 OpenWebText。两者都是大型纯文本文件。<br>校内学生可在实验室机器的 <code>/data</code> 目录下找到。校外人员/自学者可根据<strong>README.md中的命令下载。</strong></p>
<h3 id="降低规模的建议："><a href="#降低规模的建议：" class="headerlink" title="降低规模的建议："></a>降低规模的建议：</h3><p>后续课程会提供建议，解释如何在 CPU 或 MPS 环境下进行调整。</p>
<h1 id="2-BPE-Byte-Pair-Encoding-分词器"><a href="#2-BPE-Byte-Pair-Encoding-分词器" class="headerlink" title="2 BPE(Byte-Pair Encoding)分词器"></a>2 BPE(Byte-Pair Encoding)分词器</h1><h2 id="2-1-unicode标准"><a href="#2-1-unicode标准" class="headerlink" title="2.1 unicode标准"></a>2.1 unicode标准</h2><p>Unicode标准将字符映射到整数code points上。例如”s” 的code point 是115（或写作十六进制的U+0073），”牛”对应29275。</p>
<p>python中，有ord()和chr()函数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">ord</span>(<span class="string">&#x27;牛&#x27;</span>)</span><br><span class="line"><span class="number">29275</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">chr</span>(<span class="number">29275</span>)</span><br><span class="line"><span class="string">&#x27;牛&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Problem-unicode1"><a href="#Problem-unicode1" class="headerlink" title="Problem (unicode1):"></a>Problem (unicode1):</h3><p>(a)<code>chr(0)</code> 返回的是什么字符？<br><code>chr(0)</code> 返回的是 Unicode 编码为 0 的字符，即空字符（Null Character）。</p>
<p>(b) 它的字符串表示形式 (<code>__repr__()</code>) 与打印形式有何不同？<br>在 Python 中，其字符串表示形式（<code>__repr__()</code>）会显示为转义序列 <code>&#39;\x00&#39;</code>，而打印该字符（<code>print()</code>）时通常是不可见的，在某些终端里可能显示为空格。</p>
<p>(c)当该字符出现在文本中时会发生什么？<br>在 Python 字符串内部它可以正常存在并拼接，但在将其打印到终端或与底层 C 语言编写的程序交互时，它可能会被当做文本结束符而导致后面的内容被截断，或者干脆显示为一个空白区域。</p>
<h2 id="2-2-unicode编码"><a href="#2-2-unicode编码" class="headerlink" title="2.2 unicode编码"></a>2.2 unicode编码</h2><p>主要介绍了UTF-8。编码和解码UTF-8的函数包括encode()和decode()，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_string = <span class="string">&quot;hello! こんにちは!&quot;</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>utf8_encoded = test_string.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded) </span><br><span class="line"><span class="string">b&#x27;hello! \xe3\x81\x93\xe3\x82\x93\xe3\x81\xab\xe3\x81\xa1\xe3\x81\xaf!&#x27;</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">type</span>(utf8_encoded))</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;bytes&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Get the byte values for the encoded string (integers from 0 to 255).</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(utf8_encoded) </span><br><span class="line">[<span class="number">104</span>, <span class="number">101</span>, <span class="number">108</span>, <span class="number">108</span>, <span class="number">111</span>, <span class="number">33</span>, <span class="number">32</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">130</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">171</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">161</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">175</span>, <span class="number">33</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># One byte does not necessarily correspond to one Unicode character! </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(test_string)) </span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(utf8_encoded))</span><br><span class="line"><span class="number">23</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded.decode(<span class="string">&quot;utf-8&quot;</span>)) <span class="number">1</span></span><br><span class="line">hello! こんにちは!</span><br></pre></td></tr></table></figure></p>
<p>我们将整数的codepoints转换成了一个byte序列，它们会更易于处理。例如，因为任何文本实质上都被转化为介于 $0-255$ 之间的整数序列，只要词表包含了这 256 个基础字节，就不需要担心训练和搭建的过程中词表外（OOV）词汇。</p>
<h3 id="课程文档外的补充：UTF-8字节序列对应规则"><a href="#课程文档外的补充：UTF-8字节序列对应规则" class="headerlink" title="课程文档外的补充：UTF-8字节序列对应规则"></a>课程文档外的补充：UTF-8字节序列对应规则</h3><p>字节数1：<br>对应Unicode编码范围：U+0000 至 U+007F 或 0~127 (7 bits)<br>对应utf-8字节序列：0xxxxxxx</p>
<p>字节数2：<br>对应Unicode编码范围：U+0080 至 U+07FF 或 128~2047 (11 bits)<br>对应utf-8字节序列：110xxxxx 10xxxxxx</p>
<p>字节数3：<br>对应Unicode编码范围：U+0800 至 U+FFFF 或 2048~65535 (16 bits)<br>对应utf-8字节序列：1110xxxx 10xxxxxx 10xxxxxx</p>
<p>字节数4：<br>对应Unicode编码范围：U+10000 至 U+10FFFF 或 65536~1114111 (21 bits)<br>应utf-8字节序列：11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</p>
<p>对应方式：先把unicode编码翻译成二进制，在填入对应字节序列的’x’处。（长度不够填则在左侧补0）。</p>
<h3 id="Problem-unicode2"><a href="#Problem-unicode2" class="headerlink" title="Problem (unicode2):"></a>Problem (unicode2):</h3><p>(a) 为什么 Tokenizer 训练更偏好 UTF-8而不是UTF-16, UTF-32？<br>UTF-8 是变长编码，能够将常用的 ASCII 字符保持为单字节，避免了 UTF-16/32 在处理英文或代码时产生大量冗余零字节。</p>
<p>(b) 错误的解码函数分析：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decode_utf8_bytes_to_str_wrong</span>(<span class="params">bytestring: <span class="built_in">bytes</span></span>): </span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([<span class="built_in">bytes</span>([b]).decode(<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">for</span> b <span class="keyword">in</span> bytestring]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>decode_utf8_bytes_to_str_wrong(<span class="string">&quot;hello&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)) <span class="string">&#x27;hello&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p>错误输入：如<code>b&#39;\xe4\xb8\xad&#39;</code> （汉字“中”的编码字节流）<br>因为UTF-8 中的中文字符或复杂符号是由 2 到 4 个字节共同组成的，单独解码其中任何一个字节都会因不符合 UTF-8 规范而报错（或产生乱码）。</p>
<p>(c)给出无法解码的字节：<br>如<code>b&#39;\xff\xff&#39;</code>。<br>原因：任何以 <code>11111xxx</code> 开头的字节都没有对应的 5 字节或更长的有效模板，无法解码为任何 Unicode 字符。</p>
<h2 id="2-3-子词分词方案（subword-tokenization）"><a href="#2-3-子词分词方案（subword-tokenization）" class="headerlink" title="2.3 子词分词方案（subword tokenization）"></a>2.3 子词分词方案（subword tokenization）</h2><p>将词语拆成byte序列之后，仍然不能逐个byte拆分来看作为token：这会导致序列变得极长，训练的step变长，计算量也增大（如transformer计算复杂度与序列长度成正比）；同时更长的序列也导致信息密度被稀释，网络寻找token间的关系变得更困难。</p>
<p>而subword tokenization就是指代这样一个折中的方案。它选择用一个更大的词汇表(vocabulary)来trade-off更短的序列。比如说，如果‘the’经常出现，我们就可以给它单独分配一个条目(entry)，词汇表维度+1，但把3个token缩短成了1个。</p>
<p>为了做这样的工作，Sennrich et al. (2016) 提出使用byte-pair encoding (BPE; Gage, 1994)。作为一种subword tokenization，它简单地基于出现频率，将经常出现的byte pair合并(merge)成一个未被使用的索引。基于BPE的subword tokenizer被称为BPE tokenizer。</p>
<h2 id="2-4-训练BPE分词器"><a href="#2-4-训练BPE分词器" class="headerlink" title="2.4 训练BPE分词器"></a>2.4 训练BPE分词器</h2><h3 id="1-初始化-vocabulary"><a href="#1-初始化-vocabulary" class="headerlink" title="1 初始化 vocabulary"></a>1 初始化 vocabulary</h3><p>初始化的vocabulary是从byte到整数ID的映射。因此，vocabulary的大小为256。</p>
<h3 id="2-预分词（Pre-tokenization"><a href="#2-预分词（Pre-tokenization" class="headerlink" title="2 预分词（Pre-tokenization)"></a>2 预分词（Pre-tokenization)</h3><p>理论上，有了词汇表我们就可以开始进行上述的合并（merge）工作了。然而，有两个关键的问题：<br>i 每次合并的时候，都需要从头到尾过一遍语料库。这在计算上是很昂贵的。<br>ii 直接合并会导致出现一些新的token，它们只有标点符号的区别（比如”dog.“和”dog!“），它们会拥有完全不同的ID，即使它们在语义上是完全相同的。</p>
<p>为了解决上述问题，我们进行对语料库的预分词(pre-tokenize)，先把语料库切成单词。这是如何省下计算成本的？举个例子，当我们已经计数了’text’的出现次数（比如说10次），当我们需要计数’te’的出现次数时，就可以直接+=10，从而避免了每次遇到同一个单词时重复的计算。</p>
<p>早期分词方案包括直接通过空格进行切分（s.split(“”))，但显然这不能解决上述提到的问题2。</p>
<p>于是，我们将使用的时一个基于正则表达式(regex)的分词器 (used by GPT-2; Radford et al., 2019)，可以在github.com/openai/tiktoken/pull/234/files中找到。如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>PAT = <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>这里的五个部分分别处理：缩写后缀（’ll, ‘ve, ‘t , ‘s等）；连续字母（单词）；连续数字；连续标点符号；连续纯空格。</p>
<p>例子如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># requires `regex` package </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> regex <span class="keyword">as</span> re </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> re.findall(PAT, <span class="string">&quot;some text that i&#x27;ll pre-tokenize&quot;</span>) </span><br><span class="line">[<span class="string">&#x27;some&#x27;</span>, <span class="string">&#x27; text&#x27;</span>, <span class="string">&#x27; that&#x27;</span>, <span class="string">&#x27; i&#x27;</span>, <span class="string">&quot;&#x27;ll&quot;</span>, <span class="string">&#x27; pre&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;tokenize&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<p>实际使用中，用findall内存容易溢出，建议使用re.finditer这一迭代计算的函数。</p>
<h3 id="3-计算BPE合并-amp-处理特殊token"><a href="#3-计算BPE合并-amp-处理特殊token" class="headerlink" title="3 计算BPE合并&amp;处理特殊token"></a>3 计算BPE合并&amp;处理特殊token</h3><p>现在我们可以开始按照前述方法计算合并了。只需要注意两点：<br>i  不能跨越预分词边界合并。<code>[&#39;some&#39;, &#39;text&#39;]</code>，那么 <code>e</code>（来自 some）和 <code>t</code>（来自 text）永远不会被统计在一起。<br>ii 多个byte pair出现频率并列第一时，选择字典序最大（Lexicographically greater）的那一对。(“A”, “B”), (“A”, “C”), (“B”, “ZZ”), 和 (“BA”, “A”)频率相同时，在 ASCII 中 “BA” &gt; “B” &gt; “A”），因此 <code>max</code> 会选出 <code>(&#39;BA&#39;, &#39;A&#39;)</code>。<br>iii 有一些特殊token不能和其他合并，比如&lt;|endoftext|&gt;。它不应该被分成几个零碎的token，因此我们会给它安排一个固定的tokenID。</p>
<h3 id="一个训练的具体例子："><a href="#一个训练的具体例子：" class="headerlink" title="一个训练的具体例子："></a>一个训练的具体例子：</h3><p>例如我们现在有如下corpus：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">low low low low </span><br><span class="line">low lower lower widest widest widest </span><br><span class="line">newest newest newest newest newest newest</span><br></pre></td></tr></table></figure><br>, 且vocabulary里已经放好了&lt;|endoftext|&gt;这一special token。</p>
<h4 id="1-初始化vocabulary："><a href="#1-初始化vocabulary：" class="headerlink" title="1 初始化vocabulary："></a>1 初始化vocabulary：</h4><p>一个special token和256个byte value。</p>
<h4 id="2-Pre-tokenization"><a href="#2-Pre-tokenization" class="headerlink" title="2 Pre-tokenization:"></a>2 Pre-tokenization:</h4><p>为了简化，我们仅使用用空格分隔，最终得到频率表：{low: 5, lower: 2, widest: 3, newest: 6}。为容易处理，将它写成dict[tuple[bytes], int]的格式，如{(l,o,w): 5 …}。</p>
<h4 id="3-合并："><a href="#3-合并：" class="headerlink" title="3 合并："></a>3 合并：</h4><p>数出byte pair的出现频率：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;lo: <span class="number">7</span>, ow: <span class="number">7</span>, we: <span class="number">8</span>, er: <span class="number">2</span>, wi: <span class="number">3</span>, <span class="built_in">id</span>: <span class="number">3</span>, de: <span class="number">3</span>, es: <span class="number">9</span>, st: <span class="number">9</span>, ne: <span class="number">6</span>, ew: <span class="number">6</span>&#125;</span><br></pre></td></tr></table></figure><br>(‘es’) 和 (‘st’)并列，我们就取字典序更大的(‘st’)。<br>于是表格变为：{(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}.</p>
<p>如是循环，进行6次merge，可以得到新产生的vocabulary：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&#x27;s t&#x27;</span>, <span class="string">&#x27;e st&#x27;</span>, <span class="string">&#x27;o w&#x27;</span>, <span class="string">&#x27;l ow&#x27;</span>, <span class="string">&#x27;w est&#x27;</span>, <span class="string">&#x27;n e&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<p>于是新的词汇表变为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[&lt;|endoftext|&gt;, [..<span class="number">.256</span> BYTE CHARS], st, est, ow, low, west, ne]</span><br></pre></td></tr></table></figure></p>
<p>在此语境下，‘newest’ 将被分词为[ne, west]。</p>
<h2 id="2-5-实验：训练BPE"><a href="#2-5-实验：训练BPE" class="headerlink" title="2.5 实验：训练BPE"></a>2.5 实验：训练BPE</h2><p>我们接下来再TinyStories数据集上训练一个BPE。你可以在Section1里找到它的下载方式。在开始之前，推荐你先大概看一眼里面都是什么内容。</p>
<h4 id="1"><a href="#1" class="headerlink" title="1"></a>1</h4>]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>矩阵理论</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵求导理论：几种表示</title>
    <url>/2026/01/21/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</url>
    <content><![CDATA[<p><strong>提示：如果latex矩阵的渲染有问题，刷新往往能解决问题</strong><br><br></p>
<p>也许我们可以说，张量就是矩阵，梯度就是导数，张量的梯度本质上就是矩阵求导的ml小黑话说法。然而由于一些众所周知的原因，很多人甚至在学习线性代数之前就已经走进了深度学习，初次面对高维张量的求导往往感到混乱。而市面上如The Matrix Cookbook等的工具书又往往太细节，翻译也不尽如人意。本文将以最基础的多元 Logistic 回归（Multinomial Logistic Regression）为例，从大方向上探讨三种不同的推导范式。</p>
<h3 id="例子背景"><a href="#例子背景" class="headerlink" title="例子背景"></a>例子背景</h3><p>我们设定一个典型的分类场景：</p>
<ul>
<li><p><strong>输入向量</strong>：$\mathbf{x} \in \mathbb{R}^{n \times 1}$。</p>
</li>
<li><p><strong>权重矩阵</strong>：$W \in \mathbb{R}^{m \times n}$。</p>
</li>
<li><p><strong>线性输出</strong>：$\mathbf{a} = W\mathbf{x} \in \mathbb{R}^{m \times 1}$。</p>
</li>
<li><p><strong>预测概率</strong>：$\mathbf{p} = \text{softmax}(\mathbf{a}) \in \mathbb{R}^{m \times 1}$，其中第 $i$ 个分量 $p_i = \frac{\exp(a_i)}{\sum_{j=1}^m \exp(a_j)}$。</p>
</li>
<li><p><strong>目标标签</strong>：$\mathbf{y} \in \mathbb{R}^{m \times 1}$，是一个 One-hot 向量（仅有一个元素为 1，其余为 0）。</p>
</li>
<li><p><strong>损失函数</strong>：标量 $l = -\mathbf{y}^\top \log \mathbf{p} = -\sum_{k=1}^m y_k \ln p_k$。</p>
</li>
</ul>
<p>我们的目标是求出梯度矩阵 $\nabla_W l$（即 $\frac{\partial l}{\partial W}$）。</p>
<span id="more"></span>
<p><br> <br> </p>
<h3 id="方法一：逐项求导"><a href="#方法一：逐项求导" class="headerlink" title="方法一：逐项求导"></a>方法一：逐项求导</h3><p>当然了，只要你会求偏导，肯定就会对世界上所有的矩阵求导，只要确定了定义，就可以对应地求导，因为矩阵本质上是标量的排列。我们通过观察单个权重 $W_{ij}$ 的变动如何影响最终损失 $l$，最后再将规律汇总回矩阵形式。</p>
<h4 id="1-链式法则"><a href="#1-链式法则" class="headerlink" title="1. 链式法则"></a>1. 链式法则</h4><p>我们的目标是求 $l$ 对 $W_{ij}$（矩阵 $W$ 第 $i$ 行第 $j$ 列的元素）的偏导数。</p>
<p>根据函数依赖关系：$W_{ij}$ 的变动首先影响线性输出 $a_i$，由于 $a_i$ 是 Softmax 函数的输入，它会进一步影响<strong>所有的</strong>预测概率 $p_k (k=1, \dots, m)$，最后导致损失 $l$ 的变化。</p>
<p>链式法则展开如下：</p>

$$\frac{\partial l}{\partial W_{ij}} = \sum_{k=1}^m \frac{\partial l}{\partial p_k} \cdot \frac{\partial p_k}{\partial a_i} \cdot \frac{\partial a_i}{\partial W_{ij}}$$
<h4 id="2-第一阶段：基础项求导"><a href="#2-第一阶段：基础项求导" class="headerlink" title="2. 第一阶段：基础项求导"></a>2. 第一阶段：基础项求导</h4><ul>
<li><p><strong>计算 $\frac{\partial a_i}{\partial W_{ij}}$</strong>：</p>
<p>  已知 $a_i = \sum_{t=1}^n W_{it}x_t = W_{i1}x_1 + \dots + W_{ij}x_j + \dots$。</p>
<p>  对 $W_{ij}$ 求导得：$\frac{\partial a_i}{\partial W_{ij}} = x_j$。</p>
</li>
<li><p><strong>计算 $\frac{\partial l}{\partial p_k}$</strong>：</p>
<p>  已知 $l = -\sum_{k=1}^m y_k \ln p_k$。</p>
<p>  对 $p_k$ 求导得：$\frac{\partial l}{\partial p_k} = -\frac{y_k}{p_k}$。</p>
</li>
</ul>
<h4 id="3-第二阶段：Softmax-梯度的分类讨论"><a href="#3-第二阶段：Softmax-梯度的分类讨论" class="headerlink" title="3. 第二阶段：Softmax 梯度的分类讨论"></a>3. 第二阶段：Softmax 梯度的分类讨论</h4><p>我们需要计算 $\frac{\partial p_k}{\partial a_i}$。由于 $p_k = \frac{e^{a_k}}{\sum_{j=1}^m e^{a_j}}$，分母包含所有 $a$，因此需要分两种情况：</p>
<ul>
<li><p><strong>情况 A：当 $k = i$ 时（输出下标等于求导下标）</strong></p>
<p>  利用商的求导法则：</p>

$$\frac{\partial p_i}{\partial a_i} = \frac{e^{a_i}(\sum e^a) - e^{a_i}(e^{a_i})}{(\sum e^a)^2} = \frac{e^{a_i}}{\sum e^a} - \left(\frac{e^{a_i}}{\sum e^a}\right)^2 = p_i(1 - p_i)$$
</li>
<li><p><strong>情况 B：当 $k \neq i$ 时（输出下标不等于求导下标）</strong></p>

$$\frac{\partial p_k}{\partial a_i} = \frac{0 \cdot (\sum e^a) - e^{a_k}(e^{a_i})}{(\sum e^a)^2} = -\frac{e^{a_k}}{\sum e^a} \cdot \frac{e^{a_i}}{\sum e^a} = -p_k p_i$$
</li>
</ul>
<h4 id="4-第三阶段：组合求和"><a href="#4-第三阶段：组合求和" class="headerlink" title="4. 第三阶段：组合求和"></a>4. 第三阶段：组合求和</h4><p>将上述结果代入第一步的链式法则总式中，并将求和号按照 $k=i$ 和 $k \neq i$ 进行拆解：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} \frac{\partial l}{\partial p_k} \frac{\partial p_k}{\partial a_i} + \frac{\partial l}{\partial p_i} \frac{\partial p_i}{\partial a_i} \right) \cdot \frac{\partial a_i}{\partial W_{ij}}$$
<p>代入具体的偏导表达式：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} \left(-\frac{y_k}{p_k}\right)(-p_k p_i) + \left(-\frac{y_i}{p_i}\right)p_i(1-p_i) \right) \cdot x_j$$
<p>简化括号内的各项：</p>
<ul>
<li><p>第一部分：$\sum_{k \neq i} y_k p_i$</p>
</li>
<li><p>第二部分：$-y_i(1 - p_i) = -y_i + y_i p_i$</p>
</li>
</ul>
<p>组合后得到：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} y_k p_i + y_i p_i - y_i \right) x_j$$

$$\frac{\partial l}{\partial W_{ij}} = \left( p_i \left( \sum_{k=1}^m y_k \right) - y_i \right) x_j$$
<p>由于标签向量 $\mathbf{y}$ 是 One-hot 的，满足 $\sum_{k=1}^m y_k = 1$，上式最终简化为：</p>

$$\frac{\partial l}{\partial W_{ij}} = (p_i - y_i)x_j$$
<h4 id="5-还原为矩阵形式"><a href="#5-还原为矩阵形式" class="headerlink" title="5. 还原为矩阵形式"></a>5. 还原为矩阵形式</h4><p>我们求出了梯度矩阵中每一个元素 $(i, j)$ 的值。观察发现，这正好对应向量 $(\mathbf{p}-\mathbf{y})$ 的第 $i$ 个元素与向量 $\mathbf{x}$ 的第 $j$ 个元素的乘积。</p>
<p>于是结果为：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
<p><br> <br> <br></p>
<h3 id="方法二：利用全微分"><a href="#方法二：利用全微分" class="headerlink" title="方法二：利用全微分"></a>方法二：利用全微分</h3><p>你都用矩阵了，为什么还要逐项求导呢？矩阵就是矩阵，矩阵是不能不和标量切割的。下面我们将会看到，将矩阵整体不加拆分地求导有一些很美好的性质。实践上，整体的矩阵求导也使我们更少地受维度匹配问题的侵扰，理解上也更为简洁与直觉。</p>
<h4 id="1-整体流程"><a href="#1-整体流程" class="headerlink" title="1. 整体流程"></a>1. 整体流程</h4><p>在标量微积分中，我们知道 $df = f’(x)dx$。全微分法将这一思想推广到矩阵：对于一个以矩阵 $X$ 为自变量的标量函数 $f(X)$，其全微分一定可以表示为如下标准形式：</p>

$$df = \sum_{i,j} \frac{\partial f}{\partial X_{ij}} dX_{ij}$$
<p>为了避免陷入下标求和的泥潭，我们利用迹函数（Trace）。两个同维矩阵 $A$ 和 $B$ 的内积（对应元素相乘再求和）可以写成 $\text{Tr}(A^\top B)$是常用而易得的结论。因此，上述微分式可以写成：</p>

$$df = \text{Tr}(G^\top dX)$$
<p>这里， $G$ 就是梯度矩阵 $\frac{\partial f}{\partial X}$； dX的定义你可以显然地看出。</p>
<p><strong>全微分法的标准操作流程如下：</strong></p>
<ol>
<li><p><strong>写出微分式</strong>：利用微分运算法则，对目标标量 $l$ 直接求微分 $dl$。</p>
</li>
<li><p><strong>套入迹函数</strong>：将 $dl$ 放入迹中，即 $dl = \text{Tr}(dl)$（因为标量的迹等于自身）。</p>
</li>
<li><p><strong>应用Trace Trick</strong>：利用迹函数的循环复用性 $\text{Tr}(ABC) = \text{Tr}(BCA)$，通过左右挪动，将所有的 $dX$（或 $dW$）变动项孤立到式子的最右端。</p>
</li>
<li><p><strong>读取梯度</strong>：将式子整理成 $dl = \text{Tr}(G^\top dW)$ 的标准形态，直接读出 $G$。</p>
</li>
</ol>
<h4 id="2-矩阵微分的运算法则与Trace-trick"><a href="#2-矩阵微分的运算法则与Trace-trick" class="headerlink" title="2. 矩阵微分的运算法则与Trace trick"></a>2. 矩阵微分的运算法则与Trace trick</h4><p>以下的法则需要掌握。当然了，你也可以自己推。</p>
<p>证明基本上可以在张贤达《矩阵分析与应用》中找到。</p>
<ul>
<li><p><strong>微分法则（与标量高度相似）</strong>：</p>
<ul>
<li><p>$d(X + Y) = dX + dY$</p>
</li>
<li><p>$d(XY) = (dX)Y + X(dY)$ （注意保持矩阵先后顺序，不可交换）</p>
</li>
<li><p>$d(X^\top) = (dX)^\top$</p>
</li>
<li><p>$d(\text{exp}(X)) = \text{exp}(X) \odot dX$ （$\odot$ 为逐元素乘积）</p>
</li>
</ul>
</li>
<li><p><strong>Trace的性质</strong>：</p>
<ul>
<li><p><strong>循环移位</strong>：$\text{Tr}(ABC) = \text{Tr}(BCA) = \text{Tr}(CAB)$ </p>
</li>
<li><p><strong>转置不变</strong>：$\text{Tr}(A) = \text{Tr}(A^\top)$。</p>
</li>
<li><p><strong>线性</strong>：$d\text{Tr}(X) = \text{Tr}(dX)$。</p>
</li>
<li><p><strong>矩阵乘法/逐元素乘法交换</strong>：$\text{tr}(A^T(B\odot C)) = \text{tr}((A\odot B)^TC)$，其中A, B, C尺寸相同。两侧都等于$\sum_{i,j}A_{ij}B_{ij}C_{ij}$。</p>
</li>
</ul>
</li>
</ul>
<h4 id="3-多元-Logistic-的梯度"><a href="#3-多元-Logistic-的梯度" class="headerlink" title="3. 多元 Logistic 的梯度"></a>3. 多元 Logistic 的梯度</h4><p>现在，我们可以很容易地利用上述结论和流程解决多元logistic。</p>
<p><strong>第一步：化简目标函数</strong></p>
<p>为了便于求导，利用对数性质将 $l$ 展开：</p>

$$l = -\mathbf{y}^\top \log \text{softmax}(\mathbf{a}) = -\mathbf{y}^\top (\mathbf{a} - \mathbf{1} \ln(\mathbf{1}^\top \exp(\mathbf{a})))$$
<p>这里 $\mathbf{1}^\top \exp(\mathbf{a})$ 实际上就是 Softmax 的分母部分 $\sum e^{a_j}$。</p>
<p><strong>第二步：对 $l$ 求微分 $dl$</strong></p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \mathbf{y}^\top \mathbf{1} \cdot d\ln(\mathbf{1}^\top \exp(\mathbf{a}))$$
<p>由于 $\mathbf{y}$ 是 One-hot 向量，$\mathbf{y}^\top \mathbf{1} = 1$。再对对数项求微分：</p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \frac{1}{\mathbf{1}^\top \exp(\mathbf{a})} d(\mathbf{1}^\top \exp(\mathbf{a}))$$

$$dl = -\mathbf{y}^\top d\mathbf{a} + \frac{1}{\mathbf{1}^\top \exp(\mathbf{a})} \mathbf{1}^\top (\exp(\mathbf{a}) \odot d\mathbf{a})$$
<p>观察发现，$\frac{\exp(\mathbf{a})}{\mathbf{1}^\top \exp(\mathbf{a})}$ 正好是预测概率向量 $\mathbf{p}$。根据向量内积性质，$(\mathbf{p} \odot d\mathbf{a})$ 的各项求和等于 $\mathbf{p}^\top d\mathbf{a}$：</p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \mathbf{p}^\top d\mathbf{a} = (\mathbf{p} - \mathbf{y})^\top d\mathbf{a}$$
<p><strong>第三步：代入变量关系锁定 $dW$</strong></p>
<p>已知 $\mathbf{a} = W\mathbf{x}$，其中 $\mathbf{x}$ 是常数输入，则 $d\mathbf{a} = (dW)\mathbf{x}$。</p>
<p>代入上式：</p>

$$dl = (\mathbf{p} - \mathbf{y})^\top (dW) \mathbf{x}$$
<p><strong>第四步：利用迹函数提取梯度</strong></p>
<p>为了读出梯度，我们将标量 $dl$ 写成迹的形式，并利用 $\text{Tr}(ABC) = \text{Tr}(BCA)$ 变换位置：</p>

$$dl = \text{Tr}( (\mathbf{p} - \mathbf{y})^\top dW \mathbf{x} )$$
<p>将末尾的 $\mathbf{x}$ 移到最前面：</p>

$$dl = \text{Tr}( \mathbf{x} (\mathbf{p} - \mathbf{y})^\top dW )$$
<p>为了匹配标准形式 $dl = \text{Tr}(G^\top dW)$，我们需要把 $dW$ 左边的矩阵整体转置：</p>

$$dl = \text{Tr}( ((\mathbf{p} - \mathbf{y})\mathbf{x}^\top)^\top dW )$$
<p>于是：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
<p><br> <br> <br></p>
<h3 id="三：那么什么是Jacobian-矩阵？"><a href="#三：那么什么是Jacobian-矩阵？" class="headerlink" title="三：那么什么是Jacobian 矩阵？"></a>三：那么什么是Jacobian 矩阵？</h3><p>一般地，数学上利用全微分法可以优雅地解决问题。然而计算机并不擅长处理那些trace trick。所以在 PyTorch （TensorFlow大概也是一样）自动反向传播的底层，实际上，使用的是 Jacobian（雅可比）矩阵。阅读完之后你会发现，这其实并不是一种新的求导方法，只是通过向量化的方法将所有复杂的高维张量通通化成一维，从而形成某种理解维度上的方便。</p>
<h4 id="1-向量化"><a href="#1-向量化" class="headerlink" title="1. 向量化"></a>1. 向量化</h4><p>对于一个 $m \times n$ 的矩阵 $W$，第一步是将其元素按顺序排列成列向量。</p>
<ul>
<li><strong>向量化（Vectorization）</strong>：记作 $\text{vec}(W)$。如果你有一个 $2 \times 2$ 的矩阵，$\text{vec}(W)$ 就是 $4 \times 1$ 向量。</li>
</ul>
<h4 id="2-对Kronecker-积的利用（张量积）"><a href="#2-对Kronecker-积的利用（张量积）" class="headerlink" title="2. 对Kronecker 积的利用（张量积）"></a>2. 对Kronecker 积的利用（张量积）</h4><p>当我们要处理类似 $\mathbf{a} = W\mathbf{x}$ 这样的矩阵乘法求导时，数学上有：</p>

$$\text{vec}(AXB) = (B^\top \otimes A)\text{vec}(X)$$
<p>以$\mathbf{a} = W\mathbf{x}$ 为例。为了套用公式，我们补上单位阵：$\mathbf{a} = I_m W \mathbf{x}$。</p>
<p>利用上面的公式，它等价于：</p>

$$\mathbf{a} = (\mathbf{x}^\top \otimes I_m) \text{vec}(W)$$
<p>这意味着，$\mathbf{a}$ 是由 $\text{vec}(W)$ 经过一个系数矩阵 $(\mathbf{x}^\top \otimes I_m)$ 线性变换得到的。</p>
<h4 id="3-逐步推导"><a href="#3-逐步推导" class="headerlink" title="3. 逐步推导"></a>3. 逐步推导</h4><p>根据向量化的链式法则：</p>

$$\frac{\partial l}{\partial \text{vec}(W)} = \underbrace{\frac{\partial l}{\partial \mathbf{p}}}_{1 \times m} \cdot \underbrace{\frac{\partial \mathbf{p}}{\partial \mathbf{a}}}_{m \times m} \cdot \underbrace{\frac{\partial \mathbf{a}}{\partial \text{vec}(W)}}_{m \times mn}$$
<p><strong>第一步：求 $\frac{\partial l}{\partial \mathbf{p}}$</strong></p>
<p>损失函数 $l$ 对预测概率向量 $\mathbf{p}$ 的偏导，根据导数的定义，是一个行向量。这个行向量就是 $l$ 关于 $\mathbf{p}$ 的 Jacobian：</p>

$$J_{l,p} = \left[ -\frac{y_1}{p_1}, -\frac{y_2}{p_2}, \dots, -\frac{y_m}{p_m} \right]$$
<p><strong>第二步：求 $\frac{\partial \mathbf{p}}{\partial \mathbf{a}}$（Softmax 的雅可比矩阵）</strong></p>
<p>这里a和p都是向量，所以直接用方法一中的结论。</p>
<p>结果是一个 $m \times m$ 的矩阵，其形式为 $\text{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top$。即：</p>

$$J_{p,a} = \begin{bmatrix} p_1(1-p_1) & -p_1 p_2 & \dots & -p_1 p_m \\ -p_2 p_1 & p_2(1-p_2) & \dots & -p_2 p_m \\ \vdots & \vdots & \ddots & \vdots \\ -p_m p_1 & -p_m p_2 & \dots & p_m(1-p_m) \end{bmatrix}$$
<p><strong>第三步：求 $\frac{\partial \mathbf{a}}{\partial \text{vec}(W)}$</strong></p>
<p>根据在第 2 点提到的线性转换关系，这一项直接就是那个系数矩阵：</p>

$$\mathbf{x}^\top \otimes I_m$$
<p>即：</p>

$$J_{a,W} = \frac{\partial \mathbf{a}}{\partial \text{vec}(W)} = \mathbf{x}^\top \otimes I_m$$
<p>展开来看，就是 $n$ 个 $m \times m$ 的块：</p>

$$J_{a,W} = \begin{bmatrix} x_1 I_m & x_2 I_m & \dots & x_n I_m \end{bmatrix}$$
<h4 id="4-组合"><a href="#4-组合" class="headerlink" title="4. 组合"></a>4. 组合</h4><p>我们将这几项乘起来。</p>

$$\frac{\partial l}{\partial \text{vec}(W)} = (\mathbf{p} - \mathbf{y})^\top (\mathbf{x}^\top \otimes I_m)$$
<p>根据矩阵运算规则，这个结果实际上就是：</p>

$$\text{vec}((\mathbf{p} - \mathbf{y})\mathbf{x}^\top)^\top$$
<p>这是一个 $1 \times mn$ 的行向量。</p>
<h4 id="5-还原形状（逆向量化）"><a href="#5-还原形状（逆向量化）" class="headerlink" title="5. 还原形状（逆向量化）"></a>5. 还原形状（逆向量化）</h4><p>最后一步，我们将这个 $1 \times mn$ 的长向量重新折叠回 $m \times n$ 的矩阵形状。</p>
<p>即：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>矩阵理论</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
