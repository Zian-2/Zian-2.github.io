<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>tokenizer.py</title>
    <url>/2026/01/27/tokenizer.py/</url>
    <content><![CDATA[<p>本文包含对Problem (train_bpe): BPE Tokenizer Training一问的解答及说明。</p>
<p>完整代码见<a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">from</span> .pretokenization_example <span class="keyword">import</span> find_chunk_boundaries</span><br></pre></td></tr></table></figure>
<p>Counter : 一种特殊的数据类型，本质上是字典，下面你会看到它在计数层面上会有一些方便。<br>regex：引入正则表达式<br>multiprocessing: 用于并行处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预编译正则表达式</span></span><br><span class="line">PAT = re.<span class="built_in">compile</span>(<span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pre_tokenization</span>(<span class="params">chunk,special_tokens</span>)-&gt;<span class="built_in">list</span>[<span class="built_in">str</span>]:</span><br><span class="line">        <span class="comment"># 预分词：先从special_token切开，再过一遍正则切成单词</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="comment"># 切割special_tokens</span></span><br><span class="line">        special_pat = <span class="string">&quot;(&quot;</span> + <span class="string">&quot;|&quot;</span>.join(re.escape(t) <span class="keyword">for</span> t <span class="keyword">in</span> special_tokens) + <span class="string">&quot;)&quot;</span></span><br><span class="line">        parts = re.split(special_pat, chunk)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> part <span class="keyword">in</span> parts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> part:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> part <span class="keyword">in</span> special_tokens:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 预分词</span></span><br><span class="line">            matches = PAT.finditer(part)</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> matches:</span><br><span class="line">                result.append(<span class="string">f&quot;<span class="subst">&#123;m.group()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>五个部分分别处理：缩写后缀（’ll, ‘ve, ‘t , ‘s等）；连续字母（单词）；连续数字；连续标点符号；连续纯空格。<br>正则表达式的编译是比较昂贵的，放在外面从而不用每次都编译一遍。<br><span id="more"></span><br>pre_tokenization：输入分块了的chunk(需要先在下方process_chunk中转成str格式，不能是bytes流)； 输出分好词的列表，如[‘some’, ‘ text’, ‘ that’, ‘ i’, “‘ll”, ‘ pre’, ‘-‘, ‘tokenize’]。</p>
<p>special_pat一行：用于切开从special_token，将”textA special_token textB” 切成”textA“，”special_token“，”textB”。</p>
<p>matches = PAT.finditer()的使用：matches是迭代器，需要用m.group来调用正则出来的单词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_chunk</span>(<span class="params">input_path, </span></span><br><span class="line"><span class="params">                    start,</span></span><br><span class="line"><span class="params">                    end,</span></span><br><span class="line"><span class="params">                    special_tokens</span>)-&gt; <span class="built_in">dict</span>[<span class="built_in">tuple</span>[<span class="built_in">int</span>, ...], <span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 每个核的并行任务：(预分词)，encode，然后计数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(input_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.seek(start)</span><br><span class="line"></span><br><span class="line">            raw_data = f.read(end - start).replace(<span class="string">b&quot;\r\n&quot;</span>, <span class="string">b&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">            chunk = raw_data.decode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 预分词</span></span><br><span class="line">            chunk_split = pre_tokenization(chunk, special_tokens)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># encode每个单词并计数</span></span><br><span class="line">            word_counts = Counter()</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> chunk_split:</span><br><span class="line">                word_b = <span class="built_in">tuple</span>(word.encode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;ignore&quot;</span>))</span><br><span class="line">                word_counts[word_b] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">dict</span>(word_counts) </span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<p>process_chunk： 1 输入文本中任意的start和end位置，2 为了避免windows自动将\n更改为\r\n的问题，替换\r\n  3 将其解码为str  4 调用前面的预分词(pre_tokenization0)切成单词  5 将单词编码回bytes流  6 对单词计数。7 返回一个包含了单词计数的Counter类型。注意这里的单词都是bytes形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BPEtokenizer</span>:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    输入input_path(包含training data), vocab_size(int), special_tokens(list[str]).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回vocab(dict[int, bytes], int为原ID, bytes为token bytes), </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    和merges(list[tuple[bytes, bytes]],记录所有被合并的bytes). </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab=<span class="literal">None</span>, merges=<span class="literal">None</span>, special_tokens=<span class="literal">None</span>, pair_counts = Counter(<span class="params"></span>)</span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab = vocab <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> &#123;i: <span class="built_in">bytes</span>([i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>)&#125;</span><br><span class="line">        <span class="variable language_">self</span>.merges = merges <span class="keyword">if</span> merges <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> []</span><br><span class="line">        <span class="variable language_">self</span>.special_tokens = special_tokens <span class="keyword">if</span> special_tokens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> []</span><br><span class="line">        <span class="variable language_">self</span>.pair_counts = Counter()</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>开始计数，并开始创建题目要求的vocab, merges。为此创建一个类并初始化参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">counting_init</span>(<span class="params">self, input_path, </span></span><br><span class="line"><span class="params">          special_tokens, num_chunks = <span class="number">1000</span></span></span><br><span class="line"><span class="params">          </span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">tuple</span>[<span class="built_in">int</span>, ...], <span class="built_in">int</span>]:</span><br><span class="line">    <span class="comment"># 初始化：第一次计数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按边界切割</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(input_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        boundaries = find_chunk_boundaries(</span><br><span class="line">            f, </span><br><span class="line">            desired_num_chunks = num_chunks, </span><br><span class="line">            split_special_token = special_tokens[<span class="number">0</span>].encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        )        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 并行处理</span></span><br><span class="line">    num_cpu = multiprocessing.cpu_count()</span><br><span class="line">    tasks = []</span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> <span class="built_in">zip</span>(boundaries[:-<span class="number">1</span>], boundaries[<span class="number">1</span>:]):</span><br><span class="line">        tasks.append((input_path, start, end,special_tokens))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> multiprocessing.Pool(processes = num_cpu) <span class="keyword">as</span> pool:</span><br><span class="line">        results = pool.starmap(process_chunk, tasks)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成dict[bytes:int]</span></span><br><span class="line">    final_counts = Counter()</span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> results:</span><br><span class="line">        final_counts.update(res)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(final_counts)</span><br></pre></td></tr></table></figure>
<p>self.counting_init: 输入input_path, special_tokens, num_chunks（为了并行处理将原文本切成的大致块数），利用之前的process_chunk进行单词计数，然后合并所有结果。过程包括：</p>
<p>1 切割：按照课程提供的find_chunk_boundaries及其接口切割原文本。详情见<a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py">https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py</a>  。</p>
<p>2 并行：利用multiprocessing , 主要函数：<br>i multiprocessing.cpu_count()，输出你的cpu的核数。<br>ii  with multiprocessing.Pool(processes = num_cpu) as pool  :  创建并行池，processes = 并行个数。<br>iii  results = pool.starmap(process_chunk, tasks)： 第一个形参是你要并行使用的函数， 第二个tasks是一个列表，每个元素是一个tuple, 内含你要传入前面那个函数的所有形参。</p>
<p>3 将Counter转回dict。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_replace_pair_and_pair_counts</span>(<span class="params">self, word, count, pair, new_id</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新单词</span></span><br><span class="line">    new_word = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    has_pair = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(word):</span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(word)-<span class="number">1</span> <span class="keyword">and</span> word[i] == pair[<span class="number">0</span>] <span class="keyword">and</span> word[i+<span class="number">1</span>] == pair[<span class="number">1</span>]:</span><br><span class="line">            has_pair = <span class="literal">True</span></span><br><span class="line">            new_word.append(new_id)</span><br><span class="line">            i += <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_word.append(word[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 动态更新pair计数</span></span><br><span class="line">    <span class="keyword">if</span> has_pair == <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">zip</span>(word[:-<span class="number">1</span>], word[<span class="number">1</span>:]):</span><br><span class="line">            <span class="variable language_">self</span>.pair_counts[p] -= count</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">zip</span>(new_word[:-<span class="number">1</span>], new_word[<span class="number">1</span>:]):</span><br><span class="line">            <span class="variable language_">self</span>.pair_counts[p] += count</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">tuple</span>(new_word)</span><br></pre></td></tr></table></figure>
<p> _ replace_pair_and_pair_counts：输入一个单词（bytes形式），它对应的计数，要合并的pair，以及给合并后的新词分配的new_id 。输出新的单词，并在这个过程中更新self.pair_counts。<br> 容易理解，过程为先创建单词，再更新计数。注意此处更新应当只对有所更新的单词进行重新计数，否则pytest中的test_train_bpe_speed有1.5秒的速度要求将难以达成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pair_count</span>(<span class="params">self, word_counts</span>):</span><br><span class="line">    <span class="comment"># 第一次pair计数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对pair计数（第一次）</span></span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> word_counts.items():</span><br><span class="line">        word_byte_pair = <span class="built_in">zip</span>(word[:-<span class="number">1</span>], word[<span class="number">1</span>:])</span><br><span class="line">        <span class="comment"># 逐个地 += count</span></span><br><span class="line">        <span class="keyword">for</span> pair <span class="keyword">in</span> word_byte_pair:</span><br><span class="line">            <span class="variable language_">self</span>.pair_counts[pair] += count</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们已经有了对单词的计数，开始自然地对pair计数。<br>1 注意zip()的语法：输入两个长度相同的tuple，两两组合。<br>2 注意这里的计数方式符合我们的预分词目的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_step</span>(<span class="params">self, word_counts, new_id</span>):</span><br><span class="line">    <span class="comment"># merge迭代</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 找出应合并的pair</span></span><br><span class="line">    best_pair = <span class="built_in">max</span>(</span><br><span class="line">        <span class="variable language_">self</span>.pair_counts.items(), </span><br><span class="line">        key=<span class="keyword">lambda</span> item: (</span><br><span class="line">            item[<span class="number">1</span>],                                       </span><br><span class="line">            (<span class="variable language_">self</span>.vocab[item[<span class="number">0</span>][<span class="number">0</span>]], <span class="variable language_">self</span>.vocab[item[<span class="number">0</span>][<span class="number">1</span>]]) <span class="comment"># 合并后的实际 bytes 内容</span></span><br><span class="line">        )</span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    new_word_counts = &#123;&#125;</span><br><span class="line">    <span class="comment"># 合并pair并更新对单词和pair的计数</span></span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> word_counts.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word) &gt; <span class="number">1</span> <span class="keyword">and</span> (best_pair[<span class="number">0</span>] <span class="keyword">in</span> word <span class="keyword">and</span> best_pair[<span class="number">1</span>] <span class="keyword">in</span> word):</span><br><span class="line">            word = <span class="variable language_">self</span>._replace_pair_and_pair_counts(word, count, best_pair, new_id)</span><br><span class="line">        new_word_counts[word] = new_word_counts.get(word, <span class="number">0</span>) + count</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.vocab[new_id] = <span class="variable language_">self</span>.vocab[best_pair[<span class="number">0</span>]] + <span class="variable language_">self</span>.vocab[best_pair[<span class="number">1</span>]] </span><br><span class="line">    <span class="variable language_">self</span>.merges.append(( <span class="variable language_">self</span>.vocab[best_pair[<span class="number">0</span>]], <span class="variable language_">self</span>.vocab[best_pair[<span class="number">1</span>]] ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_word_counts</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>merge_step包含一步的step。输入：word_counts, new_id，返回new_word_counts，并在这个过程中更新self.vocab, self.merges 。  过程包括：<br>1 找出频率最高的pair<br>2 通过_replace_pair_and_pair_counts的更新，合并这些更新，产生新的单词计数new_word_counts传给下一步迭代。更新对pair的计数。<br>3 更新self.vocab和self.merges。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, input_path, vocab_size, </span></span><br><span class="line"><span class="params">          special_tokens, num_chunks = <span class="number">1000</span>, new_id = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">          </span>):</span><br><span class="line">    <span class="comment"># 迭代 merge_step</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单词计数</span></span><br><span class="line">    word_counts = <span class="variable language_">self</span>.counting_init(input_path, special_tokens, num_chunks = <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在最前面加上special_tokens </span></span><br><span class="line">    actual_merge_steps = vocab_size - <span class="number">256</span> - <span class="built_in">len</span>(special_tokens)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数pair（第一次）</span></span><br><span class="line">    <span class="variable language_">self</span>.pair_count(word_counts)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 迭代merge</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(actual_merge_steps):</span><br><span class="line">        new_id = <span class="number">256</span> + i</span><br><span class="line">        word_counts = <span class="variable language_">self</span>.merge_step(word_counts, new_id)</span><br><span class="line"></span><br><span class="line">    offset = <span class="built_in">len</span>(special_tokens)</span><br><span class="line">    new_vocab = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, st <span class="keyword">in</span> <span class="built_in">enumerate</span>(special_tokens):</span><br><span class="line">        new_vocab[i] = st.encode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> old_id, b <span class="keyword">in</span> <span class="variable language_">self</span>.vocab.items():</span><br><span class="line">        new_vocab[old_id + offset] = b</span><br><span class="line">        </span><br><span class="line">    <span class="variable language_">self</span>.vocab = new_vocab</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.vocab, <span class="variable language_">self</span>.merges</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>train：合并上述过程。包括：<br>1 先进行第一步单词计数counting_init<br>2 进行第一步pair计数pair_count<br>3 迭代刚才的merge_step。  得到完整的vocab和merges<br>4 在最终的vocab开头给special_tokens分配固定的id。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bpe</span>(<span class="params">input_path, vocab_size, special_tokens, num_chunks = <span class="number">1000</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> cProfile, pstats</span><br><span class="line">    profiler = cProfile.Profile()</span><br><span class="line">    profiler.enable() <span class="comment"># 开始分析</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    tokenizer = BPEtokenizer()</span><br><span class="line">    tokenizer.train(input_path, vocab_size, </span><br><span class="line">              special_tokens, num_chunks = <span class="number">1000</span>, new_id = <span class="literal">None</span>, </span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    profiler.disable() <span class="comment"># 结束分析</span></span><br><span class="line">    stats = pstats.Stats(profiler).sort_stats(<span class="string">&#x27;cumulative&#x27;</span>)</span><br><span class="line">    stats.print_stats(<span class="number">15</span>) <span class="comment"># 打印耗时最长的 15 个函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenizer</span><br></pre></td></tr></table></figure>
<p>train_bpe: 在类外面。用于创建新实例，和adapter.py相接。<br>1 创建一个新的tokenizer实例，进行训练。<br>2 按照课程的要求，使用cProfile进行分析。</p>
]]></content>
      <categories>
        <category>cs336</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>cs336</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>cs336：Assignment1-basics-chapter1&amp;2</title>
    <url>/2026/01/24/Assignment1-basics-chapter1&amp;2/</url>
    <content><![CDATA[<p>本文为cs336的assignment1中分词器对应章节的参考解答和课程笔记。   </p>
<p>1 本文范围内的作业暂时还不需要用到GPU<br>2 本课程并未提供好系统适配，所以事实上不推荐在win上完成作业。在你的服务器or虚拟机上完成可以避免很多麻烦。</p>
<h1 id="Setup："><a href="#Setup：" class="headerlink" title="Setup："></a>Setup：</h1><h2 id="环境配置："><a href="#环境配置：" class="headerlink" title="环境配置："></a>环境配置：</h2><p>按照<a href="https://github.com/stanford-cs336/assignment1-basics/tree/main">https://github.com/stanford-cs336/assignment1-basics/tree/main</a> 中README的说明配置并测试uv。  </p>
<p>如果你在Windows下，uv run pytest 时会出现问题，因为你没有办法import resource。<br>进行如下操作：<br>打开\assignment1-basics\tests\test_tokenizer.py， 删除第5行的import resource，或修改：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> resource</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    resource = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>然后重新uv run pytest。  </p>
<h2 id="数据下载："><a href="#数据下载：" class="headerlink" title="数据下载："></a>数据下载：</h2><p>下载 TinyStories data 和 subsample of OpenWebText：<br>课程原生使用的是wget； windows下，推荐使用curl。试着使用如下命令：<br><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建并进入数据目录</span></span><br><span class="line">mkdir <span class="literal">-p</span> <span class="keyword">data</span></span><br><span class="line"><span class="built_in">cd</span> <span class="keyword">data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 TinyStories</span></span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> TinyStoriesV2<span class="literal">-GPT4-train</span>.txt https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2<span class="literal">-GPT4-train</span>.txt</span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> TinyStoriesV2<span class="literal">-GPT4-valid</span>.txt https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2<span class="literal">-GPT4-valid</span>.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 OpenWebText</span></span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> owt_train.txt.gz https://huggingface.co/datasets/stanford<span class="literal">-cs336</span>/owt<span class="literal">-sample</span>/resolve/main/owt_train.txt.gz</span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> owt_valid.txt.gz https://huggingface.co/datasets/stanford<span class="literal">-cs336</span>/owt<span class="literal">-sample</span>/resolve/main/owt_valid.txt.gz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 .gz 文件</span></span><br><span class="line"><span class="comment"># Windows 10/11 自带 tar.exe，可以替代 gunzip</span></span><br><span class="line">tar.exe <span class="literal">-xvzf</span> owt_train.txt.gz</span><br><span class="line">tar.exe <span class="literal">-xvzf</span> owt_valid.txt.gz</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure></p>
<p>如果解压不成功，考虑使用python原生解压。<br><span id="more"></span></p>
<p>先输入python进入交互模式，然后：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.<span class="built_in">open</span>(<span class="string">&#x27;owt_valid.txt.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f_in:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;owt_valid.txt&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f_out:</span><br><span class="line">        shutil.copyfileobj(f_in, f_out)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p><br> <br><br> </p>
<h1 id="1-作业总览"><a href="#1-作业总览" class="headerlink" title="1 作业总览"></a>1 作业总览</h1><p>你将构建训练标准 Transformer LM 所需的所有组件，并训练一些模型。</p>
<h3 id="你将实现："><a href="#你将实现：" class="headerlink" title="你将实现："></a>你将实现：</h3><p>1 BPE 分词器 (Byte-pair encoding tokenizer)：第 2 节<br>2 Transformer 语言模型 (LM)：第 3 节<br>3 交叉熵损失函数与 AdamW 优化器：第 4 节<br>4 训练循环：支持模型和优化器状态的序列化与加载（保存与读取）：第 5 节  </p>
<h3 id="你将完成如下任务："><a href="#你将完成如下任务：" class="headerlink" title="你将完成如下任务："></a>你将完成如下任务：</h3><p>1 在 TinyStories 数据集上训练一个 BPE 分词器。<br>2 对数据集运行训练好的分词器，将其转换为整数 ID 序列。<br>3 在 TinyStories 数据集上训练 Transformer 语言模型。<br>4 使用训练好的模型生成样本并评估困惑度 (Perplexity)。<br>5 在 OpenWebText 数据集上训练模型，并将达到的困惑度结果提交到排行榜。  </p>
<h3 id="允许使用的工具："><a href="#允许使用的工具：" class="headerlink" title="允许使用的工具："></a>允许使用的工具：</h3><p>课程希望你从0开始搭组件，所以不得使用<code>torch.nn</code>、<code>torch.nn.functional</code> 或 <code>torch.optim</code> 中的任何定义，除了：</p>
<p>1 <code>torch.nn.Parameter</code><br>2 <code>torch.nn</code> 中的容器类（<code>Module</code>, <code>ModuleList</code>, <code>Sequential</code> 等）<br>3 <code>torch.optim.Optimizer</code> 基类   </p>
<h3 id="关于-AI-工具："><a href="#关于-AI-工具：" class="headerlink" title="关于 AI 工具："></a>关于 AI 工具：</h3><p>1 允许使用大语言模型进行低级的编程问题或提问高级的概念问题的咨询，但严禁直接使用 AI 来完成作业题目。<br>2 建议在完成作业时关闭 IDE 中的 AI 自动补全（如 Cursor Tab, GitHub CoPilot）。</p>
<h3 id="数据集获取："><a href="#数据集获取：" class="headerlink" title="数据集获取："></a>数据集获取：</h3><p>本次作业使用两个预处理过的数据集：TinyStories 和 OpenWebText。两者都是大型纯文本文件。<br>校内学生可在实验室机器的 <code>/data</code> 目录下找到。校外人员/自学者可根据<strong>README.md中的命令下载。</strong></p>
<h3 id="降低规模的建议："><a href="#降低规模的建议：" class="headerlink" title="降低规模的建议："></a>降低规模的建议：</h3><p>后续课程会提供建议，解释如何在 CPU 或 MPS 环境下进行调整。</p>
<p><br> <br> <br></p>
<h1 id="2-BPE-Byte-Pair-Encoding-分词器"><a href="#2-BPE-Byte-Pair-Encoding-分词器" class="headerlink" title="2 BPE(Byte-Pair Encoding)分词器"></a>2 BPE(Byte-Pair Encoding)分词器</h1><h2 id="2-1-unicode标准"><a href="#2-1-unicode标准" class="headerlink" title="2.1 unicode标准"></a>2.1 unicode标准</h2><p>Unicode标准将字符映射到整数code points上。例如”s” 的code point 是115（或写作十六进制的U+0073），”牛”对应29275。</p>
<p>python中，有ord()和chr()函数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">ord</span>(<span class="string">&#x27;牛&#x27;</span>)</span><br><span class="line"><span class="number">29275</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">chr</span>(<span class="number">29275</span>)</span><br><span class="line"><span class="string">&#x27;牛&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Problem-unicode1-unicode介绍-（1分）"><a href="#Problem-unicode1-unicode介绍-（1分）" class="headerlink" title="Problem (unicode1): unicode介绍 （1分）"></a>Problem (unicode1): unicode介绍 （1分）</h3><p>(a)<code>chr(0)</code> 返回的是什么字符？<br><code>chr(0)</code> 返回的是 Unicode 编码为 0 的字符，即空字符（Null Character）。</p>
<p>(b) 它的字符串表示形式 (<code>__repr__()</code>) 与打印形式有何不同？<br>在 Python 中，其字符串表示形式（<code>__repr__()</code>）会显示为转义序列 <code>&#39;\x00&#39;</code>，而打印该字符（<code>print()</code>）时通常是不可见的，在某些终端里可能显示为空格。</p>
<p>(c)当该字符出现在文本中时会发生什么？<br>在 Python 字符串内部它可以正常存在并拼接，但在将其打印到终端或与底层 C 语言编写的程序交互时，它可能会被当做文本结束符而导致后面的内容被截断，或者干脆显示为一个空白区域。</p>
<p><br> <br></p>
<h2 id="2-2-unicode编码"><a href="#2-2-unicode编码" class="headerlink" title="2.2 unicode编码"></a>2.2 unicode编码</h2><p>主要介绍了UTF-8。编码和解码UTF-8的函数包括encode()和decode()，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_string = <span class="string">&quot;hello! こんにちは!&quot;</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>utf8_encoded = test_string.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded) </span><br><span class="line"><span class="string">b&#x27;hello! \xe3\x81\x93\xe3\x82\x93\xe3\x81\xab\xe3\x81\xa1\xe3\x81\xaf!&#x27;</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">type</span>(utf8_encoded))</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;bytes&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Get the byte values for the encoded string (integers from 0 to 255).</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(utf8_encoded) </span><br><span class="line">[<span class="number">104</span>, <span class="number">101</span>, <span class="number">108</span>, <span class="number">108</span>, <span class="number">111</span>, <span class="number">33</span>, <span class="number">32</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">130</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">171</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">161</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">175</span>, <span class="number">33</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># One byte does not necessarily correspond to one Unicode character! </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(test_string)) </span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(utf8_encoded))</span><br><span class="line"><span class="number">23</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded.decode(<span class="string">&quot;utf-8&quot;</span>)) <span class="number">1</span></span><br><span class="line">hello! こんにちは!</span><br></pre></td></tr></table></figure></p>
<p>我们将整数的codepoints转换成了一个byte序列，它们会更易于处理。例如，因为任何文本实质上都被转化为介于 $0-255$ 之间的整数序列，只要词表包含了这 256 个基础字节，就不需要担心训练和搭建的过程中词表外（OOV）词汇。</p>
<h3 id="课程文档外的补充：UTF-8字节序列对应规则"><a href="#课程文档外的补充：UTF-8字节序列对应规则" class="headerlink" title="课程文档外的补充：UTF-8字节序列对应规则"></a>课程文档外的补充：UTF-8字节序列对应规则</h3><p>字节数1：<br>对应Unicode编码范围：U+0000 至 U+007F 或 0~127 (7 bits)<br>对应utf-8字节序列：0xxxxxxx</p>
<p>字节数2：<br>对应Unicode编码范围：U+0080 至 U+07FF 或 128~2047 (11 bits)<br>对应utf-8字节序列：110xxxxx 10xxxxxx</p>
<p>字节数3：<br>对应Unicode编码范围：U+0800 至 U+FFFF 或 2048~65535 (16 bits)<br>对应utf-8字节序列：1110xxxx 10xxxxxx 10xxxxxx</p>
<p>字节数4：<br>对应Unicode编码范围：U+10000 至 U+10FFFF 或 65536~1114111 (21 bits)<br>应utf-8字节序列：11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</p>
<p>对应方式：先把unicode编码翻译成二进制，在填入对应字节序列的’x’处。（长度不够填则在左侧补0）。</p>
<h3 id="Problem-unicode2-unicode编码（3分）"><a href="#Problem-unicode2-unicode编码（3分）" class="headerlink" title="Problem (unicode2): unicode编码（3分）"></a>Problem (unicode2): unicode编码（3分）</h3><p>(a) 为什么 Tokenizer 训练更偏好 UTF-8而不是UTF-16, UTF-32？<br>UTF-8 是变长编码，能够将常用的 ASCII 字符保持为单字节，避免了 UTF-16/32 在处理英文或代码时产生大量冗余零字节。</p>
<p>(b) 错误的解码函数分析：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decode_utf8_bytes_to_str_wrong</span>(<span class="params">bytestring: <span class="built_in">bytes</span></span>): </span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([<span class="built_in">bytes</span>([b]).decode(<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">for</span> b <span class="keyword">in</span> bytestring]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>decode_utf8_bytes_to_str_wrong(<span class="string">&quot;hello&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)) <span class="string">&#x27;hello&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p>错误输入：如<code>b&#39;\xe4\xb8\xad&#39;</code> （汉字“中”的编码字节流）<br>因为UTF-8 中的中文字符或复杂符号是由 2 到 4 个字节共同组成的，单独解码其中任何一个字节都会因不符合 UTF-8 规范而报错（或产生乱码）。</p>
<p>(c)给出无法解码的字节：<br>如<code>b&#39;\xff\xff&#39;</code>。<br>原因：任何以 <code>11111xxx</code> 开头的字节都没有对应的 5 字节或更长的有效模板，无法解码为任何 Unicode 字符。</p>
<p><br> <br></p>
<h2 id="2-3-子词分词方案（subword-tokenization）"><a href="#2-3-子词分词方案（subword-tokenization）" class="headerlink" title="2.3 子词分词方案（subword tokenization）"></a>2.3 子词分词方案（subword tokenization）</h2><p>将词语拆成byte序列之后，仍然不能逐个byte拆分来看作为token：这会导致序列变得极长，训练的step变长，计算量也增大（如transformer计算复杂度与序列长度成正比）；同时更长的序列也导致信息密度被稀释，网络寻找token间的关系变得更困难。</p>
<p>而subword tokenization就是指代这样一个折中的方案。它选择用一个更大的词汇表(vocabulary)来trade-off更短的序列。比如说，如果‘the’经常出现，我们就可以给它单独分配一个条目(entry)，词汇表维度+1，但把3个token缩短成了1个。</p>
<p>为了做这样的工作，Sennrich et al. (2016) 提出使用byte-pair encoding (BPE; Gage, 1994)。作为一种subword tokenization，它简单地基于出现频率，将经常出现的byte pair合并(merge)成一个未被使用的索引。基于BPE的subword tokenizer被称为BPE tokenizer。</p>
<p><br> <br></p>
<h2 id="2-4-训练BPE分词器"><a href="#2-4-训练BPE分词器" class="headerlink" title="2.4 训练BPE分词器"></a>2.4 训练BPE分词器</h2><h3 id="1-初始化-vocabulary"><a href="#1-初始化-vocabulary" class="headerlink" title="1 初始化 vocabulary"></a>1 初始化 vocabulary</h3><p>初始化的vocabulary是从byte到整数ID的映射。因此，vocabulary的大小为256。</p>
<h3 id="2-预分词（Pre-tokenization"><a href="#2-预分词（Pre-tokenization" class="headerlink" title="2 预分词（Pre-tokenization)"></a>2 预分词（Pre-tokenization)</h3><p>理论上，有了词汇表我们就可以开始进行上述的合并（merge）工作了。然而，有两个关键的问题：<br>i 每次合并的时候，都需要从头到尾过一遍语料库。这在计算上是很昂贵的。<br>ii 直接合并会导致出现一些新的token，它们只有标点符号的区别（比如”dog.“和”dog!“），它们会拥有完全不同的ID，即使它们在语义上是完全相同的。  </p>
<p>为了解决上述问题，我们进行对语料库的预分词(pre-tokenize)，先把语料库切成单词。这是如何省下计算成本的？举个例子，当我们已经计数了’text’的出现次数（比如说10次），当我们需要计数’te’的出现次数时，就可以直接+=10，从而避免了每次遇到同一个单词时重复的计算。</p>
<p>早期分词方案包括直接通过空格进行切分（s.split(“”))，但显然这不能解决上述提到的问题2。</p>
<p>于是，我们将使用的时一个基于正则表达式(regex)的分词器 (used by GPT-2; Radford et al., 2019)，可以在github.com/openai/tiktoken/pull/234/files中找到。如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>PAT = <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>这里的五个部分分别处理：缩写后缀（’ll, ‘ve, ‘t , ‘s等）；连续字母（单词）；连续数字；连续标点符号；连续纯空格。</p>
<p>例子如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># requires `regex` package </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> regex <span class="keyword">as</span> re </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> re.findall(PAT, <span class="string">&quot;some text that i&#x27;ll pre-tokenize&quot;</span>) </span><br><span class="line">[<span class="string">&#x27;some&#x27;</span>, <span class="string">&#x27; text&#x27;</span>, <span class="string">&#x27; that&#x27;</span>, <span class="string">&#x27; i&#x27;</span>, <span class="string">&quot;&#x27;ll&quot;</span>, <span class="string">&#x27; pre&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;tokenize&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<p>实际使用中，用findall内存容易溢出，建议使用re.finditer这一迭代计算的函数。</p>
<h3 id="3-计算BPE合并-amp-处理特殊token"><a href="#3-计算BPE合并-amp-处理特殊token" class="headerlink" title="3 计算BPE合并&amp;处理特殊token"></a>3 计算BPE合并&amp;处理特殊token</h3><p>现在我们可以开始按照前述方法计算合并了。只需要注意两点：<br>i  不能跨越预分词边界合并。<code>[&#39;some&#39;, &#39;text&#39;]</code>，那么 <code>e</code>（来自 some）和 <code>t</code>（来自 text）永远不会被统计在一起。<br>ii 多个byte pair出现频率并列第一时，选择字典序最大（Lexicographically greater）的那一对。(“A”, “B”), (“A”, “C”), (“B”, “ZZ”), 和 (“BA”, “A”)频率相同时，在 ASCII 中 “BA” &gt; “B” &gt; “A”），因此 <code>max</code> 会选出 <code>(&#39;BA&#39;, &#39;A&#39;)</code>。<br>iii 有一些特殊token不能和其他合并，比如&lt;|endoftext|&gt;。它不应该被分成几个零碎的token，因此我们会给它安排一个固定的tokenID。  </p>
<h3 id="一个训练的具体例子："><a href="#一个训练的具体例子：" class="headerlink" title="一个训练的具体例子："></a>一个训练的具体例子：</h3><p>例如我们现在有如下corpus：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">low low low low </span><br><span class="line">low lower lower widest widest widest </span><br><span class="line">newest newest newest newest newest newest</span><br></pre></td></tr></table></figure><br>, 且vocabulary里已经放好了&lt;|endoftext|&gt;这一special token。</p>
<h4 id="1-初始化vocabulary："><a href="#1-初始化vocabulary：" class="headerlink" title="1 初始化vocabulary："></a>1 初始化vocabulary：</h4><p>一个special token和256个byte value。</p>
<h4 id="2-Pre-tokenization"><a href="#2-Pre-tokenization" class="headerlink" title="2 Pre-tokenization:"></a>2 Pre-tokenization:</h4><p>为了简化，我们仅使用用空格分隔，最终得到频率表：{low: 5, lower: 2, widest: 3, newest: 6}。为容易处理，将它写成dict[tuple[bytes], int]的格式，如{(l,o,w): 5 …}。</p>
<h4 id="3-合并："><a href="#3-合并：" class="headerlink" title="3 合并："></a>3 合并：</h4><p>数出上述例子中byte pair的出现频率：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;lo: <span class="number">7</span>, ow: <span class="number">7</span>, we: <span class="number">8</span>, er: <span class="number">2</span>, wi: <span class="number">3</span>, <span class="built_in">id</span>: <span class="number">3</span>, de: <span class="number">3</span>, es: <span class="number">9</span>, st: <span class="number">9</span>, ne: <span class="number">6</span>, ew: <span class="number">6</span>&#125;</span><br></pre></td></tr></table></figure><br>(‘es’) 和 (‘st’)并列，我们就取字典序更大的(‘st’)。<br>于是表格变为：{(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}.</p>
<p>如是循环，进行6次merge，可以得到新产生的vocabulary：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&#x27;s t&#x27;</span>, <span class="string">&#x27;e st&#x27;</span>, <span class="string">&#x27;o w&#x27;</span>, <span class="string">&#x27;l ow&#x27;</span>, <span class="string">&#x27;w est&#x27;</span>, <span class="string">&#x27;n e&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<p>于是新的词汇表变为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[&lt;|endoftext|&gt;, [..<span class="number">.256</span> BYTE CHARS], st, est, ow, low, west, ne]</span><br></pre></td></tr></table></figure></p>
<p>在此语境下，‘newest’ 将被分词为[ne, west]。</p>
<p><br> <br></p>
<h2 id="2-5-实验：训练BPE"><a href="#2-5-实验：训练BPE" class="headerlink" title="2.5 实验：训练BPE"></a>2.5 实验：训练BPE</h2><p>我们接下来再TinyStories数据集上训练一个BPE。你可以在Section1里找到它的下载方式。在开始之前，推荐你先大概看一眼里面都是什么内容。</p>
<h3 id="1-并行处理预分词"><a href="#1-并行处理预分词" class="headerlink" title="1 并行处理预分词"></a>1 并行处理预分词</h3><p>训练过程中，预分词会是一个主要的瓶颈。你可以使用内置库<code>multiprocessing</code>并行化你的代码。<br>你可以逐字使用以下链接中的入门代码来获取分块边界，然后使用这些边界将工作分配到各个进程：<br><a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py">https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py</a><br>阅读此代码，你会发现它实现的是确保每个分块边界都切割在special_token之前。</p>
<h3 id="2-预分词前，剔除特殊token不进行处理"><a href="#2-预分词前，剔除特殊token不进行处理" class="headerlink" title="2 预分词前，剔除特殊token不进行处理"></a>2 预分词前，剔除特殊token不进行处理</h3><p>用”|” .join(special_tokens)来re.split以移除&lt;|endoftext|&gt;</p>
<h3 id="3-优化合并"><a href="#3-优化合并" class="headerlink" title="3 优化合并"></a>3 优化合并</h3><p>naive BPE training（逐个合并，每次合并从头遍历一次）计算量太大。考虑到每次合并改变的计数只有这个合并前后的计数，我们可以动态处理合并，即每次合并只将前后的计数-1。如合并‘text’中的‘ex’，只需要将te和xt的计数-1，而不需要合并完之后再从头数一遍。</p>
<h3 id="Downscaling-提示"><a href="#Downscaling-提示" class="headerlink" title="Downscaling 提示"></a>Downscaling 提示</h3><p>1 cProfile和scalene等分析工具可以分析你的实现中的瓶颈，于是你可以专注于分析这些瓶颈。<br>2 与其直接现在TInyStories的training set上训练，你可以先将validation set作为‘调试数据(debug dataset)’训练。后者只有22K个文档，前者有2.12M个文档。</p>
<h3 id="Problem-train-bpe-BPE训练（15分）"><a href="#Problem-train-bpe-BPE训练（15分）" class="headerlink" title="Problem (train_bpe): BPE训练（15分）"></a>Problem (train_bpe): BPE训练（15分）</h3><p>1 编写你的tokenizer，然后在adapter.py中import，最后uv run pytest tests/test_train_bpe.py。<br>需要注意，如果出现类似” Extra items in the left set: b’\r\n\r’  “的报错，这是因为linux文本会被windows自动将\n更改为\r\n。这也很好解决，直接处理一步.replace(b”\r\n”, b”\n”)即可。    </p>
<p>笔者的个人实现见<br><a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a>  ，<br>本节答案对应截止在#—2.6 Encoding &amp; Decoding—#上方的内容。<br>对其的说明参考<a href="https://zian-2.github.io/2026/01/21/tokenizer.py/">https://zian-2.github.io/2026/01/21/tokenizer.py/</a>  </p>
<p>2 按照文章的建议，You should use profiling tools like <strong><code>cProfile</code></strong> or <strong><code>scalene</code></strong> to identify the bottlenecks in your implementation.  个人的实现，在主函数内部添加如下代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bpe</span>(<span class="params">input_path, vocab_size, special_tokens, num_chunks = <span class="number">1000</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> cProfile, pstats</span><br><span class="line">    profiler = cProfile.Profile()</span><br><span class="line">    profiler.enable() <span class="comment"># 开始分析</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"><span class="comment">#  你的完整的分词进程</span></span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"></span><br><span class="line">    profiler.disable() <span class="comment"># 结束分析</span></span><br><span class="line">    stats = pstats.Stats(profiler).sort_stats(<span class="string">&#x27;cumulative&#x27;</span>)</span><br><span class="line">    stats.print_stats(<span class="number">15</span>) <span class="comment"># 打印耗时最长的 15 个函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenizer</span><br></pre></td></tr></table></figure></p>
<p>添加分析之后运行时长增加是正常的。注意在表格中只需要关注tottime（函数内部的运行时长，不包括调用的外部函数）。</p>
<h3 id="Problem-train-bpe-tinystories-：TinyStories上的训练（2分）"><a href="#Problem-train-bpe-tinystories-：TinyStories上的训练（2分）" class="headerlink" title="Problem (train_bpe_tinystories)：TinyStories上的训练（2分）"></a>Problem (train_bpe_tinystories)：TinyStories上的训练（2分）</h3><p>(a) 在 TinyStories 数据集上训练一个字节级（byte-level）的 BPE 分词器，词表最大容量为 10,000。确保在词表中加入 <code>&lt;|endoftext|&gt;</code> 特殊标记。将生成的词表（vocabulary）和合并规则（merges）序列化保存到磁盘以便后续检查。训练耗时多少小时？消耗了多少内存？词表中最长的 Token 是什么？它合理吗？</p>
<p>资源要求： 耗时 ≤ 30 分钟（不使用 GPU），内存 ≤ 30GB RAM。<br>提示：你可以使用multiprocessing，可以使训练达到2分钟以内（这里应该指的是训练集TinyStoriesV2-GPT4-train.txt）</p>
<p>(b) 对你的代码进行性能分析（Profile）。分词器训练过程中哪个部分最耗时？</p>
<h4 id="Answer："><a href="#Answer：" class="headerlink" title="Answer："></a>Answer：</h4><p>你需要先把TinyStories接上你的分词器。记录内存消耗峰值，同时用cProfile分析，最后输出longest token，并将vocab和merges导入到你的磁盘里。参考实现如下（不包括profile）：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> .tokenizer <span class="keyword">import</span> train_bpe</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 路径配置</span></span><br><span class="line">    input_path = <span class="string">&quot;../data/TinyStoriesV2-GPT4-valid.txt&quot;</span></span><br><span class="line">    vocab_size = <span class="number">10000</span> </span><br><span class="line">    special_tokens = [<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>] </span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;开始训练&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用训练函数</span></span><br><span class="line">    tokenizer = train_bpe(</span><br><span class="line">        input_path=input_path,</span><br><span class="line">        vocab_size=vocab_size,</span><br><span class="line">        special_tokens=special_tokens,</span><br><span class="line">        num_chunks=<span class="number">1000</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n训练完成&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;词表大小: <span class="subst">&#123;<span class="built_in">len</span>(tokenizer.vocab)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;合并次数: <span class="subst">&#123;<span class="built_in">len</span>(tokenizer.merges)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    output_dir = <span class="string">&quot;run_bpe_train_on_tinystories_output&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">        os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line">    readable_vocab = &#123;<span class="built_in">int</span>(k): <span class="built_in">list</span>(v) <span class="keyword">for</span> k, v <span class="keyword">in</span> tokenizer.vocab.items()&#125;</span><br><span class="line">    vocab_path = os.path.join(output_dir, <span class="string">&quot;vocab.json&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(vocab_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(readable_vocab, f, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    merges_path = os.path.join(output_dir, <span class="string">&quot;merges.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(merges_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> p1, p2 <span class="keyword">in</span> tokenizer.merges:</span><br><span class="line">            <span class="comment"># 将 bytes 转换为逗号分隔的数字字符串</span></span><br><span class="line">            s1 = <span class="string">&quot;,&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">list</span>(p1)))</span><br><span class="line">            s2 = <span class="string">&quot;,&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">list</span>(p2)))</span><br><span class="line">            f.write(<span class="string">f&quot;<span class="subst">&#123;s1&#125;</span> <span class="subst">&#123;s2&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h4 id="Deliverable"><a href="#Deliverable" class="headerlink" title="Deliverable:"></a>Deliverable:</h4><p>(a)  笔者的实现参考：<a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a> 。<br>在TinyStoriesV2-GPT4-train.txt上训练在40秒以内，内存峰值0.8938 GB。<br>设备条件：28GB内存，Intel Core Ultra 7 255HX CPU。<br>longest token: ‘ accomplishment’。输出合理。</p>
<p>(b)Profile : 你可以以任何方式完成Profile。在TinyStoriesV2-GPT4-train.txt上的参考profile如下：<br><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">==================================================</span><br><span class="line">Profile (Top <span class="number">20</span> Functions):</span><br><span class="line">         <span class="number">15808949</span> <span class="function"><span class="keyword">function</span> <span class="title">calls</span> <span class="params">(15808073 primitive calls)</span> <span class="title">in</span> <span class="title">40</span>.<span class="title">449</span> <span class="title">seconds</span></span></span><br><span class="line"></span><br><span class="line">   Ordered by: internal time</span><br><span class="line">   List reduced from <span class="number">457</span> to <span class="number">20</span> due to restriction &lt;<span class="number">20</span>&gt;</span><br><span class="line"></span><br><span class="line">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)</span><br><span class="line">  <span class="number">463</span>/<span class="number">235</span>   <span class="number">20.998</span>    <span class="number">0.045</span>    <span class="number">6.500</span>    <span class="number">0.028</span> &#123;built<span class="operator">-in</span> method _winapi.WaitForMultipleObjects&#125;</span><br><span class="line">     <span class="number">9743</span>    <span class="number">7.911</span>    <span class="number">0.001</span>   <span class="number">14.576</span>    <span class="number">0.001</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">169</span>(merge_step)</span><br><span class="line">    <span class="number">23282</span>    <span class="number">2.899</span>    <span class="number">0.000</span>    <span class="number">2.951</span>    <span class="number">0.000</span> &#123;built<span class="operator">-in</span> method builtins.max&#125;</span><br><span class="line">   <span class="number">277780</span>    <span class="number">2.271</span>    <span class="number">0.000</span>    <span class="number">3.413</span>    <span class="number">0.000</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">115</span>(_replace_pair_and_pair_counts)</span><br><span class="line">       <span class="number">80</span>    <span class="number">1.486</span>    <span class="number">0.019</span>    <span class="number">1.486</span>    <span class="number">0.019</span> &#123;built<span class="operator">-in</span> method _pickle.loads&#125;</span><br><span class="line">     <span class="number">1002</span>    <span class="number">1.238</span>    <span class="number">0.001</span>    <span class="number">2.147</span>    <span class="number">0.002</span> D:\Program Files\Python313\Lib\collections\__init__.py:<span class="number">673</span>(update)</span><br><span class="line">  <span class="number">6434620</span>    <span class="number">1.028</span>    <span class="number">0.000</span>    <span class="number">1.028</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;get&#x27;</span> of <span class="string">&#x27;dict&#x27;</span> objects&#125;</span><br><span class="line">  <span class="number">2538958</span>    <span class="number">0.486</span>    <span class="number">0.000</span>    <span class="number">0.486</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;add&#x27;</span> of <span class="string">&#x27;set&#x27;</span> objects&#125;</span><br><span class="line">  <span class="number">1362761</span>    <span class="number">0.374</span>    <span class="number">0.000</span>    <span class="number">0.374</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;discard&#x27;</span> of <span class="string">&#x27;set&#x27;</span> objects&#125;</span><br><span class="line">  <span class="number">689</span>/<span class="number">189</span>    <span class="number">0.323</span>    <span class="number">0.000</span>   <span class="number">38.936</span>    <span class="number">0.206</span> &#123;built<span class="operator">-in</span> method _winapi.ReadFile&#125;</span><br><span class="line">        <span class="number">1</span>    <span class="number">0.217</span>    <span class="number">0.217</span>   <span class="number">40.447</span>   <span class="number">40.447</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">218</span>(train)</span><br><span class="line">  <span class="number">3008052</span>    <span class="number">0.190</span>    <span class="number">0.000</span>    <span class="number">0.190</span>    <span class="number">0.000</span> &#123;built<span class="operator">-in</span> method builtins.len&#125;</span><br><span class="line">   <span class="number">277795</span>    <span class="number">0.156</span>    <span class="number">0.000</span>    <span class="number">0.156</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;pop&#x27;</span> of <span class="string">&#x27;dict&#x27;</span> objects&#125;</span><br><span class="line">        <span class="number">1</span>    <span class="number">0.135</span>    <span class="number">0.135</span>    <span class="number">0.196</span>    <span class="number">0.196</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">149</span>(pair_count)</span><br><span class="line">  <span class="number">1370918</span>    <span class="number">0.133</span>    <span class="number">0.000</span>    <span class="number">0.133</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;append&#x27;</span> of <span class="string">&#x27;list&#x27;</span> objects&#125;</span><br><span class="line">       <span class="number">20</span>    <span class="number">0.123</span>    <span class="number">0.006</span>    <span class="number">0.123</span>    <span class="number">0.006</span> &#123;built<span class="operator">-in</span> method _winapi.CreateProcess&#125;</span><br><span class="line">       <span class="number">78</span>    <span class="number">0.092</span>    <span class="number">0.001</span>    <span class="number">0.374</span>    <span class="number">0.005</span> D:\Program Files\Python313\Lib\multiprocessing\connection.py:<span class="number">246</span>(recv)</span><br><span class="line">  <span class="number">794</span>/<span class="number">768</span>    <span class="number">0.051</span>    <span class="number">0.000</span>    <span class="number">0.128</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;GetOverlappedResult&#x27;</span> of <span class="string">&#x27;_winapi.Overlapped&#x27;</span> objects&#125;</span><br><span class="line">      <span class="number">157</span>    <span class="number">0.050</span>    <span class="number">0.000</span>    <span class="number">0.050</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;write&#x27;</span> of <span class="string">&#x27;_io.BytesIO&#x27;</span> objects&#125;</span><br><span class="line">   <span class="number">270267</span>    <span class="number">0.047</span>    <span class="number">0.000</span>    <span class="number">0.048</span>    <span class="number">0.000</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">178</span>(&lt;genexpr&gt;)</span><br><span class="line"></span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure></p>
<h3 id="Problem-train-bpe-expts-owt-OpenWebText上的训练（2分）"><a href="#Problem-train-bpe-expts-owt-OpenWebText上的训练（2分）" class="headerlink" title="Problem (train_bpe_expts_owt): OpenWebText上的训练（2分）"></a>Problem (train_bpe_expts_owt): OpenWebText上的训练（2分）</h3><p>(a)在OpenWebText上训练，使用vocab_size = 32000。把你得到的vocab和merges存入硬盘。词表中longest token是什么？合理吗？<br>资源要求： 耗时 ≤ 12小时（不使用 GPU），内存 ≤ 100GB RAM。<br>(b)  对比你分别在TinyStories和OpenWebText上得到的分词器。</p>
<h4 id="Answer"><a href="#Answer" class="headerlink" title="Answer:"></a>Answer:</h4><p>(a) uv run python -m cs336_basics.run_bpe_train_on_openwebtext。<br>longest token基本都是各种长横线或者下划线，你可以通过数据清洗试图避免这一点，但是owt里东西确实比较杂，笔者尝试了一些方法都不能清洗干净。  </p>
<p>(b) id小的词差不多，id大的词区别还是比较明显。owt的词明显更多简写，更不日常，更不口语化；Tiny就有些童话色彩。同样取10000附近的词举例：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OWT:</span><br><span class="line">&quot;9985&quot;: &quot;82&quot;,</span><br><span class="line">&quot;9986&quot;: &quot; signals&quot;,</span><br><span class="line">&quot;9987&quot;: &quot; oxy&quot;,</span><br><span class="line">&quot;9988&quot;: &quot; eager&quot;,</span><br><span class="line">&quot;9989&quot;: &quot;igg&quot;,</span><br><span class="line">&quot;9990&quot;: &quot;ERS&quot;,</span><br><span class="line">&quot;9991&quot;: &quot; unprecedented&quot;,</span><br><span class="line">&quot;9992&quot;: &quot; mood&quot;,</span><br><span class="line">&quot;9993&quot;: &quot; custody&quot;,</span><br><span class="line">&quot;9994&quot;: &quot; bankrupt&quot;,</span><br><span class="line">&quot;9995&quot;: &quot; asylum&quot;,</span><br><span class="line">&quot;9996&quot;: &quot; acknowledge&quot;,</span><br><span class="line">&quot;9997&quot;: &quot;reek&quot;,</span><br><span class="line">&quot;9998&quot;: &quot;endar&quot;,</span><br><span class="line">&quot;9999&quot;: &quot;books&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TinyStories:</span><br><span class="line">&quot;9985&quot;: &quot; Froggy&quot;, </span><br><span class="line">&quot;9986&quot;: &quot; wrapper&quot;, </span><br><span class="line">&quot;9987&quot;: &quot; Reddy&quot;, </span><br><span class="line">&quot;9988&quot;: &quot; Hops&quot;, </span><br><span class="line">&quot;9989&quot;: &quot; Crusty&quot;, </span><br><span class="line">&quot;9990&quot;: &quot; whiskers&quot;, </span><br><span class="line">&quot;9991&quot;: &quot; nicest&quot;, </span><br><span class="line">&quot;9992&quot;: &quot; improving&quot;, </span><br><span class="line">&quot;9993&quot;: &quot; booth&quot;, </span><br><span class="line">&quot;9994&quot;: &quot; Land&quot;, </span><br><span class="line">&quot;9995&quot;: &quot; Surrender&quot;, </span><br><span class="line">&quot;9996&quot;: &quot; Rocky&quot;, </span><br><span class="line">&quot;9997&quot;: &quot; meadows&quot;, </span><br><span class="line">&quot;9998&quot;: &quot; imaginary&quot;, </span><br><span class="line">&quot;9999&quot;: &quot; bold&quot;</span><br></pre></td></tr></table></figure></p>
<p>作为对比，再给出32000附近的OWT：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;31985&quot;: &quot; coated&quot;,</span><br><span class="line">&quot;31986&quot;: &quot; bland&quot;,</span><br><span class="line">&quot;31987&quot;: &quot; bending&quot;,</span><br><span class="line">&quot;31988&quot;: &quot; bamboo&quot;,</span><br><span class="line">&quot;31989&quot;: &quot; assurances&quot;,</span><br><span class="line">&quot;31990&quot;: &quot; ambassadors&quot;,</span><br><span class="line">&quot;31991&quot;: &quot; alum&quot;,</span><br><span class="line">&quot;31992&quot;: &quot; Yee&quot;,</span><br><span class="line">&quot;31993&quot;: &quot; Worse&quot;,</span><br><span class="line">&quot;31994&quot;: &quot; Ware&quot;,</span><br><span class="line">&quot;31995&quot;: &quot; Ves&quot;,</span><br><span class="line">&quot;31996&quot;: &quot; TED&quot;,</span><br><span class="line">&quot;31997&quot;: &quot; surveillance&quot;,</span><br><span class="line">&quot;31998&quot;: &quot; Sequ&quot;,</span><br><span class="line">&quot;31999&quot;: &quot; Schaefer&quot;</span><br></pre></td></tr></table></figure><br><br> <br></p>
<h2 id="2-6-BPE：编码和解码"><a href="#2-6-BPE：编码和解码" class="headerlink" title="2.6 BPE：编码和解码"></a>2.6 BPE：编码和解码</h2><p>根据已给的词汇表和merges实现任意文本和vocabulary里的token IDs的互相转换（encode and decode) 。  </p>
<h3 id="2-6-1-Encoding-文本"><a href="#2-6-1-Encoding-文本" class="headerlink" title="2.6.1 Encoding 文本"></a>2.6.1 Encoding 文本</h3><p>步骤：<br>1 预分词 ：这一步和训练的时候是一样的。<br>2 merge：拿出你的vocabulary和merges，按照merges创建时的顺序应用到预分词(pre-token)上。  </p>
<p>一个merge的具体例子：<br>假设输入是 <code>&#39;the cat ate&#39;</code>，<br>词表： <code>&#123;0: b&#39; &#39;, 1: b&#39;a&#39;, 2: b&#39;c&#39;, 3: b&#39;e&#39;, 4: b&#39;h&#39;, 5: b&#39;t&#39;, 6: b&#39;th&#39;, 7: b&#39; c&#39;, 8: b&#39; a&#39;, 9: b&#39;the&#39;, 10: b&#39; at&#39;&#125;</code><br>合并 (merges) 是： <code>[(b&#39;t&#39;, b&#39;h&#39;), (b&#39; &#39;, b&#39;c&#39;), (b&#39; &#39;, b&#39;a&#39;), (b&#39;th&#39;, b&#39;e&#39;), (b&#39; a&#39;, b&#39;t&#39;)]</code></p>
<p>首先预分词，拆成 <code>[&#39;the&#39;, &#39; cat&#39;, &#39; ate&#39;]</code>，然后对每个预分词应用合并。</p>
<p>第一个词 <code>&#39;the&#39;</code> 最初表示为 <code>[b&#39;t&#39;, b&#39;h&#39;, b&#39;e&#39;]</code>。查看合并列表，按顺序逐个应用可用的合并；第一个是 <code>(b&#39;t&#39;, b&#39;h&#39;)</code>，应用它，<code>[b&#39;t&#39;, b&#39;h&#39;, b&#39;e&#39;]</code>转换为 <code>[b&#39;th&#39;, b&#39;e&#39;]</code>。下一个可用的合并是 <code>(b&#39;th&#39;, b&#39;e&#39;)</code>，将 <code>[b&#39;th&#39;, b&#39;e&#39;]</code>转换为 <code>[b&#39;the&#39;]</code>。最后，再次查看合并列表，没有更多可应用的合并则结束。因此得到 ‘the’ 对应的整数序列为 <code>[9]</code>。<br>同样地：<code>&#39; cat&#39;</code> 在应用合并后表示为 <code>[b&#39; c&#39;, b&#39;a&#39;, b&#39;t&#39;]</code>，变为整数序列 <code>[7, 1, 5]</code>。<br> <code>&#39; ate&#39;</code> 合并后为 <code>[b&#39; at&#39;, b&#39;e&#39;]</code>，变为整数序列 <code>[10, 3]</code>。<br>因此，编码输入字符串的最终结果是 <code>[9, 7, 1, 5, 10, 3]</code>。</p>
<p>3 special_tokens： 你的分词器应当能够恰当地对special tokens进行处理。<br>4 内存方面的考量：文件文本常常不能直接塞进内存里，所以还是需要切成chunks再逐个处理，这样无论文件多大，所需的内存才是恒定的，而不是随着文件大小增长所需的内存线性增大。当然，需要保证切开的时候不能把一个token切开。  </p>
<h3 id="2-6-2-Decoding-文本"><a href="#2-6-2-Decoding-文本" class="headerlink" title="2.6.2 Decoding 文本"></a>2.6.2 Decoding 文本</h3><p>Decoding，显然地，就是直接查找ID对应的token然后把它们连接起来，最后再把这些bytes转换成strings。<br>然而，需要注意的是，不是所有输入的ID都能被转换成合法的Unicode strings，所以我们需要把这些畸形的(malformed)bytes替换成官方的unicode替换字符 <code>U+FFFD</code>（参考：<code>https://en.wikipedia.org/wiki/Specials_(Unicode_block)#Replacement_character</code>）。在bytes.decode函数中，参数errors控制了unicode decoding会怎样处理这些错误的bytes。比如说，用<code>errors = &#39;replace&#39;</code>可以自动地把那些畸形的数据替换成替换标记。  </p>
<h3 id="Problem-tokenizer-实现分词器（15分）"><a href="#Problem-tokenizer-实现分词器（15分）" class="headerlink" title="Problem (tokenizer): 实现分词器（15分）"></a>Problem (tokenizer): 实现分词器（15分）</h3><p>要求：实现一个 Tokenizer class，给它输出一个vocabulary和一个merges列表，可以把text文本encode成整数ID或把整数IDdecode成text文本。同时它还应该支持用户提供的特殊token（如果还不在词表里，就把他们添加到词表里。）课程推荐你的把接口写成如下形式：</p>
<p><code>def __init__(self, vocab, merges, special_tokens=None)</code> 从给定的词表、合并列表和（可选的）特殊 Token 列表构造分词器。接受参数包括：<br><code>vocab: dict[int, bytes]</code><br><code>merges: list[tuple[bytes, bytes]]</code><br><code>special_tokens: list[str]  |  None = None</code>  </p>
<p><code>def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)</code> 一个类方法。从序列化的vocab文件和merges文件（格式与 BPE 训练代码输出的格式相同）构造并返回一个 <code>Tokenizer</code> 实例。参数：<br><code>vocab_filepath: str</code><br><code>merges_ffilepath:  str</code><br><code>special_tokens:  list[str]  |  None = None</code>  </p>
<p><code>def encode(self, text: str) -&gt; list[int]</code>把输入的text文本转换成一列token IDs。  </p>
<p><code>def encode_iterable(self, iterable: Iterable[str]) -&gt; Iterator[int]</code>  给定一个字符串迭代器（比如一个python文件句柄），返回一个有延迟的(lazily)token ID生成器。这是为了那些我们不能直接加载进内存的大型文件准备的。  </p>
<p><code>def decode(self, ids: list[int]) -&gt; str</code> 把token IDs解码成text文本。  </p>
<p>同样地，写好了之后先接入<code>adapters.get_tokenizer</code>，然后<code>uv run pytest tests/test_tokenizer.py</code>；你需要通过所有测试。  </p>
<h4 id="Answer：-1"><a href="#Answer：-1" class="headerlink" title="Answer："></a>Answer：</h4><p>见<br><a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a>  ，<br>本节答案对应截止在# —2.6 Encoding &amp; Decoding—#下方的内容。    </p>
<p>windows上测试依旧出现问题。最好还是在linux上跑吧……<br>通过测试：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">============================= test session starts ==============================</span><br><span class="line">platform linux -- Python <span class="number">3.13</span><span class="number">.3</span>, pytest-<span class="number">8.4</span><span class="number">.1</span>, pluggy-<span class="number">1.6</span><span class="number">.0</span></span><br><span class="line">rootdir: /home/wangzian/Desktop/cs336_assignments_and_notes/assignment1-basics</span><br><span class="line">configfile: pyproject.toml</span><br><span class="line">plugins: jaxtyping-<span class="number">0.3</span><span class="number">.2</span></span><br><span class="line">collected <span class="number">25</span> items                                                             </span><br><span class="line"></span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_empty PASSED</span><br><span class="line">tests/test_tokenizer.py::test_empty_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_single_character PASSED</span><br><span class="line">tests/test_tokenizer.py::test_single_character_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_single_unicode_character PASSED</span><br><span class="line">tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_ascii_string PASSED</span><br><span class="line">tests/test_tokenizer.py::test_ascii_string_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_unicode_string PASSED</span><br><span class="line">tests/test_tokenizer.py::test_unicode_string_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens PASSED</span><br><span class="line">tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_overlapping_special_tokens PASSED</span><br><span class="line">tests/test_tokenizer.py::test_address_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_address_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_german_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_german_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_tinystories_sample_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_tinystories_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_special_token_trailing_newlines PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_iterable_memory_usage PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_memory_usage XFAIL (Tokenizer.e...)</span><br><span class="line"></span><br><span class="line">======================== <span class="number">24</span> passed, <span class="number">1</span> xfailed <span class="keyword">in</span> <span class="number">5.99</span>s =========================</span><br></pre></td></tr></table></figure></p>
<p>linux虚拟机下测试时间5.99s。只要逻辑不要太离谱（比如直接遍历merge）基本时间上不会有太大问题。</p>
<h2 id="2-7-实验"><a href="#2-7-实验" class="headerlink" title="2.7 实验"></a>2.7 实验</h2><p>(a) 从 TinyStories 和 OpenWebText 中各随机抽取 10 份文档。使用你之前训练好的 TinyStories 分词器（词表大小 10K）和 OpenWebText 分词器（词表大小 32K），将这些抽样的文档编码为整数 ID。每个分词器的压缩比（字节/Token）是多少？</p>
<p>(b) 如果你用 TinyStories 的分词器去对 OpenWebText 的样本进行分词，会发生什么？比较其压缩比并（或）从定性角度描述发生的现象。</p>
<p>(c) 估算你的分词器的吞吐量（throughput)（例如：bytes/秒）。以此速度，分词整个Pile数据集（825GB 文本）需要多长时间？</p>
<p>(d) 使用你的 TinyStories 和 OpenWebText 分词器，将各自对应的训练集和开发集编码为整数 Token ID 序列。我们稍后将使用这些数据来训练语言模型。我们建议将这些 Token ID 序列序列化为 uint16 数据类型的 NumPy 数组。为什么选择 uint16 是一个合适的选择？</p>
<h4 id="Answer：-2"><a href="#Answer：-2" class="headerlink" title="Answer："></a>Answer：</h4><p>(a) Tiny 在 4.11左右， owt在4.42左右。可见owt三倍于Tiny的词表尺寸还是可以客观地提高压缩率。<br>(b) 压缩率在3.3左右。可见tiny的词表相对于owt的内容还是有比较大局限。<br>(c) 在Tiny_valid上验证，吞吐量: 5.5023 MB/s。预估分词 Pile (825GB): 42.65 小时。<br>(d)  1 无符号这一点和tokenID是相同的   2 数值范围到65536，比较适配词表大小。</p>
]]></content>
      <categories>
        <category>cs336</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>cs336</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵求导理论：几种表示</title>
    <url>/2026/01/21/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</url>
    <content><![CDATA[<p><strong>提示：如果latex矩阵的渲染有问题，刷新往往能解决问题</strong><br><br></p>
<p>也许我们可以说，张量就是矩阵，梯度就是导数，张量的梯度本质上就是矩阵求导的ml小黑话说法。然而由于一些众所周知的原因，很多人甚至在学习线性代数之前就已经走进了深度学习，初次面对高维张量的求导往往感到混乱。而市面上如The Matrix Cookbook等的工具书又往往太细节，翻译也不尽如人意。本文将以最基础的多元 Logistic 回归（Multinomial Logistic Regression）为例，从大方向上探讨三种不同的推导范式。</p>
<h3 id="例子背景"><a href="#例子背景" class="headerlink" title="例子背景"></a>例子背景</h3><p>我们设定一个典型的分类场景：</p>
<ul>
<li><p><strong>输入向量</strong>：$\mathbf{x} \in \mathbb{R}^{n \times 1}$。</p>
</li>
<li><p><strong>权重矩阵</strong>：$W \in \mathbb{R}^{m \times n}$。</p>
</li>
<li><p><strong>线性输出</strong>：$\mathbf{a} = W\mathbf{x} \in \mathbb{R}^{m \times 1}$。</p>
</li>
<li><p><strong>预测概率</strong>：$\mathbf{p} = \text{softmax}(\mathbf{a}) \in \mathbb{R}^{m \times 1}$，其中第 $i$ 个分量 $p_i = \frac{\exp(a_i)}{\sum_{j=1}^m \exp(a_j)}$。</p>
</li>
<li><p><strong>目标标签</strong>：$\mathbf{y} \in \mathbb{R}^{m \times 1}$，是一个 One-hot 向量（仅有一个元素为 1，其余为 0）。</p>
</li>
<li><p><strong>损失函数</strong>：标量 $l = -\mathbf{y}^\top \log \mathbf{p} = -\sum_{k=1}^m y_k \ln p_k$。</p>
</li>
</ul>
<p>我们的目标是求出梯度矩阵 $\nabla_W l$（即 $\frac{\partial l}{\partial W}$）。</p>
<span id="more"></span>
<p><br> <br> </p>
<h3 id="方法一：逐项求导"><a href="#方法一：逐项求导" class="headerlink" title="方法一：逐项求导"></a>方法一：逐项求导</h3><p>当然了，只要你会求偏导，肯定就会对世界上所有的矩阵求导，只要确定了定义，就可以对应地求导，因为矩阵本质上是标量的排列。我们通过观察单个权重 $W_{ij}$ 的变动如何影响最终损失 $l$，最后再将规律汇总回矩阵形式。</p>
<h4 id="1-链式法则"><a href="#1-链式法则" class="headerlink" title="1. 链式法则"></a>1. 链式法则</h4><p>我们的目标是求 $l$ 对 $W_{ij}$（矩阵 $W$ 第 $i$ 行第 $j$ 列的元素）的偏导数。</p>
<p>根据函数依赖关系：$W_{ij}$ 的变动首先影响线性输出 $a_i$，由于 $a_i$ 是 Softmax 函数的输入，它会进一步影响<strong>所有的</strong>预测概率 $p_k (k=1, \dots, m)$，最后导致损失 $l$ 的变化。</p>
<p>链式法则展开如下：</p>

$$\frac{\partial l}{\partial W_{ij}} = \sum_{k=1}^m \frac{\partial l}{\partial p_k} \cdot \frac{\partial p_k}{\partial a_i} \cdot \frac{\partial a_i}{\partial W_{ij}}$$
<h4 id="2-第一阶段：基础项求导"><a href="#2-第一阶段：基础项求导" class="headerlink" title="2. 第一阶段：基础项求导"></a>2. 第一阶段：基础项求导</h4><ul>
<li><p><strong>计算 $\frac{\partial a_i}{\partial W_{ij}}$</strong>：</p>
<p>  已知 $a_i = \sum_{t=1}^n W_{it}x_t = W_{i1}x_1 + \dots + W_{ij}x_j + \dots$。</p>
<p>  对 $W_{ij}$ 求导得：$\frac{\partial a_i}{\partial W_{ij}} = x_j$。</p>
</li>
<li><p><strong>计算 $\frac{\partial l}{\partial p_k}$</strong>：</p>
<p>  已知 $l = -\sum_{k=1}^m y_k \ln p_k$。</p>
<p>  对 $p_k$ 求导得：$\frac{\partial l}{\partial p_k} = -\frac{y_k}{p_k}$。</p>
</li>
</ul>
<h4 id="3-第二阶段：Softmax-梯度的分类讨论"><a href="#3-第二阶段：Softmax-梯度的分类讨论" class="headerlink" title="3. 第二阶段：Softmax 梯度的分类讨论"></a>3. 第二阶段：Softmax 梯度的分类讨论</h4><p>我们需要计算 $\frac{\partial p_k}{\partial a_i}$。由于 $p_k = \frac{e^{a_k}}{\sum_{j=1}^m e^{a_j}}$，分母包含所有 $a$，因此需要分两种情况：</p>
<ul>
<li><p><strong>情况 A：当 $k = i$ 时（输出下标等于求导下标）</strong></p>
<p>  利用商的求导法则：</p>

$$\frac{\partial p_i}{\partial a_i} = \frac{e^{a_i}(\sum e^a) - e^{a_i}(e^{a_i})}{(\sum e^a)^2} = \frac{e^{a_i}}{\sum e^a} - \left(\frac{e^{a_i}}{\sum e^a}\right)^2 = p_i(1 - p_i)$$
</li>
<li><p><strong>情况 B：当 $k \neq i$ 时（输出下标不等于求导下标）</strong></p>

$$\frac{\partial p_k}{\partial a_i} = \frac{0 \cdot (\sum e^a) - e^{a_k}(e^{a_i})}{(\sum e^a)^2} = -\frac{e^{a_k}}{\sum e^a} \cdot \frac{e^{a_i}}{\sum e^a} = -p_k p_i$$
</li>
</ul>
<h4 id="4-第三阶段：组合求和"><a href="#4-第三阶段：组合求和" class="headerlink" title="4. 第三阶段：组合求和"></a>4. 第三阶段：组合求和</h4><p>将上述结果代入第一步的链式法则总式中，并将求和号按照 $k=i$ 和 $k \neq i$ 进行拆解：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} \frac{\partial l}{\partial p_k} \frac{\partial p_k}{\partial a_i} + \frac{\partial l}{\partial p_i} \frac{\partial p_i}{\partial a_i} \right) \cdot \frac{\partial a_i}{\partial W_{ij}}$$
<p>代入具体的偏导表达式：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} \left(-\frac{y_k}{p_k}\right)(-p_k p_i) + \left(-\frac{y_i}{p_i}\right)p_i(1-p_i) \right) \cdot x_j$$
<p>简化括号内的各项：</p>
<ul>
<li><p>第一部分：$\sum_{k \neq i} y_k p_i$</p>
</li>
<li><p>第二部分：$-y_i(1 - p_i) = -y_i + y_i p_i$</p>
</li>
</ul>
<p>组合后得到：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} y_k p_i + y_i p_i - y_i \right) x_j$$

$$\frac{\partial l}{\partial W_{ij}} = \left( p_i \left( \sum_{k=1}^m y_k \right) - y_i \right) x_j$$
<p>由于标签向量 $\mathbf{y}$ 是 One-hot 的，满足 $\sum_{k=1}^m y_k = 1$，上式最终简化为：</p>

$$\frac{\partial l}{\partial W_{ij}} = (p_i - y_i)x_j$$
<h4 id="5-还原为矩阵形式"><a href="#5-还原为矩阵形式" class="headerlink" title="5. 还原为矩阵形式"></a>5. 还原为矩阵形式</h4><p>我们求出了梯度矩阵中每一个元素 $(i, j)$ 的值。观察发现，这正好对应向量 $(\mathbf{p}-\mathbf{y})$ 的第 $i$ 个元素与向量 $\mathbf{x}$ 的第 $j$ 个元素的乘积。</p>
<p>于是结果为：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
<p><br> <br> <br></p>
<h3 id="方法二：利用全微分"><a href="#方法二：利用全微分" class="headerlink" title="方法二：利用全微分"></a>方法二：利用全微分</h3><p>你都用矩阵了，为什么还要逐项求导呢？矩阵就是矩阵，矩阵是不能不和标量切割的。下面我们将会看到，将矩阵整体不加拆分地求导有一些很美好的性质。实践上，整体的矩阵求导也使我们更少地受维度匹配问题的侵扰，理解上也更为简洁与直觉。</p>
<h4 id="1-整体流程"><a href="#1-整体流程" class="headerlink" title="1. 整体流程"></a>1. 整体流程</h4><p>在标量微积分中，我们知道 $df = f’(x)dx$。全微分法将这一思想推广到矩阵：对于一个以矩阵 $X$ 为自变量的标量函数 $f(X)$，其全微分一定可以表示为如下标准形式：</p>

$$df = \sum_{i,j} \frac{\partial f}{\partial X_{ij}} dX_{ij}$$
<p>为了避免陷入下标求和的泥潭，我们利用迹函数（Trace）。两个同维矩阵 $A$ 和 $B$ 的内积（对应元素相乘再求和）可以写成 $\text{Tr}(A^\top B)$是常用而易得的结论。因此，上述微分式可以写成：</p>

$$df = \text{Tr}(G^\top dX)$$
<p>这里， $G$ 就是梯度矩阵 $\frac{\partial f}{\partial X}$； dX的定义你可以显然地看出。</p>
<p><strong>全微分法的标准操作流程如下：</strong></p>
<ol>
<li><p><strong>写出微分式</strong>：利用微分运算法则，对目标标量 $l$ 直接求微分 $dl$。</p>
</li>
<li><p><strong>套入迹函数</strong>：将 $dl$ 放入迹中，即 $dl = \text{Tr}(dl)$（因为标量的迹等于自身）。</p>
</li>
<li><p><strong>应用Trace Trick</strong>：利用迹函数的循环复用性 $\text{Tr}(ABC) = \text{Tr}(BCA)$，通过左右挪动，将所有的 $dX$（或 $dW$）变动项孤立到式子的最右端。</p>
</li>
<li><p><strong>读取梯度</strong>：将式子整理成 $dl = \text{Tr}(G^\top dW)$ 的标准形态，直接读出 $G$。</p>
</li>
</ol>
<h4 id="2-矩阵微分的运算法则与Trace-trick"><a href="#2-矩阵微分的运算法则与Trace-trick" class="headerlink" title="2. 矩阵微分的运算法则与Trace trick"></a>2. 矩阵微分的运算法则与Trace trick</h4><p>以下的法则需要掌握。当然了，你也可以自己推。</p>
<p>证明基本上可以在张贤达《矩阵分析与应用》中找到。</p>
<ul>
<li><p><strong>微分法则（与标量高度相似）</strong>：</p>
<ul>
<li><p>$d(X + Y) = dX + dY$</p>
</li>
<li><p>$d(XY) = (dX)Y + X(dY)$ （注意保持矩阵先后顺序，不可交换）</p>
</li>
<li><p>$d(X^\top) = (dX)^\top$</p>
</li>
<li><p>$d(\text{exp}(X)) = \text{exp}(X) \odot dX$ （$\odot$ 为逐元素乘积）</p>
</li>
</ul>
</li>
<li><p><strong>Trace的性质</strong>：</p>
<ul>
<li><p><strong>循环移位</strong>：$\text{Tr}(ABC) = \text{Tr}(BCA) = \text{Tr}(CAB)$ </p>
</li>
<li><p><strong>转置不变</strong>：$\text{Tr}(A) = \text{Tr}(A^\top)$。</p>
</li>
<li><p><strong>线性</strong>：$d\text{Tr}(X) = \text{Tr}(dX)$。</p>
</li>
<li><p><strong>矩阵乘法/逐元素乘法交换</strong>：$\text{tr}(A^T(B\odot C)) = \text{tr}((A\odot B)^TC)$，其中A, B, C尺寸相同。两侧都等于$\sum_{i,j}A_{ij}B_{ij}C_{ij}$。</p>
</li>
</ul>
</li>
</ul>
<h4 id="3-多元-Logistic-的梯度"><a href="#3-多元-Logistic-的梯度" class="headerlink" title="3. 多元 Logistic 的梯度"></a>3. 多元 Logistic 的梯度</h4><p>现在，我们可以很容易地利用上述结论和流程解决多元logistic。</p>
<p><strong>第一步：化简目标函数</strong></p>
<p>为了便于求导，利用对数性质将 $l$ 展开：</p>

$$l = -\mathbf{y}^\top \log \text{softmax}(\mathbf{a}) = -\mathbf{y}^\top (\mathbf{a} - \mathbf{1} \ln(\mathbf{1}^\top \exp(\mathbf{a})))$$
<p>这里 $\mathbf{1}^\top \exp(\mathbf{a})$ 实际上就是 Softmax 的分母部分 $\sum e^{a_j}$。</p>
<p><strong>第二步：对 $l$ 求微分 $dl$</strong></p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \mathbf{y}^\top \mathbf{1} \cdot d\ln(\mathbf{1}^\top \exp(\mathbf{a}))$$
<p>由于 $\mathbf{y}$ 是 One-hot 向量，$\mathbf{y}^\top \mathbf{1} = 1$。再对对数项求微分：</p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \frac{1}{\mathbf{1}^\top \exp(\mathbf{a})} d(\mathbf{1}^\top \exp(\mathbf{a}))$$

$$dl = -\mathbf{y}^\top d\mathbf{a} + \frac{1}{\mathbf{1}^\top \exp(\mathbf{a})} \mathbf{1}^\top (\exp(\mathbf{a}) \odot d\mathbf{a})$$
<p>观察发现，$\frac{\exp(\mathbf{a})}{\mathbf{1}^\top \exp(\mathbf{a})}$ 正好是预测概率向量 $\mathbf{p}$。根据向量内积性质，$(\mathbf{p} \odot d\mathbf{a})$ 的各项求和等于 $\mathbf{p}^\top d\mathbf{a}$：</p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \mathbf{p}^\top d\mathbf{a} = (\mathbf{p} - \mathbf{y})^\top d\mathbf{a}$$
<p><strong>第三步：代入变量关系锁定 $dW$</strong></p>
<p>已知 $\mathbf{a} = W\mathbf{x}$，其中 $\mathbf{x}$ 是常数输入，则 $d\mathbf{a} = (dW)\mathbf{x}$。</p>
<p>代入上式：</p>

$$dl = (\mathbf{p} - \mathbf{y})^\top (dW) \mathbf{x}$$
<p><strong>第四步：利用迹函数提取梯度</strong></p>
<p>为了读出梯度，我们将标量 $dl$ 写成迹的形式，并利用 $\text{Tr}(ABC) = \text{Tr}(BCA)$ 变换位置：</p>

$$dl = \text{Tr}( (\mathbf{p} - \mathbf{y})^\top dW \mathbf{x} )$$
<p>将末尾的 $\mathbf{x}$ 移到最前面：</p>

$$dl = \text{Tr}( \mathbf{x} (\mathbf{p} - \mathbf{y})^\top dW )$$
<p>为了匹配标准形式 $dl = \text{Tr}(G^\top dW)$，我们需要把 $dW$ 左边的矩阵整体转置：</p>

$$dl = \text{Tr}( ((\mathbf{p} - \mathbf{y})\mathbf{x}^\top)^\top dW )$$
<p>于是：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
<p><br> <br> <br></p>
<h3 id="三：那么什么是Jacobian-矩阵？"><a href="#三：那么什么是Jacobian-矩阵？" class="headerlink" title="三：那么什么是Jacobian 矩阵？"></a>三：那么什么是Jacobian 矩阵？</h3><p>一般地，数学上利用全微分法可以优雅地解决问题。然而计算机并不擅长处理那些trace trick。所以在 PyTorch （TensorFlow大概也是一样）自动反向传播的底层，实际上，使用的是 Jacobian（雅可比）矩阵。阅读完之后你会发现，这其实并不是一种新的求导方法，只是通过向量化的方法将所有复杂的高维张量通通化成一维，从而形成某种理解维度上的方便。</p>
<h4 id="1-向量化"><a href="#1-向量化" class="headerlink" title="1. 向量化"></a>1. 向量化</h4><p>对于一个 $m \times n$ 的矩阵 $W$，第一步是将其元素按顺序排列成列向量。</p>
<ul>
<li><strong>向量化（Vectorization）</strong>：记作 $\text{vec}(W)$。如果你有一个 $2 \times 2$ 的矩阵，$\text{vec}(W)$ 就是 $4 \times 1$ 向量。</li>
</ul>
<h4 id="2-对Kronecker-积的利用（张量积）"><a href="#2-对Kronecker-积的利用（张量积）" class="headerlink" title="2. 对Kronecker 积的利用（张量积）"></a>2. 对Kronecker 积的利用（张量积）</h4><p>当我们要处理类似 $\mathbf{a} = W\mathbf{x}$ 这样的矩阵乘法求导时，数学上有：</p>

$$\text{vec}(AXB) = (B^\top \otimes A)\text{vec}(X)$$
<p>以$\mathbf{a} = W\mathbf{x}$ 为例。为了套用公式，我们补上单位阵：$\mathbf{a} = I_m W \mathbf{x}$。</p>
<p>利用上面的公式，它等价于：</p>

$$\mathbf{a} = (\mathbf{x}^\top \otimes I_m) \text{vec}(W)$$
<p>这意味着，$\mathbf{a}$ 是由 $\text{vec}(W)$ 经过一个系数矩阵 $(\mathbf{x}^\top \otimes I_m)$ 线性变换得到的。</p>
<h4 id="3-逐步推导"><a href="#3-逐步推导" class="headerlink" title="3. 逐步推导"></a>3. 逐步推导</h4><p>根据向量化的链式法则：</p>

$$\frac{\partial l}{\partial \text{vec}(W)} = \underbrace{\frac{\partial l}{\partial \mathbf{p}}}_{1 \times m} \cdot \underbrace{\frac{\partial \mathbf{p}}{\partial \mathbf{a}}}_{m \times m} \cdot \underbrace{\frac{\partial \mathbf{a}}{\partial \text{vec}(W)}}_{m \times mn}$$
<p><strong>第一步：求 $\frac{\partial l}{\partial \mathbf{p}}$</strong></p>
<p>损失函数 $l$ 对预测概率向量 $\mathbf{p}$ 的偏导，根据导数的定义，是一个行向量。这个行向量就是 $l$ 关于 $\mathbf{p}$ 的 Jacobian：</p>

$$J_{l,p} = \left[ -\frac{y_1}{p_1}, -\frac{y_2}{p_2}, \dots, -\frac{y_m}{p_m} \right]$$
<p><strong>第二步：求 $\frac{\partial \mathbf{p}}{\partial \mathbf{a}}$（Softmax 的雅可比矩阵）</strong></p>
<p>这里a和p都是向量，所以直接用方法一中的结论。</p>
<p>结果是一个 $m \times m$ 的矩阵，其形式为 $\text{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top$。即：</p>

$$J_{p,a} = \begin{bmatrix} p_1(1-p_1) & -p_1 p_2 & \dots & -p_1 p_m \\ -p_2 p_1 & p_2(1-p_2) & \dots & -p_2 p_m \\ \vdots & \vdots & \ddots & \vdots \\ -p_m p_1 & -p_m p_2 & \dots & p_m(1-p_m) \end{bmatrix}$$
<p><strong>第三步：求 $\frac{\partial \mathbf{a}}{\partial \text{vec}(W)}$</strong></p>
<p>根据在第 2 点提到的线性转换关系，这一项直接就是那个系数矩阵：</p>

$$\mathbf{x}^\top \otimes I_m$$
<p>即：</p>

$$J_{a,W} = \frac{\partial \mathbf{a}}{\partial \text{vec}(W)} = \mathbf{x}^\top \otimes I_m$$
<p>展开来看，就是 $n$ 个 $m \times m$ 的块：</p>

$$J_{a,W} = \begin{bmatrix} x_1 I_m & x_2 I_m & \dots & x_n I_m \end{bmatrix}$$
<h4 id="4-组合"><a href="#4-组合" class="headerlink" title="4. 组合"></a>4. 组合</h4><p>我们将这几项乘起来。</p>

$$\frac{\partial l}{\partial \text{vec}(W)} = (\mathbf{p} - \mathbf{y})^\top (\mathbf{x}^\top \otimes I_m)$$
<p>根据矩阵运算规则，这个结果实际上就是：</p>

$$\text{vec}((\mathbf{p} - \mathbf{y})\mathbf{x}^\top)^\top$$
<p>这是一个 $1 \times mn$ 的行向量。</p>
<h4 id="5-还原形状（逆向量化）"><a href="#5-还原形状（逆向量化）" class="headerlink" title="5. 还原形状（逆向量化）"></a>5. 还原形状（逆向量化）</h4><p>最后一步，我们将这个 $1 \times mn$ 的长向量重新折叠回 $m \times n$ 的矩阵形状。</p>
<p>即：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数学</tag>
        <tag>矩阵理论</tag>
      </tags>
  </entry>
  <entry>
    <title>cs336：Assignment1-basics-chapter3</title>
    <url>/2026/01/24/Assignment1-basics-chapter3/</url>
    <content><![CDATA[<p>本章带你编写Transformer，并且最后带你简单算一遍内存和FLOPs，但不包括训练和生成（在4-6章）。</p>
<p>本文少量mathcal相关的markdown无法被github原生渲染器渲染，你可以选择跳过它们或把此.md文件尝试在其他软件中打开。</p>
<p>本课程对transformer相关的算法本身既不全面也不清楚。如果读者对相关算法并不熟悉，笔者推荐去选择性地完成cs231n中神经网络和transformer相关的作业；内含充足的scaffolding和数学原理内容（主要包括反向传播的推导），能够在带你速通相关算法的同时，让你对这些神经网络的整个过程有更良好的认识。  </p>
<p>如果你熟悉相关算法，chapter3和接下来的章节要比chapter2简单得多。你大可以vibecode softmax之类你非常熟悉的任务，在数个小时内完成ch.3。  </p>
<h1 id="3-Transformer-架构"><a href="#3-Transformer-架构" class="headerlink" title="3  Transformer 架构"></a>3  Transformer 架构</h1><p>一个语言模型（LM）接受一批整数token ID序列作为输入（即，形状为<code>(batch_size, sequence_length)</code> 的 <code>torch.Tensor</code>），并返回词表上的（批量）归一化概率分布（即形状为 <code>(batch_size, sequence_length, vocab_size)</code> 的 PyTorch 张量），其中预测的分布是针对每个输入 token 的下一个词的（即，对词典里的每一个词得到一个score）。<br>在训练语言模型（training）时，我们使用这些下文预测来计算实际下一个词与预测下一个词之间的交叉熵损失。  （然后通过反向传播减小这些损失）<br>在推理过程中利用语言模型生成文本（validation/testing）时，我们取最后一个时间步（即序列中的最后一项）预测的下文概率分布来生成序列中的下一个 token（例如，通过选取概率最高的 token、从分布中采样等），将这个 token 添加到输入序列中，并重复此过程。<br>这一部分的作业中，你将从零开始构建这个模型。我们将先从模型的高层描述开始，然后逐步介绍各个组件。</p>
<h2 id="3-1-Transformer-语言模型（LM）"><a href="#3-1-Transformer-语言模型（LM）" class="headerlink" title="3.1 Transformer 语言模型（LM）"></a>3.1 Transformer 语言模型（LM）</h2><p>给定一个 token ID 序列，Transformer 语言模型使用输入嵌入（input embedding，通过它将离散的整数ID映射到高维连续空间，转换成稠密向量），将嵌入后的 token 通过 <code>num_layers</code> 个 Transformer 块，然后应用一个学习到的线性投影（“输出嵌入”或“LM head”）来产生预测的下一 token 的未归一化的得分（logits）。<br>示意图请参见图 1。  </p>
<h3 id="Figure-1"><a href="#Figure-1" class="headerlink" title="Figure 1"></a>Figure 1</h3><p><img src="./images/image.png" alt="|324x370"></p>
<h3 id="3-1-1-Token-嵌入"><a href="#3-1-1-Token-嵌入" class="headerlink" title="3.1.1 Token 嵌入"></a>3.1.1 Token 嵌入</h3><p>第一步，Transformer 将（批量的）token ID 序列嵌入为包含 token 身份信息的向量序列（图 1 中的红色块）。</p>
<p>具体来说，给定一个 token ID 序列，Transformer 语言模型使用一个 token 嵌入层来生成一个向量序列。每个嵌入层接收一个形状为 <code>(batch_size, sequence_length)</code> 的整数张量，并产生一个形状为 <code>(batch_size, sequence_length, d_model)</code> 的向量序列。(也即，把每个数字索引变成一个长度为<code>d_model</code>的向量。)</p>
<h3 id="3-1-2-Pre-norm-Transformer-块"><a href="#3-1-2-Pre-norm-Transformer-块" class="headerlink" title="3.1.2 Pre-norm Transformer 块"></a>3.1.2 Pre-norm Transformer 块</h3><p>嵌入之后，激活值由几个结构相同的神经网络处理。一个标准的纯解码器（decoder-only）Transformer 语言模型由 <code>num_layers</code> 个相同的层（通常称为 Transformer “块（blocks）”）组成。每个 Transformer 块接收形状为 <code>(batch_size, sequence_length, d_model)</code> 的输入，并返回形状为 <code>(batch_size, sequence_length, d_model)</code> 的输出。每个块通过自注意力机制（self-attention）聚合整个序列的信息，并通过前馈层（feed-forward layers）对其进行非线性转换。</p>
<h2 id="3-2-输出归一化与嵌入"><a href="#3-2-输出归一化与嵌入" class="headerlink" title="3.2 输出归一化与嵌入"></a>3.2 输出归一化与嵌入</h2><p>在经过 <code>num_layers</code> 个 Transformer 块之后，我们将取最终的激活值并将其转化成词表上的分布。</p>
<p>我们将实现“Pre-norm” Transformer 块（详见 §3.5），这额外要求在最后一个 Transformer 块之后使用层归一化（Layer Normalization，详见下文），以确保其输出被正确缩放。</p>
<p>在此归一化之后，我们将使用一个标准的学习线性变换，将 Transformer 块的输出转换为预测的下一 token 的 logits（例如，参见 Radford 等人 [2018] 的公式 2）。</p>
<h2 id="3-3-注：批处理、Einsum-与高效计算"><a href="#3-3-注：批处理、Einsum-与高效计算" class="headerlink" title="3.3 注：批处理、Einsum 与高效计算"></a>3.3 注：批处理、Einsum 与高效计算</h2><p>在整个 Transformer 中，我们将对许多类似批次的输入执行相同的计算。比如：</p>
<p>1 批次元素：我们对每个批次元素应用相同的 Transformer 前向操作。<br>2 序列长度：像 RMSNorm 和前馈层这样的“位置级（position-wise）”操作对序列的每个位置执行完全相同的操作。<br>3 注意力头：在“多头”注意力操作中，注意力操作跨注意力头进行批处理。</p>
<p>拥有一个符合人体工程学的方式来执行这些操作是很有用的，这样既能充分利用 GPU，又易于阅读和理解。许多 PyTorch 操作可以在张量开头接收多余的“类批次（batch-like）”维度，并高效地跨这些维度重复/广播操作。</p>
<p>例如，假设我们正在做一个位置级的批量操作。我们有一个形状为 <code>(batch_size, sequence_length, d_model)</code> 的“数据张量” $D$，我们想对一个形状为 <code>(d_model, d_model)</code> 的矩阵 $A$ 进行批量向量-矩阵乘法。在这种情况下，<code>D @ A</code> 将执行批量矩阵乘法，这是 PyTorch 中一个高效的原语（primitive），其中 <code>(batch_size, sequence_length)</code> 维度被批量处理。</p>
<p>因此，假设你的函数可能会被赋予额外的类批次维度，并将这些维度保持在 PyTorch 形状的开头是有帮助的。为了以这种方式组织张量以便进行批处理，可能需要使用 <code>view</code>、<code>reshape</code> 和 <code>transpose</code> 的多个步骤来调整它们。这可能有点痛苦，而且通常很难读懂代码在做什么以及张量的形状是什么。</p>
<p>一个更符合人体工程学的选择是在 <code>torch.einsum</code> 中使用 einsum 符号，或者使用像 <code>einops</code> 或 <code>einx</code> 这样与框架无关的库。两个核心操作是 <code>einsum</code>（可以对输入张量的任意维度进行张量收缩）和 <code>rearrange</code>（可以对任意维度进行重排、连接和拆分）。事实证明，机器学习中几乎所有的操作都是维度操作和张量收缩的某种组合，偶尔带有（通常是逐元素的）非线性函数。这意味着使用 einsum 符号可以使你的很多代码更具可读性和灵活性。</p>
<p>我们强烈建议在课程中学习并使用 einsum 符号。以前没有接触过 einsum 符号的学生应该使用 <code>einops</code>（文档见<a href="https://einops.rocks/1-einops-basics/">此处</a>），而已经熟悉 <code>einops</code> 的学生应该学习更通用的 <code>einx</code>（见 <a href="https://einx.readthedocs.io/en/stable/gettingstarted/tutorial_notation.html">https://einx.readthedocs.io/en/stable/gettingstarted/tutorial_notation.html</a> ）。这两个包都已经安装在我们提供的环境中。</p>
<p>（einops的文档还是写得很清楚的，推荐直接阅读）</p>
<p>这里我们给出一些如何使用 einsum 符号的例子。这些是对 <code>einops</code> 文档的补充，你应该先阅读文档。  </p>
<h3 id="例-einstein-example1-使用-einops-einsum-进行批量矩阵乘法"><a href="#例-einstein-example1-使用-einops-einsum-进行批量矩阵乘法" class="headerlink" title="例 (einstein_example1): 使用 einops.einsum 进行批量矩阵乘法"></a>例 (einstein_example1): 使用 einops.einsum 进行批量矩阵乘法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, einsum</span><br><span class="line"></span><br><span class="line"><span class="comment">## 基础实现</span></span><br><span class="line">Y = D @ A.T</span><br><span class="line"><span class="comment"># 很难看出输入和输出的形状及其含义。</span></span><br><span class="line"><span class="comment"># D 和 A 可以有哪些形状？其中是否有任何形状会导致意外行为？</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Einsum 是自文档化的且robust的</span></span><br><span class="line"><span class="comment"># D A -&gt; Y</span></span><br><span class="line">Y = einsum(D, A, <span class="string">&quot;batch sequence d_in, d_out d_in -&gt; batch sequence d_out&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 或者，一个 D 可以拥有任意前导维度但 A 受约束的批量版本。</span></span><br><span class="line">Y = einsum(D, A, <span class="string">&quot;... d_in, d_out d_in -&gt; ... d_out&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="例-einstein-example2-使用-einops-rearrange-进行广播操作"><a href="#例-einstein-example2-使用-einops-rearrange-进行广播操作" class="headerlink" title="例 (einstein_example2): 使用 einops.rearrange 进行广播操作"></a>例 (einstein_example2): 使用 einops.rearrange 进行广播操作</h3><p>我们有一批图像，对于每张图像，我们想根据某些缩放因子生成 10 个变暗的版本：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">images = torch.randn(<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>) <span class="comment"># (batch, height, width, channel)</span></span><br><span class="line">dim_by = torch.linspace(start=<span class="number">0.0</span>, end=<span class="number">1.0</span>, steps=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重塑并相乘</span></span><br><span class="line">dim_value = rearrange(dim_by, <span class="string">&quot;dim_value -&gt; 1 dim_value 1 1 1&quot;</span>)</span><br><span class="line">images_rearr = rearrange(images, <span class="string">&quot;b height width channel -&gt; b 1 height width channel&quot;</span>)</span><br><span class="line">dimmed_images = images_rearr * dim_value</span><br><span class="line"></span><br><span class="line"><span class="comment">## 或者一步：</span></span><br><span class="line">dimmed_images = einsum(</span><br><span class="line">    images, dim_by,</span><br><span class="line">    <span class="string">&quot;batch height width channel, dim_value -&gt; batch dim_value height width channel&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>值得注意的是，虽然 <code>einops</code> 拥有大量的支持，但 <code>einx</code> 尚未经过充分的实战检验。如果你发现 <code>einx</code> 有任何限制或错误，请随时退回到使用 <code>einops</code> 配合一些基础的 PyTorch 操作。</p>
<h3 id="例-einstein-example3-使用-einops-rearrange-进行像素混合"><a href="#例-einstein-example3-使用-einops-rearrange-进行像素混合" class="headerlink" title="例 (einstein_example3): 使用 einops.rearrange 进行像素混合"></a>例 (einstein_example3): 使用 einops.rearrange 进行像素混合</h3><p>假设我们有一批图像，表示形状为 <code>(batch, height, width, channel)</code> 的张量，我们想对图像的所有像素线性变换，但这种变换应该在每个通道上独立进行。我们的线性变换表示为一个形状为 <code>(height × width, height × width)</code> 的矩阵 $B$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">channels_last = torch.randn(<span class="number">64</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>) <span class="comment"># (batch, height, width, channel)</span></span><br><span class="line">B = torch.randn(<span class="number">32</span>*<span class="number">32</span>, <span class="number">32</span>*<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重新排列图像张量以便跨所有像素进行混合</span></span><br><span class="line">channels_last_flat = channels_last.view(</span><br><span class="line">    -<span class="number">1</span>, channels_last.size(<span class="number">1</span>) * channels_last.size(<span class="number">2</span>), channels_last.size(<span class="number">3</span>)</span><br><span class="line">)</span><br><span class="line">channels_first_flat = channels_last_flat.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">channels_first_flat_transformed = channels_first_flat @ B.T</span><br><span class="line">channels_last_flat_transformed = channels_first_flat_transformed.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相反，使用 einops：</span></span><br><span class="line">height = width = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## rearrange 取代了笨拙的 torch view + transpose</span></span><br><span class="line">channels_first = rearrange(</span><br><span class="line">    channels_last,</span><br><span class="line">    <span class="string">&quot;batch height width channel -&gt; batch channel (height width)&quot;</span></span><br><span class="line">)</span><br><span class="line">channels_first_transformed = einsum(</span><br><span class="line">    channels_first, B,</span><br><span class="line">    <span class="string">&quot;batch channel pixel_in, pixel_out pixel_in -&gt; batch channel pixel_out&quot;</span></span><br><span class="line">)</span><br><span class="line">channels_last_transformed = rearrange(</span><br><span class="line">    channels_first_transformed,</span><br><span class="line">    <span class="string">&quot;batch channel (height width) -&gt; batch height width channel&quot;</span>,</span><br><span class="line">    height=height, width=width</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 或者，如果你想挑战一下：使用 einx.dot（einops.einsum 的 einx 等效项）一步到位</span></span><br><span class="line">height = width = <span class="number">32</span></span><br><span class="line">channels_last_transformed = einx.dot(</span><br><span class="line">    <span class="string">&quot;batch row_in col_in channel, (row_out col_out) (row_in col_in) &quot;</span></span><br><span class="line">    <span class="string">&quot;-&gt; batch row_out col_out channel&quot;</span>,</span><br><span class="line">    channels_last, B,</span><br><span class="line">    col_in=width, col_out=width</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里的第一种实现可以通过在前后添加注释来指示输入和输出的形状，但这很笨拙且容易出错。使用 einsum 符号，代码实现即文档！</p>
<p>Einsum 符号可以处理任意的输入批次维度，其核心优势在于自文档化。在编写使用 einsum 符号的代码时，输入和输出张量的相关形状要清晰得多。对于其余张量，你可以考虑使用张量类型提示（Tensor type hints），例如使用 <code>jaxtyping</code> 库（并不局限于 Jax）。</p>
<p>我们将在作业 2 中更多地讨论使用 einsum 符号的性能影响，但目前请记住，它们几乎总是比替代方案更好。</p>
<h3 id="3-3-1-数学符号与内存排序"><a href="#3-3-1-数学符号与内存排序" class="headerlink" title="3.3.1 数学符号与内存排序"></a>3.3.1 数学符号与内存排序</h3><p>许多机器学习论文在记法中使用行向量，这产生的表示形式能很好地适配 NumPy 和 PyTorch 默认使用的行优先（row-major）内存排序。使用行向量时，线性变换看起来像：</p>

$$y = xW^\top, \quad (1)$$
<p>其中 W 为行优先矩阵 $W \in \mathbb{R}^{d_{out} \times d_{in}}$，x 为行向量 $x \in \mathbb{R}^{1 \times d_{in}}$。</p>
<p>在向量代数中，使用列向量通常更常见，线性变换看起来像：</p>

$$y = Wx, \quad (2)$$
<p>给定一个行优先矩阵 $W \in \mathbb{R}^{d_{out} \times d_{in}}$ 和列向量 $x \in \mathbb{R}^{d_{in}}$。我们在本次作业的数学记法中将使用列向量，因为这种方式通常更容易理解数学原理。你应该记住，如果你想使用普通的矩阵乘法记法，你必须按照行向量惯例来应用矩阵，因为 PyTorch 使用行优先内存排序。如果你在矩阵操作中使用 einsum，这就不再是一个问题。</p>
<h2 id="3-4-基础构建块：线性层与嵌入模块"><a href="#3-4-基础构建块：线性层与嵌入模块" class="headerlink" title="3.4 基础构建块：线性层与嵌入模块"></a>3.4 基础构建块：线性层与嵌入模块</h2><h3 id="3-4-1-参数初始化"><a href="#3-4-1-参数初始化" class="headerlink" title="3.4.1 参数初始化"></a>3.4.1 参数初始化</h3><p>有效地训练神经网络通常需要仔细初始化模型参数——不良的初始化可能导致不理想的行为，如梯度消失或爆炸。Pre-norm Transformer 对初始化异常健壮，但初始化仍会对训练速度和收敛产生显著影响。由于本次作业已经很长了，我们将把细节留给作业 3，并给你一些在大多数情况下都表现良好的近似初始化方案。目前，请使用：</p>
<p>线性层权重：$\mathcal{N}(\mu = 0, \sigma^2 = \frac{2}{d_{in} + d_{out}})$，在 $[-3\sigma, 3\sigma]$ 处截断。</p>
<p>嵌入层：$\mathcal{N}(\mu = 0, \sigma^2 = 1)$，在 $[-3, 3]$ 处截断。</p>
<p>RMSNorm：1</p>
<p>你应该使用 <code>torch.nn.init.trunc_normal_</code> 来初始化截断正态分布的权重。</p>
<h3 id="3-4-2-线性模块"><a href="#3-4-2-线性模块" class="headerlink" title="3.4.2 线性模块"></a>3.4.2 线性模块</h3><p>线性层是 Transformer 以及整个神经网络的基础构建块。首先，你将实现自己的 <code>Linear</code> 类，该类继承自 <code>torch.nn.Module</code> 并执行线性变换：</p>

$$y = Wx. \quad (3)$$
<p>请注意，按照大多数现代LLM的做法，我们不包含偏置（bias）项。</p>
<h4 id="Problem-linear-实现线性模块-1-分"><a href="#Problem-linear-实现线性模块-1-分" class="headerlink" title="Problem (linear): 实现线性模块 (1 分)"></a>Problem (linear): 实现线性模块 (1 分)</h4><p>实现一个继承自 <code>torch.nn.Module</code> 的 <code>Linear</code> 类，并执行线性变换。你的实现应遵循 PyTorch 内置 <code>nn.Linear</code> 模块的接口，但不包含偏置参数。我们建议使用以下接口：</p>
<p>Python</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def __init__(self, in_features, out_features, device=None, dtype=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    构造一个线性变换模块。</span><br><span class="line">    in_features: 输入的最后一个维度</span><br><span class="line">    out_features: 输出的最后一个维度</span><br><span class="line">    device: 存储参数的设备</span><br><span class="line">    dtype: 参数的数据类型</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</span><br><span class="line">    &quot;&quot;&quot;对输入应用线性变换。&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<p>请确保：<br>1 继承 <code>nn.Module</code>。<br>2 调用超类的构造函数。<br>3 由于内存排序原因，将参数构造并存储为 $W$（而不是 $W^\top$），并将其放入 <code>nn.Parameter</code> 中。<br>4 当然，不要使用 <code>nn.Linear</code> 或 <code>nn.functional.linear</code>。</p>
<p>对于初始化，请使用上述设置，并结合 <code>torch.nn.init.trunc_normal_</code> 来初始化权重。</p>
<p>要测试你的 <code>Linear</code> 模块，请实现位于 <code>[adapters.run_linear]</code> 的测试适配器。适配器应将给定的权重加载到你的 <code>Linear</code> 模块中。你可以为此使用 <code>Module.load_state_dict</code>。然后，运行 <code>uv run pytest -k test_linear</code>。</p>
<h4 id="Answer："><a href="#Answer：" class="headerlink" title="Answer："></a>Answer：</h4><p>见transformer_model.Linear和adapter.run_linear。</p>
<h3 id="3-4-3-嵌入模块"><a href="#3-4-3-嵌入模块" class="headerlink" title="3.4.3 嵌入模块"></a>3.4.3 嵌入模块</h3><p>如上所述，Transformer 的第一层是一个嵌入层，它将整数 token ID 映射到维度为 <code>d_model</code> 的向量空间。我们将实现一个自定义的 <code>Embedding</code> 类，该类继承自 <code>torch.nn.Module</code>（因此不应使用 <code>nn.Embedding</code>）。<code>forward</code> 方法应通过使用形状为 <code>(batch_size, sequence_length)</code> 的 <code>torch.LongTensor</code> token ID 在形状为 <code>(vocab_size, d_model)</code> 的嵌入矩阵中进行索引，从而为每个 token ID 选择嵌入向量。</p>
<h4 id="Problem-embedding-实现嵌入模块-1-分"><a href="#Problem-embedding-实现嵌入模块-1-分" class="headerlink" title="Problem (embedding): 实现嵌入模块 (1 分)"></a>Problem (embedding): 实现嵌入模块 (1 分)</h4><p>实现继承自 torch.nn.Module 并执行嵌入查找（lookup）的 Embedding 类。你的实现应遵循 PyTorch 内置 nn.Embedding 模块的接口。我们建议使用以下接口：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>)</span><br></pre></td></tr></table></figure>
<p>构造一个嵌入模块。该函数应接受以下参数：</p>
<p>1 num_embeddings: int 词表大小<br>2 embedding_dim: int 嵌入向量的维度，即 d_model<br>3 device: torch.device | None = None 存储参数的设备<br>4 dtype: torch.dtype | None = None 参数数据类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, token_ids: torch.Tensor</span>) -&gt; torch.Tensor</span><br></pre></td></tr></table></figure>
<p>查找给定 token ID 的嵌入向量。</p>
<p>确保：</p>
<p>1 继承 nn.Module<br>2 调用超类构造函数<br>3 将你的嵌入矩阵初始化为一个 nn.Parameter<br>4 存储嵌入矩阵时，确保 d_model 是最后一个维度<br>5 当然，不要使用 nn.Embedding 或 nn.functional.embedding</p>
<p>再次强调，使用上文提到的设置进行初始化，并使用 torch.nn.init.trunc_normal_ 来初始化权重。</p>
<p>要测试你的实现，请实现位于 [adapters.run_embedding] 的测试适配器。然后，运行 <code>uv run pytest -k test_embedding</code>。</p>
<h5 id="Answer：-1"><a href="#Answer：-1" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.Embedding和adapter.run_embedding。</p>
<h2 id="3-5-Pre-Norm-Transformer-块"><a href="#3-5-Pre-Norm-Transformer-块" class="headerlink" title="3.5 Pre-Norm Transformer 块"></a>3.5 Pre-Norm Transformer 块</h2><p>每个 Transformer 块有两个子层：多头自注意力机制和位置级前馈网络 (Vaswani et al., 2017, section 3.1)。</p>
<p>在原始的 Transformer 论文中，模型在两个子层周围各使用了一个残差连接，随后进行层归一化（layer normalization）。这种架构通常被称为“post-norm” Transformer，因为层归一化被应用于子层的输出。<br>然而，多项研究发现，将层归一化从每个子层的输出移动到每个子层的输入（并在最后一个 Transformer 块之后增加一个额外的层归一化）可以提高 Transformer 训练的稳定性 [Nguyen and Salazar, 2019, Xiong et al., 2020] —— 参见图 2 了解这种“pre-norm” Transformer 块的视觉表示。随后，每个 Transformer 块子层的输出通过残差连接加到子层输入上 (Vaswani et al., 2017, section 5.4)。pre-norm 的一种直觉是，从输入嵌入到 Transformer 的最终输出之间存在一条没有经过任何归一化的清晰“残差流”，据称这可以改善梯度流动。这种 pre-norm Transformer 现在已成为当今语言模型（例如 GPT-3, LLaMA, PaLM 等）使用的标准，因此我们将实现这一变体。我们将逐一介绍 pre-norm Transformer 块的每个组件，并按顺序实现它们。  </p>
<h3 id="Figure-2"><a href="#Figure-2" class="headerlink" title="Figure 2"></a>Figure 2</h3><p><img src="./images/image-2.png" alt="|324x438"></p>
<h3 id="3-5-1-均方根层归一化-Layer-Normalization"><a href="#3-5-1-均方根层归一化-Layer-Normalization" class="headerlink" title="3.5.1 均方根层归一化 (Layer Normalization)"></a>3.5.1 均方根层归一化 (Layer Normalization)</h3><p>Vaswani et al. [2017] 的原始 Transformer 实现使用层归一化 [Ba et al., 2016] 来归一化激活值。参考 Touvron et al. [2023]，我们将使用均方根层归一化 (RMSNorm; Zhang and Sennrich, 2019, equation 4) 进行层归一化。给定一个激活值向量 $a \in \mathbb{R}^{d_{model}}$，RMSNorm 将按如下方式缩放每个激活值 $a_i$：</p>

$$RMSNorm(a_i) = \frac{a_i}{RMS(a)} g_i, \quad (4)$$
<p>其中 $RMS(a) = \sqrt{\frac{1}{d_{model}} \sum_{i=1}^{d_{model}} a_i^2 + \varepsilon}$。这里，$g_i$ 是一个可学习的“增益（gain）”参数（总共有 d_model 个此类参数），$\varepsilon$ 是一个通常固定为 1e-5 的超参数。</p>
<p>你应该将输入向上转换（upcast）为 torch.float32，以防止对输入求平方时发生溢出。总的来说，你的 forward 方法应该如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">in_dtype = x.dtype</span><br><span class="line">x = x.to(torch.float32)</span><br><span class="line"><span class="comment"># 此处编写执行 RMSNorm 的代码</span></span><br><span class="line">...</span><br><span class="line">result = ...</span><br><span class="line"><span class="comment"># 以原始数据类型返回结果</span></span><br><span class="line"><span class="keyword">return</span> result.to(in_dtype)</span><br></pre></td></tr></table></figure>
<h4 id="Problem-rmsnorm-均方根层归一化-1-分"><a href="#Problem-rmsnorm-均方根层归一化-1-分" class="headerlink" title="Problem (rmsnorm): 均方根层归一化 (1 分)"></a>Problem (rmsnorm): 均方根层归一化 (1 分)</h4><p>交付成果：将 RMSNorm 实现为一个 torch.nn.Module。我们建议使用以下接口：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>)</span><br></pre></td></tr></table></figure>
<p>构造 RMSNorm 模块。该函数应接受以下参数：</p>
<p>1 d_model: int 模型的隐藏维度<br>2 eps: float = 1e-5 用于数值稳定性的 epsilon 值<br>3 device: torch.device | None = None 存储参数的设备<br>4 dtype: torch.dtype | None = None 参数的数据类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor</span><br></pre></td></tr></table></figure>
<p>处理形状为 (batch_size, sequence_length, d_model) 的输入张量，并返回相同形状的张量。</p>
<p>注意：记住在执行归一化之前将输入向上转换为 torch.float32（随后转换回原始数据类型），如上文所述。</p>
<p>要测试你的实现，请实现位于 [adapters.run_rmsnorm] 的测试适配器。然后，运行 uv run pytest -k test_rmsnorm。</p>
<h5 id="Answer：-2"><a href="#Answer：-2" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.RMSNorm和adapter.run_rmsnorm。</p>
<h3 id="3-5-2-逐位置前馈网络"><a href="#3-5-2-逐位置前馈网络" class="headerlink" title="3.5.2 逐位置前馈网络"></a>3.5.2 逐位置前馈网络</h3><p>在原始的 Transformer 论文中（Vaswani et al. [2017] 第 3.3 节），Transformer 前馈网络由两个线性变换组成，中间夹有一个 ReLU 激活函数（ReLU(x) = max(0, x)）。内部前馈层的维度通常是输入维度的 4 倍。</p>
<p>然而，与这种原始设计相比，现代语言模型倾向于引入两个主要的改进：它们使用另一种激活函数并采用门控机制（gating mechanism）。具体来说，我们将实现 Llama 3 [Grattafiori et al., 2024] 和 Qwen 2.5 [Yang et al., 2024] 等大语言模型中所采用的 SwiGLU 激活函数，它结合了 SiLU（通常被称为 Swish）激活函数和一种称为门控线性单元（GLU, Gated Linear Unit）的门控机制。遵循 PaLM [Chowdhery et al., 2022] 和 LLaMA [Touvron et al., 2023] 以来大多数现代大语言模型的做法，我们也省略了线性层中有时会使用的偏置项。</p>
<p>SiLU 或 Swish 激活函数 [Hendrycks and Gimpel, 2016, Elfwing et al., 2017] 定义如下：</p>

$$SiLU(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}} \quad (5)$$
<p>如图 3 所示，SiLU 激活函数与 ReLU 激活函数相似，但在零点处是平滑的。</p>
<p>门控线性单元（GLUs）最初由 Dauphin et al. [2017] 定义为经过 sigmoid 函数的线性变换与另一个线性变换的逐元素乘积：</p>

$$GLU(x, W_1, W_2) = \sigma(W_1x) \odot W_2x, \quad (6)$$
<p>其中 $\odot$ 表示逐元素相乘。门控线性单元被认为能够通过为梯度提供线性路径同时保留非线性能力，从而“减轻深度架构中的梯度消失问题”。</p>
<p>将 SiLU/Swish 和 GLU 结合在一起，我们得到了 SwiGLU，我们将把它用于我们的前馈网络：</p>

$$FFN(x) = SwiGLU(x, W_1, W_2, W_3) = W_2(SiLU(W_1x) \odot W_3x), \quad (7)$$
<p>其中 $x \in \mathbb{R}^{d_{model}}$，$W_1, W_3 \in \mathbb{R}^{d_{ff} \times d_{model}}$，$W_2 \in \mathbb{R}^{d_{model} \times d_{ff}}$，并且按照惯例，$d_{ff} = \frac{8}{3}d_{model}$。</p>
<p>Shazeer [2020] 首先提出了将 SiLU/Swish 激活与 GLU 结合，并进行实验证明 SwiGLU 在语言建模任务上的表现优于 ReLU 和 SiLU（无门控）等基准模型。在本次作业的后续部分，你将对比 SwiGLU 和 SiLU。尽管我们为这些组件提到了一些启发式论据（且相关论文提供了更多支持证据），但保持经验式的观念也是有益的：Shazeer 论文中有一句如今著名的名言——</p>
<p>我们无法解释为什么这些架构似乎有效；我们将它们的成功与其他所有事情一样，归功于神赐的仁慈。</p>
<h4 id="Problem-positionwise-feedforward-实现逐位置前馈网络-2-分"><a href="#Problem-positionwise-feedforward-实现逐位置前馈网络-2-分" class="headerlink" title="Problem (positionwise_feedforward): 实现逐位置前馈网络 (2 分)"></a>Problem (positionwise_feedforward): 实现逐位置前馈网络 (2 分)</h4><p>实现由 SiLU 激活函数和 GLU 组成的 SwiGLU 前馈网络。</p>
<p>注意：在这种特定情况下，为了数值稳定性，你可以在实现中使用 torch.sigmoid。</p>
<p>在你的实现中，如前所述，你应该将 $d_{ff}$ 设置为大约 $\frac{8}{3} \times d_{model}$，同时确保内部前馈层的维度是 64 的倍数，以便充分利用你的硬件。要根据我们提供的测试来验证你的实现，你需要完成位于 [adapters.run_swiglu] 的测试适配器。然后，运行 <code>uv run pytest -k test_swiglu</code> 来测试你的实现。</p>
<h5 id="Answer：-3"><a href="#Answer：-3" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.SwiGLU和adapter.run_swiglu。</p>
<h3 id="3-5-3-相对位置嵌入"><a href="#3-5-3-相对位置嵌入" class="headerlink" title="3.5.3 相对位置嵌入"></a>3.5.3 相对位置嵌入</h3><p>为了将位置信息注入模型，我们将实现旋转位置嵌入 [Su et al., 2021]，通常被称为 RoPE。对于给定位置 i 的查询 token $q^{(i)} = W_q x^{(i)} \in \mathbb{R}^d$，我们将应用一个两两旋转矩阵 $R_i$，得到 $q’^{(i)} = R_i q^{(i)} = R_i W_q x^{(i)}$。这里，$R_i$ 将嵌入元素的配对 $q^{(i)}_{2k-1:2k}$ 作为二维向量按角度 $\theta_{i,k} = \frac{i}{\Theta^{(2k-2)/d}}$ 进行旋转，其中 $k \in {1, \dots, d/2}$且$\Theta$ 为某个常数。因此，我们可以认为 $R_i$ 是一个大小为 $d \times d$ 的分块对角矩阵，其分块为 $R_k^i$（$k \in {1, \dots, d/2}$），其中</p>

$$R_k^i = \begin{bmatrix} \cos(\theta_{i,k}) & -\sin(\theta_{i,k}) \\ \sin(\theta_{i,k}) & \cos(\theta_{i,k}) \end{bmatrix}. \quad (8)$$
<p>由此我们得到完整的旋转矩阵</p>

$$R_i = \begin{bmatrix} R_1^i & 0 & 0 & \dots & 0 \\ 0 & R_2^i & 0 & \dots & 0 \\ 0 & 0 & R_3^i & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & R_{d/2}^i \end{bmatrix}, \quad (9)$$
<p>其中 0 代表 $2 \times 2$ 的零矩阵。虽然可以构造完整的 $d \times d$ 矩阵，但一个好的解决方案应该利用该矩阵的性质来更高效地实现变换。由于我们只关心给定序列内 token 的相对旋转，我们可以跨层和跨不同批次复用我们计算的 $\cos(\theta_{i,k})$ 和 $\sin(\theta_{i,k})$ 值。如果你想对其进行优化，可以使用一个被所有层引用的单个 RoPE 模块，它可以拥有一个在初始化期间通过 self.register_buffer(persistent=False) 创建的 sin 和 cos 值的二维预计算缓存，而不是使用 nn.Parameter（因为我们不想学习这些固定的余弦和正弦值）。对 $k^{(j)}$ 执行与对 $q^{(i)}$ 完全相同的旋转过程，即按相应的 $R_j$ 进行旋转。注意该层没有可学习参数。</p>
<h4 id="Problem-rope-实现-RoPE-2-分"><a href="#Problem-rope-实现-RoPE-2-分" class="headerlink" title="Problem (rope): 实现 RoPE (2 分)"></a>Problem (rope): 实现 RoPE (2 分)</h4><p>实现一个 RotaryPositionalEmbedding 类，将 RoPE 应用于输入张量。建议使用以下接口：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, theta: <span class="built_in">float</span>, d_k: <span class="built_in">int</span>, max_seq_len: <span class="built_in">int</span>, device=<span class="literal">None</span></span>)</span><br></pre></td></tr></table></figure>
<p>构造 RoPE 模块并在需要时创建缓存（register_buffer）。</p>
<p>1 theta: float RoPE 的 $\Theta$ 值<br>2 d_k: int 查询和键向量的维度<br>3 max_seq_len: int 将输入的最高序列长度<br>4 device: torch.device | None = None 存储缓存的设备</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor, token_positions: torch.Tensor</span>) -&gt; torch.Tensor</span><br></pre></td></tr></table></figure>
<p>处理形状为 (…, seq_len, d_k) 的输入张量并返回相同形状的张量。</p>
<p>注意，你应该允许 x 具有任意数量的批次维度。你应该假设 token_positions 是一个形状为 (…, seq_len) 的张量，指定了 x 沿序列维度的 token 位置。</p>
<p>你应该使用 token 位置来沿序列维度对你（可能预计算好的）cos 和 sin 张量进行切片。</p>
<p>要测试你的实现，请完成 [adapters.run_rope] 并确保其通过 <code>uv run pytest -k test_rope</code>。</p>
<h5 id="Answer：-4"><a href="#Answer：-4" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.RotaryPositionalEmbedding和adapter.run_rope。</p>
<h3 id="3-5-4-缩放点积注意力"><a href="#3-5-4-缩放点积注意力" class="headerlink" title="3.5.4 缩放点积注意力"></a>3.5.4 缩放点积注意力</h3><p>我们现在将实现 Vaswani et al. [2017]（第 3.2.1 节）中描述的缩放点积注意力。作为初步步骤，注意力操作的定义将使用 softmax，这是一种将未归一化的分数向量转变为归一化分布的操作：</p>

$$softmax(v)_i = \frac{\exp(v_i)}{\sum_{j=1}^n \exp(v_j)}. \quad (10)$$
<p>注意对于较大的值，$\exp(v_i)$ 可能会变成 inf（那么 inf/inf = NaN）。我们可以通过注意到 softmax 操作对于所有输入增加任何常数 c 具有不变性来避免这种情况（就是通常说的数值稳定性技巧）。我们可以利用这一性质来保证数值稳定性 —— 通常，我们会从 $o_i$ 的所有元素中减去 $o_i$ 的最大项，使得新的最大项为 0。你现在将实现 softmax，并使用这个技巧来保证数值稳定性。</p>
<h4 id="Problem-softmax-实现-softmax-1-分"><a href="#Problem-softmax-实现-softmax-1-分" class="headerlink" title="Problem (softmax): 实现 softmax (1 分)"></a>Problem (softmax): 实现 softmax (1 分)</h4><p>编写一个对张量应用 softmax 操作的函数。你的函数应该接受两个参数：一个张量和一个维度 i，并将 softmax 应用于输入张量的第 i 维。输出张量应具有与输入张量相同的形状，但其第 i 维现在将拥有归一化的概率分布。使用从第 i 维所有元素中减去第 i 维最大值的技巧，以避免数值稳定性问题。</p>
<p>要测试你的实现，请完成 [adapters.run_softmax] 并确保其通过 uv run pytest -k test_softmax_matches_pytorch。</p>
<h5 id="Answer：-5"><a href="#Answer：-5" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.softmax和adapter.run_softmax。  </p>
<p>我们现在可以按如下方式给出注意力操作的数学定义：</p>

$$Attention(Q, K, V) = softmax \left( \frac{Q K^\top}{\sqrt{d_k}} \right) V \quad (11)$$
<p>其中 $Q \in \mathbb{R}^{n \times d_k}$，$K \in \mathbb{R}^{m \times d_k}$，且 $V \in \mathbb{R}^{m \times d_v}$。在这里，$Q$、$K$ 和 $V$ 都是该操作的输入；注意这些不是可学习的参数。这里公式里是 $K^\top$ 而不是原文提到的 $Q^\top K$，原因参阅 3.3.1。</p>
<p><strong>掩码</strong>（softmax前一步）：有时为注意力操作的输出添加掩码是很方便的。掩码的形状应为 $M \in {True, False}^{n \times m}$，该布尔矩阵的每一行 $i$ 表示查询 $i$ 应该关注哪些键。按照惯例（虽然略微令人困惑），位置 $(i, j)$ 处的 $True$ 值表示查询 $i$ 确实关注键 $j$，而 $False$ 值则表示该查询不关注该键。换句话说，“信息流”在值为 $True$ 的 $(i, j)$ 对处流动。例如，考虑一个条目为 [[True, True, False]] 的 $1 \times 3$ 掩码矩阵。单个查询向量仅关注前两个键。在计算上，使用掩码比在子序列上计算注意力要高效得多，我们可以通过获取 softmax 前的值 $\left( \frac{Q K^\top}{\sqrt{d_k}} \right)$ 并在掩码矩阵为 $False$ 的任何条目中添加 $-\infty$ 来实现这一点。</p>
<h4 id="Problem-scaled-dot-product-attention-实现点积注意力函数-5-points"><a href="#Problem-scaled-dot-product-attention-实现点积注意力函数-5-points" class="headerlink" title="Problem (scaled_dot_product_attention): 实现点积注意力函数(5 points)"></a>Problem (scaled_dot_product_attention): 实现点积注意力函数(5 points)</h4><p>实现缩放点积注意力函数。你的实现应处理形状为 (batch_size, …, seq_len, d_k) 的键和查询，以及形状为 (batch_size, …, seq_len, d_v) 的值，其中 … 代表任意数量的其他类批次维度（如果已给定）。该实现应返回形状为 (batch_size, …, d_v) 的输出。有关类批次维度的讨论，请参见3.3 。</p>
<p>你的实现还应支持一个可选的、用户提供的形状为 (seq_len, seq_len) 的布尔掩码。掩码值为 $True$ 的位置的注意力概率总和应为 1，掩码值为 $False$ 的位置的注意力概率应为零。</p>
<p>为了针对我们提供的测试来测试你的实现，你将需要实现位于 [adapters.run_scaled_dot_product_attention] 的测试适配器。</p>
<p>uv run pytest -k test_scaled_dot_product_attention 在三阶输入张量上测试你的实现，而 uv run pytest -k test_4d_scaled_dot_product_attention 则在四阶输入张量上测试你的实现。</p>
<h5 id="Answer：-6"><a href="#Answer：-6" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.scaled_dot_product_attention和adapter.run_scaled_dot_product_attention。</p>
<h3 id="3-5-5-Causal-Multi-Head-Self-Attention"><a href="#3-5-5-Causal-Multi-Head-Self-Attention" class="headerlink" title="3.5.5 Causal Multi-Head Self-Attention"></a>3.5.5 Causal Multi-Head Self-Attention</h3><p>我们将实现 Vaswani 等人 [2017] 第 3.2.2 节中所述的多头自注意力。回想一下，在数学上，应用多头注意力的操作定义如下：</p>

$$MultiHead(Q, K, V) = Concat(head_1, \dots, head_h) \quad (12)$$
<p>对于</p>

$$head_i = Attention(Q_i, K_i, V_i) \quad (13)$$
<p>$Q_i, K_i, V_i$ 分别是 $Q, K, V$ 嵌入维度中大小为 $d_k$ 或 $d_v$ 的第 $i \in {1, \dots, h}$个分片。这里的$Attention$ 是 §3.5.4 中定义的缩放点积注意力操作。由此我们可以形成多头自注意力操作：</p>

$$MultiHeadSelfAttention(x) = W^O MultiHead(W^Q x, W^K x, W^V x) \quad (14)$$
<p>这里，可学习参数为 $W^Q \in \mathbb{R}^{hd_k \times d_{model}}$，$W^K \in \mathbb{R}^{hd_k \times d_{model}}$，$W^V \in \mathbb{R}^{hd_v \times d_{model}}$，以及 $W^O \in \mathbb{R}^{d_{model} \times hd_v}$。由于 $Q, K$ 和 $V$ 在多头注意力操作中被分片，我们可以认为 $W^Q, W^K$ 和 $W^V$ 在输出维度上针对每个 head 进行了分离。当你完成此项工作时，你应该通过总共三次矩阵乘法来计算键、值和查询的投影。</p>
<p><strong>因果掩码</strong>。你的实现应防止模型关注序列中的未来标记。换句话说，如果模型被给定标记序列 $t_1, \dots, t_n$，并且我们想要计算前缀 $t_1, \dots, t_i$（其中 $i &lt; n$）的下一个词预测，模型不应能够访问（关注）位置 $t_{i+1}, \dots, t_n$ 处的标记表示，因为在推理生成文本时它无法访问这些标记（且这些未来标记会泄露关于真实下一个词身份的信息，使语言模型预训练目标变得平庸）。对于输入标记序列 $t_1, \dots, t_n$，我们可以通过运行 $n$ 次多头自注意力（针对序列中的 $n$ 个唯一前缀）来幼稚地防止访问未来标记。相反，我们将使用因果注意力掩码，它允许标记 $i$ 关注序列中所有位置 $j \le i$。你可以使用 torch.triu 或广播索引比较来构建此掩码，并且你应该利用 §3.5.4 中的缩放点积注意力实现已经支持注意力掩码这一事实。</p>
<p><strong>应用 RoPE</strong>。RoPE 应应用于查询和键向量，但不应应用于值向量。此外，头维度应被视为批次维度，因为在多头注意力中，注意力是针对每个 head 独立应用的。这意味着应向每个 head 的查询和键向量应用完全相同的 RoPE 旋转。</p>
<h4 id="Problem-multihead-self-attention-Implement-causal-multi-head-self-attention-5-points"><a href="#Problem-multihead-self-attention-Implement-causal-multi-head-self-attention-5-points" class="headerlink" title="Problem (multihead_self_attention): Implement causal multi-head self-attention (5 points)"></a>Problem (multihead_self_attention): Implement causal multi-head self-attention (5 points)</h4><p>将因果多头自注意力实现为一个 torch.nn.Module。你的实现应（至少）接受以下参数：</p>
<p>1 d_model: int Transformer 块输入的维度。<br>2 num_heads: int 多头自注意力中使用的头数。</p>
<p>遵循 Vaswani 等人 [2017]，设置 $d_k = d_v = d_{model} / h$。为了针对我们提供的测试来测试你的实现，请实现位于 [adapters.run_multihead_self_attention] 的测试适配器。然后，运行 <code>uv run pytest -k test_multihead_self_attention</code> 以测试你的实现。</p>
<h5 id="Answer：-7"><a href="#Answer：-7" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.CausalMultiHeadSelfAttention和adapter.run_multihead_self_attention &amp; adapter.run_multihead_self_attention_with_rope 。</p>
<h3 id="3-6-The-Full-Transformer-LM"><a href="#3-6-The-Full-Transformer-LM" class="headerlink" title="3.6 The Full Transformer LM"></a>3.6 The Full Transformer LM</h3><p>让我们开始组装 Transformer 块（回顾图 2 将会有所帮助）。一个 Transformer 块包含两个“子层”，一个用于多头自注意力，另一个用于前馈网络。在每个子层中，我们首先执行 RMSNorm，然后是主要操作（MHA/FF），最后加入残差连接。</p>
<p>具体地，Transformer 块的前一半（第一个“子层”）应实现以下更新集，以从输入 $x$ 产生输出 $y$，</p>

$$y = x + MultiHeadSelfAttention(RMSNorm(x)) \quad (15)$$
<h4 id="Problem-transformer-block-Implement-the-Transformer-block-3-points"><a href="#Problem-transformer-block-Implement-the-Transformer-block-3-points" class="headerlink" title="Problem (transformer_block): Implement the Transformer block (3 points)"></a>Problem (transformer_block): Implement the Transformer block (3 points)</h4><p>实现 §3.5 中所述且如图 2 所示的 pre-norm Transformer 块。你的 Transformer 块应（至少）接受以下参数。</p>
<p>1 d_model: int Transformer 块输入的维度。<br>2 num_heads: int 多头自注意力中使用的头数。<br>3 d_ff: int 逐位置前馈内部层的维度。</p>
<p>为了测试你的实现，请实现适配器 [adapters.run_transformer_block]。然后运行 <code>uv run pytest -k test_transformer_block</code> 以测试你的实现。</p>
<h5 id="Answer：-8"><a href="#Answer：-8" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.TransformerBlock和adapter.run_transformer_block。</p>
<p>现在我们将这些块组合在一起，遵循图 1 中的高层图示。按照我们在 3.1.1 节中对嵌入的描述，将其输入到 num_layers 个 Transformer 块中，然后将其传递到三个输出层以获得词汇表上的分布。</p>
<h4 id="Problem-transformer-lm-Implementing-the-Transformer-LM-3-points"><a href="#Problem-transformer-lm-Implementing-the-Transformer-LM-3-points" class="headerlink" title="Problem (transformer_lm): Implementing the Transformer LM (3 points)"></a>Problem (transformer_lm): Implementing the Transformer LM (3 points)</h4><p>是时候把一切组合起来了！实现 §3.1 中所述且如图 1 所示的 Transformer 语言模型。你的实现至少应接受所有上述 Transformer 块的构造参数，以及这些附加参数：</p>
<p>vocab_size: int 词汇表大小，对于确定标记嵌入矩阵的维度是必需的。</p>
<p>context_length: int 最大上下文长度，对于确定位置嵌入矩阵的维度是必需的。</p>
<p>num_layers: int 要使用的 Transformer 块的数量。</p>
<p>为了针对我们提供的测试来测试你的实现，你首先需要实现位于 [adapters.run_transformer_lm] 的测试适配器。然后，运行 uv run pytest -k test_transformer_lm 以测试你的实现。</p>
<h5 id="Answer：-9"><a href="#Answer：-9" class="headerlink" title="Answer："></a>Answer：</h5><p>见transformer_model.TransformerLM和adapter.run_transformer_lm。</p>
<p><strong>资源核算</strong>。能够理解 Transformer 的各个部分如何消耗计算和内存是有用的。我们将通过一些步骤来执行一些基础的“FLOPs（Floating-point Operations）核算”。Transformer 中的绝大多数 FLOPS 都是矩阵乘法，所以我们的核心方法简单：</p>
<p>1 写下 Transformer 前向传递中的所有矩阵乘法。<br>2 将每个矩阵乘法转换为所需的 FLOPs。</p>
<p>对于第二步，用到以下事实：</p>
<p>规则：给定 $A \in \mathbb{R}^{m \times n}$ 和 $B \in \mathbb{R}^{n \times p}$，矩阵-矩阵乘积 $AB$ 需要 $2mnp$ FLOPs。</p>
<p>要理解这一点，请注意 $(AB)[i, j] = A[i, :] \cdot B[:, j]$，并且该点积需要 $n$ 次加法和 $n$ 次乘法（$2n$ FLOPs）。然后，由于矩阵-矩阵乘积 $AB$ 有 $m \times p$ 个条目，总的 FLOPS 数量为 $(2n)(mp) = 2mnp$。</p>
<p>现在，在做下一个题目之前，检查 Transformer 块和 Transformer 语言模型的每个组件，并列出所有矩阵乘法及其相关的 FLOPs 成本会很有帮助。</p>
<h4 id="Problem-transformer-accounting-Transformer-LM-resource-accounting-5-points"><a href="#Problem-transformer-accounting-Transformer-LM-resource-accounting-5-points" class="headerlink" title="Problem (transformer_accounting): Transformer LM resource accounting (5 points)"></a>Problem (transformer_accounting): Transformer LM resource accounting (5 points)</h4><p>(a) 考虑 GPT-2 XL，它具有以下配置：</p>
<p>vocab_size : 50,257</p>
<p>context_length : 1,024</p>
<p>num_layers : 48</p>
<p>d_model : 1,600</p>
<p>num_heads : 25</p>
<p>d_ff : 6,400</p>
<p>假设我们使用此配置构造了我们的模型。我们的模型将有多少个可训练参数？假设每个参数使用单精度浮点数表示，仅加载此模型需要多少内存？</p>
<p>(b) 识别完成我们的 GPT-2 XL 形状模型的前向传递所需的矩阵乘法。这些矩阵乘法总共需要多少个 FLOPs？假设我们的输入序列具有 context_length 个token。</p>
<p>交付：矩阵乘法列表（附带描述），以及所需的总 FLOPs 数量。</p>
<p>(c) 根据你上方的分析，模型的哪些部分需要最多的 FLOPs？</p>
<p>(d) 重复你的分析对于 GPT-2 small（12 层，768 d_model，12 头）、GPT-2 medium（24 层，1024 d_model，16 头）和 GPT-2 large（36 层，1280 d_model，20 头）。随着模型尺寸的增加，Transformer 语言模型的哪些部分占总 FLOPs 的比例变得更多或更少？</p>
<p>对于每个模型，提供模型组件及其相关 FLOPs 的分解（作为前向传递所需总 FLOPs 的比例）。此外，提供一到两句话的描述，说明改变模型尺寸如何改变每个组件的比例 FLOPs。</p>
<p>(e) 采用 GPT-2 XL 并将上下文长度增加到 16,384。一次前向传递的总 FLOPs 如何变化？模型组件的 FLOPs 相对贡献如何变化？</p>
<h5 id="Answer：-10"><a href="#Answer：-10" class="headerlink" title="Answer："></a>Answer：</h5><p>(a) 内存占用<br>1 embedding : 50257(vocab) <em> 1600(d_model) = 80.41millioins<br>2 transformer每层：<br>i. 2层ln，每层1600(d_model)个参数，总共2 </em> 1600 = 3200<br>ii. mha：4个矩阵(qkvo)，每个d_model <em> d_model。总共10.24m<br>iii. ffn：3个矩阵(W1W2W3)，每个d_model </em> d_ff 。总共30.72m<br>总共48层，48 <em> (10.24 + 30.72 ) = 1966 m<br>3 output head： 1600 </em> 50257 = 80.41m</p>
<p>总共2.12billion参数，占用内存2.127 <em> 10^9 </em> 4 bytes = 8.51 GB  </p>
<p>(b) 前向FLOPs<br>输入一个context_length=1024=T，transformer每层：<br>1 QKV：3 <em> (2 </em> T <em> d_model </em> d_model)<br>2 QK^T : 2 <em> T </em> d_model <em> T<br>3 PV : 2 </em> T <em> T </em> d_model<br>4 O : 2 <em> T </em> d_model <em> d_model<br>5 FFN: 3 </em> (2 <em> T </em> d_model * d_ff)<br>一层总计：90.59GFLOPs</p>
<p>output head：2 <em> T </em> d_model <em> vocab = 164 GFLOPs<br>前向总计：48 </em> 90.59 + 164 = 4.5 TFLOPs</p>
<p>(c) FFN最多。主要原因是这里d_ff是d_model四倍</p>
<p>(d) 只考虑transformer：<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">| 模型规格 (Model) | d<span class="emphasis">_model | 层数 | FFN % | Attention % | 投影层 % |</span></span><br><span class="line"><span class="emphasis">| :---            | :---:   |:---:| :---: | :---:       | :---:   |</span></span><br><span class="line"><span class="emphasis">| <span class="strong">**GPT-2 Small**</span> | 768     | 12  | 62.6% | 11.3%       | 26.1%   |</span></span><br><span class="line"><span class="emphasis">| <span class="strong">**GPT-2 Medium**</span>| 1024    | 24  | 65.1% | 8.8%        | 26.1%   |</span></span><br><span class="line"><span class="emphasis">| <span class="strong">**GPT-2 Large**</span> | 1280    | 36  | 67.3% | 7.2%        | 25.5%   |</span></span><br><span class="line"><span class="emphasis">| <span class="strong">**GPT-2 XL**</span>    | 1600    | 48  | 69.5% | 5.9%        | 24.6%   |</span></span><br></pre></td></tr></table></figure></p>
<p>(e)<br>1 可计算出总FLOPs增大34倍。<br>2 由(b)可见，FFN和投影是关于T的(增大16倍)，attention是关于T^2的(增大256倍)，所以attention的占比会远比之前大。</p>
]]></content>
      <categories>
        <category>cs336</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>cs336</tag>
      </tags>
  </entry>
</search>
