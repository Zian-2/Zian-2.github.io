<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>矩阵求导理论：几种表示</title>
    <url>/2026/01/21/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</url>
    <content><![CDATA[<p><strong>提示：如果latex矩阵的渲染有问题，刷新往往能解决问题</strong><br><br></p>
<p>也许我们可以说，张量就是矩阵，梯度就是导数，张量的梯度本质上就是矩阵求导的ml小黑话说法。然而由于一些众所周知的原因，很多人甚至在学习线性代数之前就已经走进了深度学习，初次面对高维张量的求导往往感到混乱。而市面上如The Matrix Cookbook等的工具书又往往太细节，翻译也不尽如人意。本文将以最基础的多元 Logistic 回归（Multinomial Logistic Regression）为例，从大方向上探讨三种不同的推导范式。</p>
<h3 id="例子背景"><a href="#例子背景" class="headerlink" title="例子背景"></a>例子背景</h3><p>我们设定一个典型的分类场景：</p>
<ul>
<li><p><strong>输入向量</strong>：$\mathbf{x} \in \mathbb{R}^{n \times 1}$。</p>
</li>
<li><p><strong>权重矩阵</strong>：$W \in \mathbb{R}^{m \times n}$。</p>
</li>
<li><p><strong>线性输出</strong>：$\mathbf{a} = W\mathbf{x} \in \mathbb{R}^{m \times 1}$。</p>
</li>
<li><p><strong>预测概率</strong>：$\mathbf{p} = \text{softmax}(\mathbf{a}) \in \mathbb{R}^{m \times 1}$，其中第 $i$ 个分量 $p_i = \frac{\exp(a_i)}{\sum_{j=1}^m \exp(a_j)}$。</p>
</li>
<li><p><strong>目标标签</strong>：$\mathbf{y} \in \mathbb{R}^{m \times 1}$，是一个 One-hot 向量（仅有一个元素为 1，其余为 0）。</p>
</li>
<li><p><strong>损失函数</strong>：标量 $l = -\mathbf{y}^\top \log \mathbf{p} = -\sum_{k=1}^m y_k \ln p_k$。</p>
</li>
</ul>
<p>我们的目标是求出梯度矩阵 $\nabla_W l$（即 $\frac{\partial l}{\partial W}$）。</p>
<span id="more"></span>
<p><br> <br> </p>
<h3 id="方法一：逐项求导"><a href="#方法一：逐项求导" class="headerlink" title="方法一：逐项求导"></a>方法一：逐项求导</h3><p>当然了，只要你会求偏导，肯定就会对世界上所有的矩阵求导，只要确定了定义，就可以对应地求导，因为矩阵本质上是标量的排列。我们通过观察单个权重 $W_{ij}$ 的变动如何影响最终损失 $l$，最后再将规律汇总回矩阵形式。</p>
<h4 id="1-链式法则"><a href="#1-链式法则" class="headerlink" title="1. 链式法则"></a>1. 链式法则</h4><p>我们的目标是求 $l$ 对 $W_{ij}$（矩阵 $W$ 第 $i$ 行第 $j$ 列的元素）的偏导数。</p>
<p>根据函数依赖关系：$W_{ij}$ 的变动首先影响线性输出 $a_i$，由于 $a_i$ 是 Softmax 函数的输入，它会进一步影响<strong>所有的</strong>预测概率 $p_k (k=1, \dots, m)$，最后导致损失 $l$ 的变化。</p>
<p>链式法则展开如下：</p>

$$\frac{\partial l}{\partial W_{ij}} = \sum_{k=1}^m \frac{\partial l}{\partial p_k} \cdot \frac{\partial p_k}{\partial a_i} \cdot \frac{\partial a_i}{\partial W_{ij}}$$
<h4 id="2-第一阶段：基础项求导"><a href="#2-第一阶段：基础项求导" class="headerlink" title="2. 第一阶段：基础项求导"></a>2. 第一阶段：基础项求导</h4><ul>
<li><p><strong>计算 $\frac{\partial a_i}{\partial W_{ij}}$</strong>：</p>
<p>  已知 $a_i = \sum_{t=1}^n W_{it}x_t = W_{i1}x_1 + \dots + W_{ij}x_j + \dots$。</p>
<p>  对 $W_{ij}$ 求导得：$\frac{\partial a_i}{\partial W_{ij}} = x_j$。</p>
</li>
<li><p><strong>计算 $\frac{\partial l}{\partial p_k}$</strong>：</p>
<p>  已知 $l = -\sum_{k=1}^m y_k \ln p_k$。</p>
<p>  对 $p_k$ 求导得：$\frac{\partial l}{\partial p_k} = -\frac{y_k}{p_k}$。</p>
</li>
</ul>
<h4 id="3-第二阶段：Softmax-梯度的分类讨论"><a href="#3-第二阶段：Softmax-梯度的分类讨论" class="headerlink" title="3. 第二阶段：Softmax 梯度的分类讨论"></a>3. 第二阶段：Softmax 梯度的分类讨论</h4><p>我们需要计算 $\frac{\partial p_k}{\partial a_i}$。由于 $p_k = \frac{e^{a_k}}{\sum_{j=1}^m e^{a_j}}$，分母包含所有 $a$，因此需要分两种情况：</p>
<ul>
<li><p><strong>情况 A：当 $k = i$ 时（输出下标等于求导下标）</strong></p>
<p>  利用商的求导法则：</p>

$$\frac{\partial p_i}{\partial a_i} = \frac{e^{a_i}(\sum e^a) - e^{a_i}(e^{a_i})}{(\sum e^a)^2} = \frac{e^{a_i}}{\sum e^a} - \left(\frac{e^{a_i}}{\sum e^a}\right)^2 = p_i(1 - p_i)$$
</li>
<li><p><strong>情况 B：当 $k \neq i$ 时（输出下标不等于求导下标）</strong></p>

$$\frac{\partial p_k}{\partial a_i} = \frac{0 \cdot (\sum e^a) - e^{a_k}(e^{a_i})}{(\sum e^a)^2} = -\frac{e^{a_k}}{\sum e^a} \cdot \frac{e^{a_i}}{\sum e^a} = -p_k p_i$$
</li>
</ul>
<h4 id="4-第三阶段：组合求和"><a href="#4-第三阶段：组合求和" class="headerlink" title="4. 第三阶段：组合求和"></a>4. 第三阶段：组合求和</h4><p>将上述结果代入第一步的链式法则总式中，并将求和号按照 $k=i$ 和 $k \neq i$ 进行拆解：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} \frac{\partial l}{\partial p_k} \frac{\partial p_k}{\partial a_i} + \frac{\partial l}{\partial p_i} \frac{\partial p_i}{\partial a_i} \right) \cdot \frac{\partial a_i}{\partial W_{ij}}$$
<p>代入具体的偏导表达式：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} \left(-\frac{y_k}{p_k}\right)(-p_k p_i) + \left(-\frac{y_i}{p_i}\right)p_i(1-p_i) \right) \cdot x_j$$
<p>简化括号内的各项：</p>
<ul>
<li><p>第一部分：$\sum_{k \neq i} y_k p_i$</p>
</li>
<li><p>第二部分：$-y_i(1 - p_i) = -y_i + y_i p_i$</p>
</li>
</ul>
<p>组合后得到：</p>

$$\frac{\partial l}{\partial W_{ij}} = \left( \sum_{k \neq i} y_k p_i + y_i p_i - y_i \right) x_j$$

$$\frac{\partial l}{\partial W_{ij}} = \left( p_i \left( \sum_{k=1}^m y_k \right) - y_i \right) x_j$$
<p>由于标签向量 $\mathbf{y}$ 是 One-hot 的，满足 $\sum_{k=1}^m y_k = 1$，上式最终简化为：</p>

$$\frac{\partial l}{\partial W_{ij}} = (p_i - y_i)x_j$$
<h4 id="5-还原为矩阵形式"><a href="#5-还原为矩阵形式" class="headerlink" title="5. 还原为矩阵形式"></a>5. 还原为矩阵形式</h4><p>我们求出了梯度矩阵中每一个元素 $(i, j)$ 的值。观察发现，这正好对应向量 $(\mathbf{p}-\mathbf{y})$ 的第 $i$ 个元素与向量 $\mathbf{x}$ 的第 $j$ 个元素的乘积。</p>
<p>于是结果为：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
<p><br> <br> <br></p>
<h3 id="方法二：利用全微分"><a href="#方法二：利用全微分" class="headerlink" title="方法二：利用全微分"></a>方法二：利用全微分</h3><p>你都用矩阵了，为什么还要逐项求导呢？矩阵就是矩阵，矩阵是不能不和标量切割的。下面我们将会看到，将矩阵整体不加拆分地求导有一些很美好的性质。实践上，整体的矩阵求导也使我们更少地受维度匹配问题的侵扰，理解上也更为简洁与直觉。</p>
<h4 id="1-整体流程"><a href="#1-整体流程" class="headerlink" title="1. 整体流程"></a>1. 整体流程</h4><p>在标量微积分中，我们知道 $df = f’(x)dx$。全微分法将这一思想推广到矩阵：对于一个以矩阵 $X$ 为自变量的标量函数 $f(X)$，其全微分一定可以表示为如下标准形式：</p>

$$df = \sum_{i,j} \frac{\partial f}{\partial X_{ij}} dX_{ij}$$
<p>为了避免陷入下标求和的泥潭，我们利用迹函数（Trace）。两个同维矩阵 $A$ 和 $B$ 的内积（对应元素相乘再求和）可以写成 $\text{Tr}(A^\top B)$是常用而易得的结论。因此，上述微分式可以写成：</p>

$$df = \text{Tr}(G^\top dX)$$
<p>这里， $G$ 就是梯度矩阵 $\frac{\partial f}{\partial X}$； dX的定义你可以显然地看出。</p>
<p><strong>全微分法的标准操作流程如下：</strong></p>
<ol>
<li><p><strong>写出微分式</strong>：利用微分运算法则，对目标标量 $l$ 直接求微分 $dl$。</p>
</li>
<li><p><strong>套入迹函数</strong>：将 $dl$ 放入迹中，即 $dl = \text{Tr}(dl)$（因为标量的迹等于自身）。</p>
</li>
<li><p><strong>应用Trace Trick</strong>：利用迹函数的循环复用性 $\text{Tr}(ABC) = \text{Tr}(BCA)$，通过左右挪动，将所有的 $dX$（或 $dW$）变动项孤立到式子的最右端。</p>
</li>
<li><p><strong>读取梯度</strong>：将式子整理成 $dl = \text{Tr}(G^\top dW)$ 的标准形态，直接读出 $G$。</p>
</li>
</ol>
<h4 id="2-矩阵微分的运算法则与Trace-trick"><a href="#2-矩阵微分的运算法则与Trace-trick" class="headerlink" title="2. 矩阵微分的运算法则与Trace trick"></a>2. 矩阵微分的运算法则与Trace trick</h4><p>以下的法则需要掌握。当然了，你也可以自己推。</p>
<p>证明基本上可以在张贤达《矩阵分析与应用》中找到。</p>
<ul>
<li><p><strong>微分法则（与标量高度相似）</strong>：</p>
<ul>
<li><p>$d(X + Y) = dX + dY$</p>
</li>
<li><p>$d(XY) = (dX)Y + X(dY)$ （注意保持矩阵先后顺序，不可交换）</p>
</li>
<li><p>$d(X^\top) = (dX)^\top$</p>
</li>
<li><p>$d(\text{exp}(X)) = \text{exp}(X) \odot dX$ （$\odot$ 为逐元素乘积）</p>
</li>
</ul>
</li>
<li><p><strong>Trace的性质</strong>：</p>
<ul>
<li><p><strong>循环移位</strong>：$\text{Tr}(ABC) = \text{Tr}(BCA) = \text{Tr}(CAB)$ </p>
</li>
<li><p><strong>转置不变</strong>：$\text{Tr}(A) = \text{Tr}(A^\top)$。</p>
</li>
<li><p><strong>线性</strong>：$d\text{Tr}(X) = \text{Tr}(dX)$。</p>
</li>
<li><p><strong>矩阵乘法/逐元素乘法交换</strong>：$\text{tr}(A^T(B\odot C)) = \text{tr}((A\odot B)^TC)$，其中A, B, C尺寸相同。两侧都等于$\sum_{i,j}A_{ij}B_{ij}C_{ij}$。</p>
</li>
</ul>
</li>
</ul>
<h4 id="3-多元-Logistic-的梯度"><a href="#3-多元-Logistic-的梯度" class="headerlink" title="3. 多元 Logistic 的梯度"></a>3. 多元 Logistic 的梯度</h4><p>现在，我们可以很容易地利用上述结论和流程解决多元logistic。</p>
<p><strong>第一步：化简目标函数</strong></p>
<p>为了便于求导，利用对数性质将 $l$ 展开：</p>

$$l = -\mathbf{y}^\top \log \text{softmax}(\mathbf{a}) = -\mathbf{y}^\top (\mathbf{a} - \mathbf{1} \ln(\mathbf{1}^\top \exp(\mathbf{a})))$$
<p>这里 $\mathbf{1}^\top \exp(\mathbf{a})$ 实际上就是 Softmax 的分母部分 $\sum e^{a_j}$。</p>
<p><strong>第二步：对 $l$ 求微分 $dl$</strong></p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \mathbf{y}^\top \mathbf{1} \cdot d\ln(\mathbf{1}^\top \exp(\mathbf{a}))$$
<p>由于 $\mathbf{y}$ 是 One-hot 向量，$\mathbf{y}^\top \mathbf{1} = 1$。再对对数项求微分：</p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \frac{1}{\mathbf{1}^\top \exp(\mathbf{a})} d(\mathbf{1}^\top \exp(\mathbf{a}))$$

$$dl = -\mathbf{y}^\top d\mathbf{a} + \frac{1}{\mathbf{1}^\top \exp(\mathbf{a})} \mathbf{1}^\top (\exp(\mathbf{a}) \odot d\mathbf{a})$$
<p>观察发现，$\frac{\exp(\mathbf{a})}{\mathbf{1}^\top \exp(\mathbf{a})}$ 正好是预测概率向量 $\mathbf{p}$。根据向量内积性质，$(\mathbf{p} \odot d\mathbf{a})$ 的各项求和等于 $\mathbf{p}^\top d\mathbf{a}$：</p>

$$dl = -\mathbf{y}^\top d\mathbf{a} + \mathbf{p}^\top d\mathbf{a} = (\mathbf{p} - \mathbf{y})^\top d\mathbf{a}$$
<p><strong>第三步：代入变量关系锁定 $dW$</strong></p>
<p>已知 $\mathbf{a} = W\mathbf{x}$，其中 $\mathbf{x}$ 是常数输入，则 $d\mathbf{a} = (dW)\mathbf{x}$。</p>
<p>代入上式：</p>

$$dl = (\mathbf{p} - \mathbf{y})^\top (dW) \mathbf{x}$$
<p><strong>第四步：利用迹函数提取梯度</strong></p>
<p>为了读出梯度，我们将标量 $dl$ 写成迹的形式，并利用 $\text{Tr}(ABC) = \text{Tr}(BCA)$ 变换位置：</p>

$$dl = \text{Tr}( (\mathbf{p} - \mathbf{y})^\top dW \mathbf{x} )$$
<p>将末尾的 $\mathbf{x}$ 移到最前面：</p>

$$dl = \text{Tr}( \mathbf{x} (\mathbf{p} - \mathbf{y})^\top dW )$$
<p>为了匹配标准形式 $dl = \text{Tr}(G^\top dW)$，我们需要把 $dW$ 左边的矩阵整体转置：</p>

$$dl = \text{Tr}( ((\mathbf{p} - \mathbf{y})\mathbf{x}^\top)^\top dW )$$
<p>于是：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
<p><br> <br> <br></p>
<h3 id="三：那么什么是Jacobian-矩阵？"><a href="#三：那么什么是Jacobian-矩阵？" class="headerlink" title="三：那么什么是Jacobian 矩阵？"></a>三：那么什么是Jacobian 矩阵？</h3><p>一般地，数学上利用全微分法可以优雅地解决问题。然而计算机并不擅长处理那些trace trick。所以在 PyTorch （TensorFlow大概也是一样）自动反向传播的底层，实际上，使用的是 Jacobian（雅可比）矩阵。阅读完之后你会发现，这其实并不是一种新的求导方法，只是通过向量化的方法将所有复杂的高维张量通通化成一维，从而形成某种理解维度上的方便。</p>
<h4 id="1-向量化"><a href="#1-向量化" class="headerlink" title="1. 向量化"></a>1. 向量化</h4><p>对于一个 $m \times n$ 的矩阵 $W$，第一步是将其元素按顺序排列成列向量。</p>
<ul>
<li><strong>向量化（Vectorization）</strong>：记作 $\text{vec}(W)$。如果你有一个 $2 \times 2$ 的矩阵，$\text{vec}(W)$ 就是 $4 \times 1$ 向量。</li>
</ul>
<h4 id="2-对Kronecker-积的利用（张量积）"><a href="#2-对Kronecker-积的利用（张量积）" class="headerlink" title="2. 对Kronecker 积的利用（张量积）"></a>2. 对Kronecker 积的利用（张量积）</h4><p>当我们要处理类似 $\mathbf{a} = W\mathbf{x}$ 这样的矩阵乘法求导时，数学上有：</p>

$$\text{vec}(AXB) = (B^\top \otimes A)\text{vec}(X)$$
<p>以$\mathbf{a} = W\mathbf{x}$ 为例。为了套用公式，我们补上单位阵：$\mathbf{a} = I_m W \mathbf{x}$。</p>
<p>利用上面的公式，它等价于：</p>

$$\mathbf{a} = (\mathbf{x}^\top \otimes I_m) \text{vec}(W)$$
<p>这意味着，$\mathbf{a}$ 是由 $\text{vec}(W)$ 经过一个系数矩阵 $(\mathbf{x}^\top \otimes I_m)$ 线性变换得到的。</p>
<h4 id="3-逐步推导"><a href="#3-逐步推导" class="headerlink" title="3. 逐步推导"></a>3. 逐步推导</h4><p>根据向量化的链式法则：</p>

$$\frac{\partial l}{\partial \text{vec}(W)} = \underbrace{\frac{\partial l}{\partial \mathbf{p}}}_{1 \times m} \cdot \underbrace{\frac{\partial \mathbf{p}}{\partial \mathbf{a}}}_{m \times m} \cdot \underbrace{\frac{\partial \mathbf{a}}{\partial \text{vec}(W)}}_{m \times mn}$$
<p><strong>第一步：求 $\frac{\partial l}{\partial \mathbf{p}}$</strong></p>
<p>损失函数 $l$ 对预测概率向量 $\mathbf{p}$ 的偏导，根据导数的定义，是一个行向量。这个行向量就是 $l$ 关于 $\mathbf{p}$ 的 Jacobian：</p>

$$J_{l,p} = \left[ -\frac{y_1}{p_1}, -\frac{y_2}{p_2}, \dots, -\frac{y_m}{p_m} \right]$$
<p><strong>第二步：求 $\frac{\partial \mathbf{p}}{\partial \mathbf{a}}$（Softmax 的雅可比矩阵）</strong></p>
<p>这里a和p都是向量，所以直接用方法一中的结论。</p>
<p>结果是一个 $m \times m$ 的矩阵，其形式为 $\text{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top$。即：</p>

$$J_{p,a} = \begin{bmatrix} p_1(1-p_1) & -p_1 p_2 & \dots & -p_1 p_m \\ -p_2 p_1 & p_2(1-p_2) & \dots & -p_2 p_m \\ \vdots & \vdots & \ddots & \vdots \\ -p_m p_1 & -p_m p_2 & \dots & p_m(1-p_m) \end{bmatrix}$$
<p><strong>第三步：求 $\frac{\partial \mathbf{a}}{\partial \text{vec}(W)}$</strong></p>
<p>根据在第 2 点提到的线性转换关系，这一项直接就是那个系数矩阵：</p>

$$\mathbf{x}^\top \otimes I_m$$
<p>即：</p>

$$J_{a,W} = \frac{\partial \mathbf{a}}{\partial \text{vec}(W)} = \mathbf{x}^\top \otimes I_m$$
<p>展开来看，就是 $n$ 个 $m \times m$ 的块：</p>

$$J_{a,W} = \begin{bmatrix} x_1 I_m & x_2 I_m & \dots & x_n I_m \end{bmatrix}$$
<h4 id="4-组合"><a href="#4-组合" class="headerlink" title="4. 组合"></a>4. 组合</h4><p>我们将这几项乘起来。</p>

$$\frac{\partial l}{\partial \text{vec}(W)} = (\mathbf{p} - \mathbf{y})^\top (\mathbf{x}^\top \otimes I_m)$$
<p>根据矩阵运算规则，这个结果实际上就是：</p>

$$\text{vec}((\mathbf{p} - \mathbf{y})\mathbf{x}^\top)^\top$$
<p>这是一个 $1 \times mn$ 的行向量。</p>
<h4 id="5-还原形状（逆向量化）"><a href="#5-还原形状（逆向量化）" class="headerlink" title="5. 还原形状（逆向量化）"></a>5. 还原形状（逆向量化）</h4><p>最后一步，我们将这个 $1 \times mn$ 的长向量重新折叠回 $m \times n$ 的矩阵形状。</p>
<p>即：</p>

$$\frac{\partial l}{\partial W} = (\mathbf{p} - \mathbf{y})\mathbf{x}^\top$$
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>矩阵理论</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>cs336：Assignment1-basics-chapter1&amp;2</title>
    <url>/2026/01/24/Assignment1-basics-chapter1&amp;2/</url>
    <content><![CDATA[<p>本文为cs336的assignment1中分词器对应章节的参考解答和课程笔记。   </p>
<p>1 本文范围内的作业暂时还不需要用到GPU<br>2 本课程并未提供好系统适配，所以事实上不推荐在win上完成作业。在你的服务器or虚拟机上完成可以避免很多麻烦。</p>
<h1 id="Setup："><a href="#Setup：" class="headerlink" title="Setup："></a>Setup：</h1><h2 id="环境配置："><a href="#环境配置：" class="headerlink" title="环境配置："></a>环境配置：</h2><p>按照<a href="https://github.com/stanford-cs336/assignment1-basics/tree/main">https://github.com/stanford-cs336/assignment1-basics/tree/main</a> 中README的说明配置并测试uv。  </p>
<p>如果你在Windows下，uv run pytest 时会出现问题，因为你没有办法import resource。<br>进行如下操作：<br>打开\assignment1-basics\tests\test_tokenizer.py， 删除第5行的import resource，或修改：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> resource</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    resource = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>然后重新uv run pytest。  </p>
<h2 id="数据下载："><a href="#数据下载：" class="headerlink" title="数据下载："></a>数据下载：</h2><p>下载 TinyStories data 和 subsample of OpenWebText：<br>课程原生使用的是wget； windows下，推荐使用curl。试着使用如下命令：<br><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建并进入数据目录</span></span><br><span class="line">mkdir <span class="literal">-p</span> <span class="keyword">data</span></span><br><span class="line"><span class="built_in">cd</span> <span class="keyword">data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 TinyStories</span></span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> TinyStoriesV2<span class="literal">-GPT4-train</span>.txt https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2<span class="literal">-GPT4-train</span>.txt</span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> TinyStoriesV2<span class="literal">-GPT4-valid</span>.txt https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2<span class="literal">-GPT4-valid</span>.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 OpenWebText</span></span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> owt_train.txt.gz https://huggingface.co/datasets/stanford<span class="literal">-cs336</span>/owt<span class="literal">-sample</span>/resolve/main/owt_train.txt.gz</span><br><span class="line">curl.exe <span class="literal">-L</span> <span class="literal">-o</span> owt_valid.txt.gz https://huggingface.co/datasets/stanford<span class="literal">-cs336</span>/owt<span class="literal">-sample</span>/resolve/main/owt_valid.txt.gz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 .gz 文件</span></span><br><span class="line"><span class="comment"># Windows 10/11 自带 tar.exe，可以替代 gunzip</span></span><br><span class="line">tar.exe <span class="literal">-xvzf</span> owt_train.txt.gz</span><br><span class="line">tar.exe <span class="literal">-xvzf</span> owt_valid.txt.gz</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure></p>
<p>如果解压不成功，考虑使用python原生解压。<br><span id="more"></span></p>
<p>先输入python进入交互模式，然后：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.<span class="built_in">open</span>(<span class="string">&#x27;owt_valid.txt.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f_in:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;owt_valid.txt&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f_out:</span><br><span class="line">        shutil.copyfileobj(f_in, f_out)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p><br> <br><br> </p>
<h1 id="1-作业总览"><a href="#1-作业总览" class="headerlink" title="1 作业总览"></a>1 作业总览</h1><p>你将构建训练标准 Transformer LM 所需的所有组件，并训练一些模型。</p>
<h3 id="你将实现："><a href="#你将实现：" class="headerlink" title="你将实现："></a>你将实现：</h3><p>1 BPE 分词器 (Byte-pair encoding tokenizer)：第 2 节<br>2 Transformer 语言模型 (LM)：第 3 节<br>3 交叉熵损失函数与 AdamW 优化器：第 4 节<br>4 训练循环：支持模型和优化器状态的序列化与加载（保存与读取）：第 5 节  </p>
<h3 id="你将完成如下任务："><a href="#你将完成如下任务：" class="headerlink" title="你将完成如下任务："></a>你将完成如下任务：</h3><p>1 在 TinyStories 数据集上训练一个 BPE 分词器。<br>2 对数据集运行训练好的分词器，将其转换为整数 ID 序列。<br>3 在 TinyStories 数据集上训练 Transformer 语言模型。<br>4 使用训练好的模型生成样本并评估困惑度 (Perplexity)。<br>5 在 OpenWebText 数据集上训练模型，并将达到的困惑度结果提交到排行榜。  </p>
<h3 id="允许使用的工具："><a href="#允许使用的工具：" class="headerlink" title="允许使用的工具："></a>允许使用的工具：</h3><p>课程希望你从0开始搭组件，所以不得使用<code>torch.nn</code>、<code>torch.nn.functional</code> 或 <code>torch.optim</code> 中的任何定义，除了：</p>
<p>1 <code>torch.nn.Parameter</code><br>2 <code>torch.nn</code> 中的容器类（<code>Module</code>, <code>ModuleList</code>, <code>Sequential</code> 等）<br>3 <code>torch.optim.Optimizer</code> 基类   </p>
<h3 id="关于-AI-工具："><a href="#关于-AI-工具：" class="headerlink" title="关于 AI 工具："></a>关于 AI 工具：</h3><p>1 允许使用大语言模型进行低级的编程问题或提问高级的概念问题的咨询，但严禁直接使用 AI 来完成作业题目。<br>2 建议在完成作业时关闭 IDE 中的 AI 自动补全（如 Cursor Tab, GitHub CoPilot）。</p>
<h3 id="数据集获取："><a href="#数据集获取：" class="headerlink" title="数据集获取："></a>数据集获取：</h3><p>本次作业使用两个预处理过的数据集：TinyStories 和 OpenWebText。两者都是大型纯文本文件。<br>校内学生可在实验室机器的 <code>/data</code> 目录下找到。校外人员/自学者可根据<strong>README.md中的命令下载。</strong></p>
<h3 id="降低规模的建议："><a href="#降低规模的建议：" class="headerlink" title="降低规模的建议："></a>降低规模的建议：</h3><p>后续课程会提供建议，解释如何在 CPU 或 MPS 环境下进行调整。</p>
<p><br> <br> <br></p>
<h1 id="2-BPE-Byte-Pair-Encoding-分词器"><a href="#2-BPE-Byte-Pair-Encoding-分词器" class="headerlink" title="2 BPE(Byte-Pair Encoding)分词器"></a>2 BPE(Byte-Pair Encoding)分词器</h1><h2 id="2-1-unicode标准"><a href="#2-1-unicode标准" class="headerlink" title="2.1 unicode标准"></a>2.1 unicode标准</h2><p>Unicode标准将字符映射到整数code points上。例如”s” 的code point 是115（或写作十六进制的U+0073），”牛”对应29275。</p>
<p>python中，有ord()和chr()函数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="built_in">ord</span>(<span class="string">&#x27;牛&#x27;</span>)</span><br><span class="line"><span class="number">29275</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">chr</span>(<span class="number">29275</span>)</span><br><span class="line"><span class="string">&#x27;牛&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Problem-unicode1-unicode介绍-（1分）"><a href="#Problem-unicode1-unicode介绍-（1分）" class="headerlink" title="Problem (unicode1): unicode介绍 （1分）"></a>Problem (unicode1): unicode介绍 （1分）</h3><p>(a)<code>chr(0)</code> 返回的是什么字符？<br><code>chr(0)</code> 返回的是 Unicode 编码为 0 的字符，即空字符（Null Character）。</p>
<p>(b) 它的字符串表示形式 (<code>__repr__()</code>) 与打印形式有何不同？<br>在 Python 中，其字符串表示形式（<code>__repr__()</code>）会显示为转义序列 <code>&#39;\x00&#39;</code>，而打印该字符（<code>print()</code>）时通常是不可见的，在某些终端里可能显示为空格。</p>
<p>(c)当该字符出现在文本中时会发生什么？<br>在 Python 字符串内部它可以正常存在并拼接，但在将其打印到终端或与底层 C 语言编写的程序交互时，它可能会被当做文本结束符而导致后面的内容被截断，或者干脆显示为一个空白区域。</p>
<p><br> <br></p>
<h2 id="2-2-unicode编码"><a href="#2-2-unicode编码" class="headerlink" title="2.2 unicode编码"></a>2.2 unicode编码</h2><p>主要介绍了UTF-8。编码和解码UTF-8的函数包括encode()和decode()，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_string = <span class="string">&quot;hello! こんにちは!&quot;</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>utf8_encoded = test_string.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded) </span><br><span class="line"><span class="string">b&#x27;hello! \xe3\x81\x93\xe3\x82\x93\xe3\x81\xab\xe3\x81\xa1\xe3\x81\xaf!&#x27;</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">type</span>(utf8_encoded))</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;bytes&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Get the byte values for the encoded string (integers from 0 to 255).</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(utf8_encoded) </span><br><span class="line">[<span class="number">104</span>, <span class="number">101</span>, <span class="number">108</span>, <span class="number">108</span>, <span class="number">111</span>, <span class="number">33</span>, <span class="number">32</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">130</span>, <span class="number">147</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">171</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">161</span>, <span class="number">227</span>, <span class="number">129</span>, <span class="number">175</span>, <span class="number">33</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># One byte does not necessarily correspond to one Unicode character! </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(test_string)) </span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">len</span>(utf8_encoded))</span><br><span class="line"><span class="number">23</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(utf8_encoded.decode(<span class="string">&quot;utf-8&quot;</span>)) <span class="number">1</span></span><br><span class="line">hello! こんにちは!</span><br></pre></td></tr></table></figure></p>
<p>我们将整数的codepoints转换成了一个byte序列，它们会更易于处理。例如，因为任何文本实质上都被转化为介于 $0-255$ 之间的整数序列，只要词表包含了这 256 个基础字节，就不需要担心训练和搭建的过程中词表外（OOV）词汇。</p>
<h3 id="课程文档外的补充：UTF-8字节序列对应规则"><a href="#课程文档外的补充：UTF-8字节序列对应规则" class="headerlink" title="课程文档外的补充：UTF-8字节序列对应规则"></a>课程文档外的补充：UTF-8字节序列对应规则</h3><p>字节数1：<br>对应Unicode编码范围：U+0000 至 U+007F 或 0~127 (7 bits)<br>对应utf-8字节序列：0xxxxxxx</p>
<p>字节数2：<br>对应Unicode编码范围：U+0080 至 U+07FF 或 128~2047 (11 bits)<br>对应utf-8字节序列：110xxxxx 10xxxxxx</p>
<p>字节数3：<br>对应Unicode编码范围：U+0800 至 U+FFFF 或 2048~65535 (16 bits)<br>对应utf-8字节序列：1110xxxx 10xxxxxx 10xxxxxx</p>
<p>字节数4：<br>对应Unicode编码范围：U+10000 至 U+10FFFF 或 65536~1114111 (21 bits)<br>应utf-8字节序列：11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</p>
<p>对应方式：先把unicode编码翻译成二进制，在填入对应字节序列的’x’处。（长度不够填则在左侧补0）。</p>
<h3 id="Problem-unicode2-unicode编码（3分）"><a href="#Problem-unicode2-unicode编码（3分）" class="headerlink" title="Problem (unicode2): unicode编码（3分）"></a>Problem (unicode2): unicode编码（3分）</h3><p>(a) 为什么 Tokenizer 训练更偏好 UTF-8而不是UTF-16, UTF-32？<br>UTF-8 是变长编码，能够将常用的 ASCII 字符保持为单字节，避免了 UTF-16/32 在处理英文或代码时产生大量冗余零字节。</p>
<p>(b) 错误的解码函数分析：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decode_utf8_bytes_to_str_wrong</span>(<span class="params">bytestring: <span class="built_in">bytes</span></span>): </span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([<span class="built_in">bytes</span>([b]).decode(<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">for</span> b <span class="keyword">in</span> bytestring]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>decode_utf8_bytes_to_str_wrong(<span class="string">&quot;hello&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)) <span class="string">&#x27;hello&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p>错误输入：如<code>b&#39;\xe4\xb8\xad&#39;</code> （汉字“中”的编码字节流）<br>因为UTF-8 中的中文字符或复杂符号是由 2 到 4 个字节共同组成的，单独解码其中任何一个字节都会因不符合 UTF-8 规范而报错（或产生乱码）。</p>
<p>(c)给出无法解码的字节：<br>如<code>b&#39;\xff\xff&#39;</code>。<br>原因：任何以 <code>11111xxx</code> 开头的字节都没有对应的 5 字节或更长的有效模板，无法解码为任何 Unicode 字符。</p>
<p><br> <br></p>
<h2 id="2-3-子词分词方案（subword-tokenization）"><a href="#2-3-子词分词方案（subword-tokenization）" class="headerlink" title="2.3 子词分词方案（subword tokenization）"></a>2.3 子词分词方案（subword tokenization）</h2><p>将词语拆成byte序列之后，仍然不能逐个byte拆分来看作为token：这会导致序列变得极长，训练的step变长，计算量也增大（如transformer计算复杂度与序列长度成正比）；同时更长的序列也导致信息密度被稀释，网络寻找token间的关系变得更困难。</p>
<p>而subword tokenization就是指代这样一个折中的方案。它选择用一个更大的词汇表(vocabulary)来trade-off更短的序列。比如说，如果‘the’经常出现，我们就可以给它单独分配一个条目(entry)，词汇表维度+1，但把3个token缩短成了1个。</p>
<p>为了做这样的工作，Sennrich et al. (2016) 提出使用byte-pair encoding (BPE; Gage, 1994)。作为一种subword tokenization，它简单地基于出现频率，将经常出现的byte pair合并(merge)成一个未被使用的索引。基于BPE的subword tokenizer被称为BPE tokenizer。</p>
<p><br> <br></p>
<h2 id="2-4-训练BPE分词器"><a href="#2-4-训练BPE分词器" class="headerlink" title="2.4 训练BPE分词器"></a>2.4 训练BPE分词器</h2><h3 id="1-初始化-vocabulary"><a href="#1-初始化-vocabulary" class="headerlink" title="1 初始化 vocabulary"></a>1 初始化 vocabulary</h3><p>初始化的vocabulary是从byte到整数ID的映射。因此，vocabulary的大小为256。</p>
<h3 id="2-预分词（Pre-tokenization"><a href="#2-预分词（Pre-tokenization" class="headerlink" title="2 预分词（Pre-tokenization)"></a>2 预分词（Pre-tokenization)</h3><p>理论上，有了词汇表我们就可以开始进行上述的合并（merge）工作了。然而，有两个关键的问题：<br>i 每次合并的时候，都需要从头到尾过一遍语料库。这在计算上是很昂贵的。<br>ii 直接合并会导致出现一些新的token，它们只有标点符号的区别（比如”dog.“和”dog!“），它们会拥有完全不同的ID，即使它们在语义上是完全相同的。  </p>
<p>为了解决上述问题，我们进行对语料库的预分词(pre-tokenize)，先把语料库切成单词。这是如何省下计算成本的？举个例子，当我们已经计数了’text’的出现次数（比如说10次），当我们需要计数’te’的出现次数时，就可以直接+=10，从而避免了每次遇到同一个单词时重复的计算。</p>
<p>早期分词方案包括直接通过空格进行切分（s.split(“”))，但显然这不能解决上述提到的问题2。</p>
<p>于是，我们将使用的时一个基于正则表达式(regex)的分词器 (used by GPT-2; Radford et al., 2019)，可以在github.com/openai/tiktoken/pull/234/files中找到。如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>PAT = <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>这里的五个部分分别处理：缩写后缀（’ll, ‘ve, ‘t , ‘s等）；连续字母（单词）；连续数字；连续标点符号；连续纯空格。</p>
<p>例子如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># requires `regex` package </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> regex <span class="keyword">as</span> re </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> re.findall(PAT, <span class="string">&quot;some text that i&#x27;ll pre-tokenize&quot;</span>) </span><br><span class="line">[<span class="string">&#x27;some&#x27;</span>, <span class="string">&#x27; text&#x27;</span>, <span class="string">&#x27; that&#x27;</span>, <span class="string">&#x27; i&#x27;</span>, <span class="string">&quot;&#x27;ll&quot;</span>, <span class="string">&#x27; pre&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;tokenize&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<p>实际使用中，用findall内存容易溢出，建议使用re.finditer这一迭代计算的函数。</p>
<h3 id="3-计算BPE合并-amp-处理特殊token"><a href="#3-计算BPE合并-amp-处理特殊token" class="headerlink" title="3 计算BPE合并&amp;处理特殊token"></a>3 计算BPE合并&amp;处理特殊token</h3><p>现在我们可以开始按照前述方法计算合并了。只需要注意两点：<br>i  不能跨越预分词边界合并。<code>[&#39;some&#39;, &#39;text&#39;]</code>，那么 <code>e</code>（来自 some）和 <code>t</code>（来自 text）永远不会被统计在一起。<br>ii 多个byte pair出现频率并列第一时，选择字典序最大（Lexicographically greater）的那一对。(“A”, “B”), (“A”, “C”), (“B”, “ZZ”), 和 (“BA”, “A”)频率相同时，在 ASCII 中 “BA” &gt; “B” &gt; “A”），因此 <code>max</code> 会选出 <code>(&#39;BA&#39;, &#39;A&#39;)</code>。<br>iii 有一些特殊token不能和其他合并，比如&lt;|endoftext|&gt;。它不应该被分成几个零碎的token，因此我们会给它安排一个固定的tokenID。  </p>
<h3 id="一个训练的具体例子："><a href="#一个训练的具体例子：" class="headerlink" title="一个训练的具体例子："></a>一个训练的具体例子：</h3><p>例如我们现在有如下corpus：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">low low low low </span><br><span class="line">low lower lower widest widest widest </span><br><span class="line">newest newest newest newest newest newest</span><br></pre></td></tr></table></figure><br>, 且vocabulary里已经放好了&lt;|endoftext|&gt;这一special token。</p>
<h4 id="1-初始化vocabulary："><a href="#1-初始化vocabulary：" class="headerlink" title="1 初始化vocabulary："></a>1 初始化vocabulary：</h4><p>一个special token和256个byte value。</p>
<h4 id="2-Pre-tokenization"><a href="#2-Pre-tokenization" class="headerlink" title="2 Pre-tokenization:"></a>2 Pre-tokenization:</h4><p>为了简化，我们仅使用用空格分隔，最终得到频率表：{low: 5, lower: 2, widest: 3, newest: 6}。为容易处理，将它写成dict[tuple[bytes], int]的格式，如{(l,o,w): 5 …}。</p>
<h4 id="3-合并："><a href="#3-合并：" class="headerlink" title="3 合并："></a>3 合并：</h4><p>数出上述例子中byte pair的出现频率：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;lo: <span class="number">7</span>, ow: <span class="number">7</span>, we: <span class="number">8</span>, er: <span class="number">2</span>, wi: <span class="number">3</span>, <span class="built_in">id</span>: <span class="number">3</span>, de: <span class="number">3</span>, es: <span class="number">9</span>, st: <span class="number">9</span>, ne: <span class="number">6</span>, ew: <span class="number">6</span>&#125;</span><br></pre></td></tr></table></figure><br>(‘es’) 和 (‘st’)并列，我们就取字典序更大的(‘st’)。<br>于是表格变为：{(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}.</p>
<p>如是循环，进行6次merge，可以得到新产生的vocabulary：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&#x27;s t&#x27;</span>, <span class="string">&#x27;e st&#x27;</span>, <span class="string">&#x27;o w&#x27;</span>, <span class="string">&#x27;l ow&#x27;</span>, <span class="string">&#x27;w est&#x27;</span>, <span class="string">&#x27;n e&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<p>于是新的词汇表变为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[&lt;|endoftext|&gt;, [..<span class="number">.256</span> BYTE CHARS], st, est, ow, low, west, ne]</span><br></pre></td></tr></table></figure></p>
<p>在此语境下，‘newest’ 将被分词为[ne, west]。</p>
<p><br> <br></p>
<h2 id="2-5-实验：训练BPE"><a href="#2-5-实验：训练BPE" class="headerlink" title="2.5 实验：训练BPE"></a>2.5 实验：训练BPE</h2><p>我们接下来再TinyStories数据集上训练一个BPE。你可以在Section1里找到它的下载方式。在开始之前，推荐你先大概看一眼里面都是什么内容。</p>
<h3 id="1-并行处理预分词"><a href="#1-并行处理预分词" class="headerlink" title="1 并行处理预分词"></a>1 并行处理预分词</h3><p>训练过程中，预分词会是一个主要的瓶颈。你可以使用内置库<code>multiprocessing</code>并行化你的代码。<br>你可以逐字使用以下链接中的入门代码来获取分块边界，然后使用这些边界将工作分配到各个进程：<br><a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py">https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py</a><br>阅读此代码，你会发现它实现的是确保每个分块边界都切割在special_token之前。</p>
<h3 id="2-预分词前，剔除特殊token不进行处理"><a href="#2-预分词前，剔除特殊token不进行处理" class="headerlink" title="2 预分词前，剔除特殊token不进行处理"></a>2 预分词前，剔除特殊token不进行处理</h3><p>用”|” .join(special_tokens)来re.split以移除&lt;|endoftext|&gt;</p>
<h3 id="3-优化合并"><a href="#3-优化合并" class="headerlink" title="3 优化合并"></a>3 优化合并</h3><p>naive BPE training（逐个合并，每次合并从头遍历一次）计算量太大。考虑到每次合并改变的计数只有这个合并前后的计数，我们可以动态处理合并，即每次合并只将前后的计数-1。如合并‘text’中的‘ex’，只需要将te和xt的计数-1，而不需要合并完之后再从头数一遍。</p>
<h3 id="Downscaling-提示"><a href="#Downscaling-提示" class="headerlink" title="Downscaling 提示"></a>Downscaling 提示</h3><p>1 cProfile和scalene等分析工具可以分析你的实现中的瓶颈，于是你可以专注于分析这些瓶颈。<br>2 与其直接现在TInyStories的training set上训练，你可以先将validation set作为‘调试数据(debug dataset)’训练。后者只有22K个文档，前者有2.12M个文档。</p>
<h3 id="Problem-train-bpe-BPE训练（15分）"><a href="#Problem-train-bpe-BPE训练（15分）" class="headerlink" title="Problem (train_bpe): BPE训练（15分）"></a>Problem (train_bpe): BPE训练（15分）</h3><p>1 编写你的tokenizer，然后在adapter.py中import，最后uv run pytest tests/test_train_bpe.py。<br>需要注意，如果出现类似” Extra items in the left set: b’\r\n\r’  “的报错，这是因为linux文本会被windows自动将\n更改为\r\n。这也很好解决，直接处理一步.replace(b”\r\n”, b”\n”)即可。    </p>
<p>笔者的个人实现见<br><a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a>  ，<br>本节答案对应截止在#—2.6 Encoding &amp; Decoding—#上方的内容。<br>对其的说明参考<a href="https://zian-2.github.io/2026/01/21/tokenizer.py/">https://zian-2.github.io/2026/01/21/tokenizer.py/</a>  </p>
<p>2 按照文章的建议，You should use profiling tools like <strong><code>cProfile</code></strong> or <strong><code>scalene</code></strong> to identify the bottlenecks in your implementation.  个人的实现，在主函数内部添加如下代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bpe</span>(<span class="params">input_path, vocab_size, special_tokens, num_chunks = <span class="number">1000</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> cProfile, pstats</span><br><span class="line">    profiler = cProfile.Profile()</span><br><span class="line">    profiler.enable() <span class="comment"># 开始分析</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"><span class="comment">#  你的完整的分词进程</span></span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"></span><br><span class="line">    profiler.disable() <span class="comment"># 结束分析</span></span><br><span class="line">    stats = pstats.Stats(profiler).sort_stats(<span class="string">&#x27;cumulative&#x27;</span>)</span><br><span class="line">    stats.print_stats(<span class="number">15</span>) <span class="comment"># 打印耗时最长的 15 个函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenizer</span><br></pre></td></tr></table></figure></p>
<p>添加分析之后运行时长增加是正常的。注意在表格中只需要关注tottime（函数内部的运行时长，不包括调用的外部函数）。</p>
<h3 id="Problem-train-bpe-tinystories-：TinyStories上的训练（2分）"><a href="#Problem-train-bpe-tinystories-：TinyStories上的训练（2分）" class="headerlink" title="Problem (train_bpe_tinystories)：TinyStories上的训练（2分）"></a>Problem (train_bpe_tinystories)：TinyStories上的训练（2分）</h3><p>(a) 在 TinyStories 数据集上训练一个字节级（byte-level）的 BPE 分词器，词表最大容量为 10,000。确保在词表中加入 <code>&lt;|endoftext|&gt;</code> 特殊标记。将生成的词表（vocabulary）和合并规则（merges）序列化保存到磁盘以便后续检查。训练耗时多少小时？消耗了多少内存？词表中最长的 Token 是什么？它合理吗？</p>
<p>资源要求： 耗时 ≤ 30 分钟（不使用 GPU），内存 ≤ 30GB RAM。<br>提示：你可以使用multiprocessing，可以使训练达到2分钟以内（这里应该指的是训练集TinyStoriesV2-GPT4-train.txt）</p>
<p>(b) 对你的代码进行性能分析（Profile）。分词器训练过程中哪个部分最耗时？</p>
<h4 id="Answer："><a href="#Answer：" class="headerlink" title="Answer："></a>Answer：</h4><p>你需要先把TinyStories接上你的分词器。记录内存消耗峰值，同时用cProfile分析，最后输出longest token，并将vocab和merges导入到你的磁盘里。参考实现如下（不包括profile）：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> .tokenizer <span class="keyword">import</span> train_bpe</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 路径配置</span></span><br><span class="line">    input_path = <span class="string">&quot;../data/TinyStoriesV2-GPT4-valid.txt&quot;</span></span><br><span class="line">    vocab_size = <span class="number">10000</span> </span><br><span class="line">    special_tokens = [<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>] </span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;开始训练&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用训练函数</span></span><br><span class="line">    tokenizer = train_bpe(</span><br><span class="line">        input_path=input_path,</span><br><span class="line">        vocab_size=vocab_size,</span><br><span class="line">        special_tokens=special_tokens,</span><br><span class="line">        num_chunks=<span class="number">1000</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n训练完成&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;词表大小: <span class="subst">&#123;<span class="built_in">len</span>(tokenizer.vocab)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;合并次数: <span class="subst">&#123;<span class="built_in">len</span>(tokenizer.merges)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    output_dir = <span class="string">&quot;run_bpe_train_on_tinystories_output&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">        os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line">    readable_vocab = &#123;<span class="built_in">int</span>(k): <span class="built_in">list</span>(v) <span class="keyword">for</span> k, v <span class="keyword">in</span> tokenizer.vocab.items()&#125;</span><br><span class="line">    vocab_path = os.path.join(output_dir, <span class="string">&quot;vocab.json&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(vocab_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(readable_vocab, f, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    merges_path = os.path.join(output_dir, <span class="string">&quot;merges.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(merges_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> p1, p2 <span class="keyword">in</span> tokenizer.merges:</span><br><span class="line">            <span class="comment"># 将 bytes 转换为逗号分隔的数字字符串</span></span><br><span class="line">            s1 = <span class="string">&quot;,&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">list</span>(p1)))</span><br><span class="line">            s2 = <span class="string">&quot;,&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">list</span>(p2)))</span><br><span class="line">            f.write(<span class="string">f&quot;<span class="subst">&#123;s1&#125;</span> <span class="subst">&#123;s2&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h4 id="Deliverable"><a href="#Deliverable" class="headerlink" title="Deliverable:"></a>Deliverable:</h4><p>(a)  笔者的实现参考：<a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a> 。<br>在TinyStoriesV2-GPT4-train.txt上训练在40秒以内，内存峰值0.8938 GB。<br>设备条件：28GB内存，Intel Core Ultra 7 255HX CPU。<br>longest token: ‘ accomplishment’。输出合理。</p>
<p>(b)Profile : 你可以以任何方式完成Profile。在TinyStoriesV2-GPT4-train.txt上的参考profile如下：<br><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">==================================================</span><br><span class="line">Profile (Top <span class="number">20</span> Functions):</span><br><span class="line">         <span class="number">15808949</span> <span class="function"><span class="keyword">function</span> <span class="title">calls</span> <span class="params">(15808073 primitive calls)</span> <span class="title">in</span> <span class="title">40</span>.<span class="title">449</span> <span class="title">seconds</span></span></span><br><span class="line"></span><br><span class="line">   Ordered by: internal time</span><br><span class="line">   List reduced from <span class="number">457</span> to <span class="number">20</span> due to restriction &lt;<span class="number">20</span>&gt;</span><br><span class="line"></span><br><span class="line">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)</span><br><span class="line">  <span class="number">463</span>/<span class="number">235</span>   <span class="number">20.998</span>    <span class="number">0.045</span>    <span class="number">6.500</span>    <span class="number">0.028</span> &#123;built<span class="operator">-in</span> method _winapi.WaitForMultipleObjects&#125;</span><br><span class="line">     <span class="number">9743</span>    <span class="number">7.911</span>    <span class="number">0.001</span>   <span class="number">14.576</span>    <span class="number">0.001</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">169</span>(merge_step)</span><br><span class="line">    <span class="number">23282</span>    <span class="number">2.899</span>    <span class="number">0.000</span>    <span class="number">2.951</span>    <span class="number">0.000</span> &#123;built<span class="operator">-in</span> method builtins.max&#125;</span><br><span class="line">   <span class="number">277780</span>    <span class="number">2.271</span>    <span class="number">0.000</span>    <span class="number">3.413</span>    <span class="number">0.000</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">115</span>(_replace_pair_and_pair_counts)</span><br><span class="line">       <span class="number">80</span>    <span class="number">1.486</span>    <span class="number">0.019</span>    <span class="number">1.486</span>    <span class="number">0.019</span> &#123;built<span class="operator">-in</span> method _pickle.loads&#125;</span><br><span class="line">     <span class="number">1002</span>    <span class="number">1.238</span>    <span class="number">0.001</span>    <span class="number">2.147</span>    <span class="number">0.002</span> D:\Program Files\Python313\Lib\collections\__init__.py:<span class="number">673</span>(update)</span><br><span class="line">  <span class="number">6434620</span>    <span class="number">1.028</span>    <span class="number">0.000</span>    <span class="number">1.028</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;get&#x27;</span> of <span class="string">&#x27;dict&#x27;</span> objects&#125;</span><br><span class="line">  <span class="number">2538958</span>    <span class="number">0.486</span>    <span class="number">0.000</span>    <span class="number">0.486</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;add&#x27;</span> of <span class="string">&#x27;set&#x27;</span> objects&#125;</span><br><span class="line">  <span class="number">1362761</span>    <span class="number">0.374</span>    <span class="number">0.000</span>    <span class="number">0.374</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;discard&#x27;</span> of <span class="string">&#x27;set&#x27;</span> objects&#125;</span><br><span class="line">  <span class="number">689</span>/<span class="number">189</span>    <span class="number">0.323</span>    <span class="number">0.000</span>   <span class="number">38.936</span>    <span class="number">0.206</span> &#123;built<span class="operator">-in</span> method _winapi.ReadFile&#125;</span><br><span class="line">        <span class="number">1</span>    <span class="number">0.217</span>    <span class="number">0.217</span>   <span class="number">40.447</span>   <span class="number">40.447</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">218</span>(train)</span><br><span class="line">  <span class="number">3008052</span>    <span class="number">0.190</span>    <span class="number">0.000</span>    <span class="number">0.190</span>    <span class="number">0.000</span> &#123;built<span class="operator">-in</span> method builtins.len&#125;</span><br><span class="line">   <span class="number">277795</span>    <span class="number">0.156</span>    <span class="number">0.000</span>    <span class="number">0.156</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;pop&#x27;</span> of <span class="string">&#x27;dict&#x27;</span> objects&#125;</span><br><span class="line">        <span class="number">1</span>    <span class="number">0.135</span>    <span class="number">0.135</span>    <span class="number">0.196</span>    <span class="number">0.196</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">149</span>(pair_count)</span><br><span class="line">  <span class="number">1370918</span>    <span class="number">0.133</span>    <span class="number">0.000</span>    <span class="number">0.133</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;append&#x27;</span> of <span class="string">&#x27;list&#x27;</span> objects&#125;</span><br><span class="line">       <span class="number">20</span>    <span class="number">0.123</span>    <span class="number">0.006</span>    <span class="number">0.123</span>    <span class="number">0.006</span> &#123;built<span class="operator">-in</span> method _winapi.CreateProcess&#125;</span><br><span class="line">       <span class="number">78</span>    <span class="number">0.092</span>    <span class="number">0.001</span>    <span class="number">0.374</span>    <span class="number">0.005</span> D:\Program Files\Python313\Lib\multiprocessing\connection.py:<span class="number">246</span>(recv)</span><br><span class="line">  <span class="number">794</span>/<span class="number">768</span>    <span class="number">0.051</span>    <span class="number">0.000</span>    <span class="number">0.128</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;GetOverlappedResult&#x27;</span> of <span class="string">&#x27;_winapi.Overlapped&#x27;</span> objects&#125;</span><br><span class="line">      <span class="number">157</span>    <span class="number">0.050</span>    <span class="number">0.000</span>    <span class="number">0.050</span>    <span class="number">0.000</span> &#123;method <span class="string">&#x27;write&#x27;</span> of <span class="string">&#x27;_io.BytesIO&#x27;</span> objects&#125;</span><br><span class="line">   <span class="number">270267</span>    <span class="number">0.047</span>    <span class="number">0.000</span>    <span class="number">0.048</span>    <span class="number">0.000</span> D:\cs336_assignments_and_notes\assignment1<span class="literal">-basics</span>\cs336_basics\tokenizer.py:<span class="number">178</span>(&lt;genexpr&gt;)</span><br><span class="line"></span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure></p>
<h3 id="Problem-train-bpe-expts-owt-OpenWebText上的训练（2分）"><a href="#Problem-train-bpe-expts-owt-OpenWebText上的训练（2分）" class="headerlink" title="Problem (train_bpe_expts_owt): OpenWebText上的训练（2分）"></a>Problem (train_bpe_expts_owt): OpenWebText上的训练（2分）</h3><p>(a)在OpenWebText上训练，使用vocab_size = 32000。把你得到的vocab和merges存入硬盘。词表中longest token是什么？合理吗？<br>资源要求： 耗时 ≤ 12小时（不使用 GPU），内存 ≤ 100GB RAM。<br>(b)  对比你分别在TinyStories和OpenWebText上得到的分词器。</p>
<h4 id="Answer"><a href="#Answer" class="headerlink" title="Answer:"></a>Answer:</h4><p>(a) uv run python -m cs336_basics.run_bpe_train_on_openwebtext。<br>longest token基本都是各种长横线或者下划线，你可以通过数据清洗试图避免这一点，但是owt里东西确实比较杂，笔者尝试了一些方法都不能清洗干净。  </p>
<p>(b) id小的词差不多，id大的词区别还是比较明显。owt的词明显更多简写，更不日常，更不口语化；Tiny就有些童话色彩。同样取10000附近的词举例：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OWT:</span><br><span class="line">&quot;9985&quot;: &quot;82&quot;,</span><br><span class="line">&quot;9986&quot;: &quot; signals&quot;,</span><br><span class="line">&quot;9987&quot;: &quot; oxy&quot;,</span><br><span class="line">&quot;9988&quot;: &quot; eager&quot;,</span><br><span class="line">&quot;9989&quot;: &quot;igg&quot;,</span><br><span class="line">&quot;9990&quot;: &quot;ERS&quot;,</span><br><span class="line">&quot;9991&quot;: &quot; unprecedented&quot;,</span><br><span class="line">&quot;9992&quot;: &quot; mood&quot;,</span><br><span class="line">&quot;9993&quot;: &quot; custody&quot;,</span><br><span class="line">&quot;9994&quot;: &quot; bankrupt&quot;,</span><br><span class="line">&quot;9995&quot;: &quot; asylum&quot;,</span><br><span class="line">&quot;9996&quot;: &quot; acknowledge&quot;,</span><br><span class="line">&quot;9997&quot;: &quot;reek&quot;,</span><br><span class="line">&quot;9998&quot;: &quot;endar&quot;,</span><br><span class="line">&quot;9999&quot;: &quot;books&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TinyStories:</span><br><span class="line">&quot;9985&quot;: &quot; Froggy&quot;, </span><br><span class="line">&quot;9986&quot;: &quot; wrapper&quot;, </span><br><span class="line">&quot;9987&quot;: &quot; Reddy&quot;, </span><br><span class="line">&quot;9988&quot;: &quot; Hops&quot;, </span><br><span class="line">&quot;9989&quot;: &quot; Crusty&quot;, </span><br><span class="line">&quot;9990&quot;: &quot; whiskers&quot;, </span><br><span class="line">&quot;9991&quot;: &quot; nicest&quot;, </span><br><span class="line">&quot;9992&quot;: &quot; improving&quot;, </span><br><span class="line">&quot;9993&quot;: &quot; booth&quot;, </span><br><span class="line">&quot;9994&quot;: &quot; Land&quot;, </span><br><span class="line">&quot;9995&quot;: &quot; Surrender&quot;, </span><br><span class="line">&quot;9996&quot;: &quot; Rocky&quot;, </span><br><span class="line">&quot;9997&quot;: &quot; meadows&quot;, </span><br><span class="line">&quot;9998&quot;: &quot; imaginary&quot;, </span><br><span class="line">&quot;9999&quot;: &quot; bold&quot;</span><br></pre></td></tr></table></figure></p>
<p>作为对比，再给出32000附近的OWT：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;31985&quot;: &quot; coated&quot;,</span><br><span class="line">&quot;31986&quot;: &quot; bland&quot;,</span><br><span class="line">&quot;31987&quot;: &quot; bending&quot;,</span><br><span class="line">&quot;31988&quot;: &quot; bamboo&quot;,</span><br><span class="line">&quot;31989&quot;: &quot; assurances&quot;,</span><br><span class="line">&quot;31990&quot;: &quot; ambassadors&quot;,</span><br><span class="line">&quot;31991&quot;: &quot; alum&quot;,</span><br><span class="line">&quot;31992&quot;: &quot; Yee&quot;,</span><br><span class="line">&quot;31993&quot;: &quot; Worse&quot;,</span><br><span class="line">&quot;31994&quot;: &quot; Ware&quot;,</span><br><span class="line">&quot;31995&quot;: &quot; Ves&quot;,</span><br><span class="line">&quot;31996&quot;: &quot; TED&quot;,</span><br><span class="line">&quot;31997&quot;: &quot; surveillance&quot;,</span><br><span class="line">&quot;31998&quot;: &quot; Sequ&quot;,</span><br><span class="line">&quot;31999&quot;: &quot; Schaefer&quot;</span><br></pre></td></tr></table></figure><br><br> <br></p>
<h2 id="2-6-BPE：编码和解码"><a href="#2-6-BPE：编码和解码" class="headerlink" title="2.6 BPE：编码和解码"></a>2.6 BPE：编码和解码</h2><p>根据已给的词汇表和merges实现任意文本和vocabulary里的token IDs的互相转换（encode and decode) 。  </p>
<h3 id="2-6-1-Encoding-文本"><a href="#2-6-1-Encoding-文本" class="headerlink" title="2.6.1 Encoding 文本"></a>2.6.1 Encoding 文本</h3><p>步骤：<br>1 预分词 ：这一步和训练的时候是一样的。<br>2 merge：拿出你的vocabulary和merges，按照merges创建时的顺序应用到预分词(pre-token)上。  </p>
<p>一个merge的具体例子：<br>假设输入是 <code>&#39;the cat ate&#39;</code>，<br>词表： <code>&#123;0: b&#39; &#39;, 1: b&#39;a&#39;, 2: b&#39;c&#39;, 3: b&#39;e&#39;, 4: b&#39;h&#39;, 5: b&#39;t&#39;, 6: b&#39;th&#39;, 7: b&#39; c&#39;, 8: b&#39; a&#39;, 9: b&#39;the&#39;, 10: b&#39; at&#39;&#125;</code><br>合并 (merges) 是： <code>[(b&#39;t&#39;, b&#39;h&#39;), (b&#39; &#39;, b&#39;c&#39;), (b&#39; &#39;, b&#39;a&#39;), (b&#39;th&#39;, b&#39;e&#39;), (b&#39; a&#39;, b&#39;t&#39;)]</code></p>
<p>首先预分词，拆成 <code>[&#39;the&#39;, &#39; cat&#39;, &#39; ate&#39;]</code>，然后对每个预分词应用合并。</p>
<p>第一个词 <code>&#39;the&#39;</code> 最初表示为 <code>[b&#39;t&#39;, b&#39;h&#39;, b&#39;e&#39;]</code>。查看合并列表，按顺序逐个应用可用的合并；第一个是 <code>(b&#39;t&#39;, b&#39;h&#39;)</code>，应用它，<code>[b&#39;t&#39;, b&#39;h&#39;, b&#39;e&#39;]</code>转换为 <code>[b&#39;th&#39;, b&#39;e&#39;]</code>。下一个可用的合并是 <code>(b&#39;th&#39;, b&#39;e&#39;)</code>，将 <code>[b&#39;th&#39;, b&#39;e&#39;]</code>转换为 <code>[b&#39;the&#39;]</code>。最后，再次查看合并列表，没有更多可应用的合并则结束。因此得到 ‘the’ 对应的整数序列为 <code>[9]</code>。<br>同样地：<code>&#39; cat&#39;</code> 在应用合并后表示为 <code>[b&#39; c&#39;, b&#39;a&#39;, b&#39;t&#39;]</code>，变为整数序列 <code>[7, 1, 5]</code>。<br> <code>&#39; ate&#39;</code> 合并后为 <code>[b&#39; at&#39;, b&#39;e&#39;]</code>，变为整数序列 <code>[10, 3]</code>。<br>因此，编码输入字符串的最终结果是 <code>[9, 7, 1, 5, 10, 3]</code>。</p>
<p>3 special_tokens： 你的分词器应当能够恰当地对special tokens进行处理。<br>4 内存方面的考量：文件文本常常不能直接塞进内存里，所以还是需要切成chunks再逐个处理，这样无论文件多大，所需的内存才是恒定的，而不是随着文件大小增长所需的内存线性增大。当然，需要保证切开的时候不能把一个token切开。  </p>
<h3 id="2-6-2-Decoding-文本"><a href="#2-6-2-Decoding-文本" class="headerlink" title="2.6.2 Decoding 文本"></a>2.6.2 Decoding 文本</h3><p>Decoding，显然地，就是直接查找ID对应的token然后把它们连接起来，最后再把这些bytes转换成strings。<br>然而，需要注意的是，不是所有输入的ID都能被转换成合法的Unicode strings，所以我们需要把这些畸形的(malformed)bytes替换成官方的unicode替换字符 <code>U+FFFD</code>（参考：<code>https://en.wikipedia.org/wiki/Specials_(Unicode_block)#Replacement_character</code>）。在bytes.decode函数中，参数errors控制了unicode decoding会怎样处理这些错误的bytes。比如说，用<code>errors = &#39;replace&#39;</code>可以自动地把那些畸形的数据替换成替换标记。  </p>
<h3 id="Problem-tokenizer-实现分词器（15分）"><a href="#Problem-tokenizer-实现分词器（15分）" class="headerlink" title="Problem (tokenizer): 实现分词器（15分）"></a>Problem (tokenizer): 实现分词器（15分）</h3><p>要求：实现一个 Tokenizer class，给它输出一个vocabulary和一个merges列表，可以把text文本encode成整数ID或把整数IDdecode成text文本。同时它还应该支持用户提供的特殊token（如果还不在词表里，就把他们添加到词表里。）课程推荐你的把接口写成如下形式：</p>
<p><code>def __init__(self, vocab, merges, special_tokens=None)</code> 从给定的词表、合并列表和（可选的）特殊 Token 列表构造分词器。接受参数包括：<br><code>vocab: dict[int, bytes]</code><br><code>merges: list[tuple[bytes, bytes]]</code><br><code>special_tokens: list[str]  |  None = None</code>  </p>
<p><code>def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)</code> 一个类方法。从序列化的vocab文件和merges文件（格式与 BPE 训练代码输出的格式相同）构造并返回一个 <code>Tokenizer</code> 实例。参数：<br><code>vocab_filepath: str</code><br><code>merges_ffilepath:  str</code><br><code>special_tokens:  list[str]  |  None = None</code>  </p>
<p><code>def encode(self, text: str) -&gt; list[int]</code>把输入的text文本转换成一列token IDs。  </p>
<p><code>def encode_iterable(self, iterable: Iterable[str]) -&gt; Iterator[int]</code>  给定一个字符串迭代器（比如一个python文件句柄），返回一个有延迟的(lazily)token ID生成器。这是为了那些我们不能直接加载进内存的大型文件准备的。  </p>
<p><code>def decode(self, ids: list[int]) -&gt; str</code> 把token IDs解码成text文本。  </p>
<p>同样地，写好了之后先接入<code>adapters.get_tokenizer</code>，然后<code>uv run pytest tests/test_tokenizer.py</code>；你需要通过所有测试。  </p>
<h4 id="Answer：-1"><a href="#Answer：-1" class="headerlink" title="Answer："></a>Answer：</h4><p>见<br><a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a>  ，<br>本节答案对应截止在# —2.6 Encoding &amp; Decoding—#下方的内容。    </p>
<p>windows上测试依旧出现问题。最好还是在linux上跑吧……<br>通过测试：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">============================= test session starts ==============================</span><br><span class="line">platform linux -- Python <span class="number">3.13</span><span class="number">.3</span>, pytest-<span class="number">8.4</span><span class="number">.1</span>, pluggy-<span class="number">1.6</span><span class="number">.0</span></span><br><span class="line">rootdir: /home/wangzian/Desktop/cs336_assignments_and_notes/assignment1-basics</span><br><span class="line">configfile: pyproject.toml</span><br><span class="line">plugins: jaxtyping-<span class="number">0.3</span><span class="number">.2</span></span><br><span class="line">collected <span class="number">25</span> items                                                             </span><br><span class="line"></span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_empty PASSED</span><br><span class="line">tests/test_tokenizer.py::test_empty_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_single_character PASSED</span><br><span class="line">tests/test_tokenizer.py::test_single_character_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_single_unicode_character PASSED</span><br><span class="line">tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_ascii_string PASSED</span><br><span class="line">tests/test_tokenizer.py::test_ascii_string_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_unicode_string PASSED</span><br><span class="line">tests/test_tokenizer.py::test_unicode_string_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens PASSED</span><br><span class="line">tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_overlapping_special_tokens PASSED</span><br><span class="line">tests/test_tokenizer.py::test_address_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_address_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_german_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_german_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_tinystories_sample_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_tinystories_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_special_token_trailing_newlines PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_iterable_memory_usage PASSED</span><br><span class="line">tests/test_tokenizer.py::test_encode_memory_usage XFAIL (Tokenizer.e...)</span><br><span class="line"></span><br><span class="line">======================== <span class="number">24</span> passed, <span class="number">1</span> xfailed <span class="keyword">in</span> <span class="number">5.99</span>s =========================</span><br></pre></td></tr></table></figure></p>
<p>linux虚拟机下测试时间5.99s。只要逻辑不要太离谱（比如直接遍历merge）基本时间上不会有太大问题。</p>
<h3 id="2-7-实验"><a href="#2-7-实验" class="headerlink" title="2.7 实验"></a>2.7 实验</h3><p>(a) 从 TinyStories 和 OpenWebText 中各随机抽取 10 份文档。使用你之前训练好的 TinyStories 分词器（词表大小 10K）和 OpenWebText 分词器（词表大小 32K），将这些抽样的文档编码为整数 ID。每个分词器的压缩比（字节/Token）是多少？</p>
<p>(b) 如果你用 TinyStories 的分词器去对 OpenWebText 的样本进行分词，会发生什么？比较其压缩比并（或）从定性角度描述发生的现象。</p>
<p>(c) 估算你的分词器的吞吐量（throughput)（例如：bytes/秒）。以此速度，分词整个Pile数据集（825GB 文本）需要多长时间？</p>
<p>(d) 使用你的 TinyStories 和 OpenWebText 分词器，将各自对应的训练集和开发集编码为整数 Token ID 序列。我们稍后将使用这些数据来训练语言模型。我们建议将这些 Token ID 序列序列化为 uint16 数据类型的 NumPy 数组。为什么选择 uint16 是一个合适的选择？</p>
<h4 id="Answer：-2"><a href="#Answer：-2" class="headerlink" title="Answer："></a>Answer：</h4><p>(a) Tiny 在 4.11左右， owt在4.42左右。可见owt三倍于Tiny的词表尺寸还是可以客观地提高压缩率。<br>(b) 压缩率在3.3左右。可见tiny的词表相对于owt的内容还是有比较大局限。<br>(c) 在Tiny_valid上验证，吞吐量: 5.5023 MB/s。预估分词 Pile (825GB): 42.65 小时。<br>(d)  1 无符号这一点和tokenID是相同的   2 数值范围到65536，比较适配词表大小。</p>
]]></content>
      <categories>
        <category>cs336</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>cs336</tag>
      </tags>
  </entry>
  <entry>
    <title>tokenizer.py</title>
    <url>/2026/01/27/tokenizer.py/</url>
    <content><![CDATA[<p>本文包含对Problem (train_bpe): BPE Tokenizer Training一问的解答及说明。</p>
<p>完整代码见<a href="https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py">https://github.com/Zian-2/cs336_assignments_and_notes/blob/main/assignment1-basics/cs336_basics/tokenizer.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">from</span> .pretokenization_example <span class="keyword">import</span> find_chunk_boundaries</span><br></pre></td></tr></table></figure>
<p>Counter : 一种特殊的数据类型，本质上是字典，下面你会看到它在计数层面上会有一些方便。<br>regex：引入正则表达式<br>multiprocessing: 用于并行处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预编译正则表达式</span></span><br><span class="line">PAT = re.<span class="built_in">compile</span>(<span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pre_tokenization</span>(<span class="params">chunk,special_tokens</span>)-&gt;<span class="built_in">list</span>[<span class="built_in">str</span>]:</span><br><span class="line">        <span class="comment"># 预分词：先从special_token切开，再过一遍正则切成单词</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="comment"># 切割special_tokens</span></span><br><span class="line">        special_pat = <span class="string">&quot;(&quot;</span> + <span class="string">&quot;|&quot;</span>.join(re.escape(t) <span class="keyword">for</span> t <span class="keyword">in</span> special_tokens) + <span class="string">&quot;)&quot;</span></span><br><span class="line">        parts = re.split(special_pat, chunk)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> part <span class="keyword">in</span> parts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> part:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> part <span class="keyword">in</span> special_tokens:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 预分词</span></span><br><span class="line">            matches = PAT.finditer(part)</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> matches:</span><br><span class="line">                result.append(<span class="string">f&quot;<span class="subst">&#123;m.group()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>五个部分分别处理：缩写后缀（’ll, ‘ve, ‘t , ‘s等）；连续字母（单词）；连续数字；连续标点符号；连续纯空格。<br>正则表达式的编译是比较昂贵的，放在外面从而不用每次都编译一遍。<br><span id="more"></span><br>pre_tokenization：输入分块了的chunk(需要先在下方process_chunk中转成str格式，不能是bytes流)； 输出分好词的列表，如[‘some’, ‘ text’, ‘ that’, ‘ i’, “‘ll”, ‘ pre’, ‘-‘, ‘tokenize’]。</p>
<p>special_pat一行：用于切开从special_token，将”textA special_token textB” 切成”textA“，”special_token“，”textB”。</p>
<p>matches = PAT.finditer()的使用：matches是迭代器，需要用m.group来调用正则出来的单词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_chunk</span>(<span class="params">input_path, </span></span><br><span class="line"><span class="params">                    start,</span></span><br><span class="line"><span class="params">                    end,</span></span><br><span class="line"><span class="params">                    special_tokens</span>)-&gt; <span class="built_in">dict</span>[<span class="built_in">tuple</span>[<span class="built_in">int</span>, ...], <span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 每个核的并行任务：(预分词)，encode，然后计数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(input_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.seek(start)</span><br><span class="line"></span><br><span class="line">            raw_data = f.read(end - start).replace(<span class="string">b&quot;\r\n&quot;</span>, <span class="string">b&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">            chunk = raw_data.decode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 预分词</span></span><br><span class="line">            chunk_split = pre_tokenization(chunk, special_tokens)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># encode每个单词并计数</span></span><br><span class="line">            word_counts = Counter()</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> chunk_split:</span><br><span class="line">                word_b = <span class="built_in">tuple</span>(word.encode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;ignore&quot;</span>))</span><br><span class="line">                word_counts[word_b] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">dict</span>(word_counts) </span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<p>process_chunk： 1 输入文本中任意的start和end位置，2 为了避免windows自动将\n更改为\r\n的问题，替换\r\n  3 将其解码为str  4 调用前面的预分词(pre_tokenization0)切成单词  5 将单词编码回bytes流  6 对单词计数。7 返回一个包含了单词计数的Counter类型。注意这里的单词都是bytes形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BPEtokenizer</span>:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    输入input_path(包含training data), vocab_size(int), special_tokens(list[str]).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回vocab(dict[int, bytes], int为原ID, bytes为token bytes), </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    和merges(list[tuple[bytes, bytes]],记录所有被合并的bytes). </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab=<span class="literal">None</span>, merges=<span class="literal">None</span>, special_tokens=<span class="literal">None</span>, pair_counts = Counter(<span class="params"></span>)</span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab = vocab <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> &#123;i: <span class="built_in">bytes</span>([i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>)&#125;</span><br><span class="line">        <span class="variable language_">self</span>.merges = merges <span class="keyword">if</span> merges <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> []</span><br><span class="line">        <span class="variable language_">self</span>.special_tokens = special_tokens <span class="keyword">if</span> special_tokens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> []</span><br><span class="line">        <span class="variable language_">self</span>.pair_counts = Counter()</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>开始计数，并开始创建题目要求的vocab, merges。为此创建一个类并初始化参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">counting_init</span>(<span class="params">self, input_path, </span></span><br><span class="line"><span class="params">          special_tokens, num_chunks = <span class="number">1000</span></span></span><br><span class="line"><span class="params">          </span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">tuple</span>[<span class="built_in">int</span>, ...], <span class="built_in">int</span>]:</span><br><span class="line">    <span class="comment"># 初始化：第一次计数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按边界切割</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(input_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        boundaries = find_chunk_boundaries(</span><br><span class="line">            f, </span><br><span class="line">            desired_num_chunks = num_chunks, </span><br><span class="line">            split_special_token = special_tokens[<span class="number">0</span>].encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        )        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 并行处理</span></span><br><span class="line">    num_cpu = multiprocessing.cpu_count()</span><br><span class="line">    tasks = []</span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> <span class="built_in">zip</span>(boundaries[:-<span class="number">1</span>], boundaries[<span class="number">1</span>:]):</span><br><span class="line">        tasks.append((input_path, start, end,special_tokens))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> multiprocessing.Pool(processes = num_cpu) <span class="keyword">as</span> pool:</span><br><span class="line">        results = pool.starmap(process_chunk, tasks)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成dict[bytes:int]</span></span><br><span class="line">    final_counts = Counter()</span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> results:</span><br><span class="line">        final_counts.update(res)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(final_counts)</span><br></pre></td></tr></table></figure>
<p>self.counting_init: 输入input_path, special_tokens, num_chunks（为了并行处理将原文本切成的大致块数），利用之前的process_chunk进行单词计数，然后合并所有结果。过程包括：</p>
<p>1 切割：按照课程提供的find_chunk_boundaries及其接口切割原文本。详情见<a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py">https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py</a>  。</p>
<p>2 并行：利用multiprocessing , 主要函数：<br>i multiprocessing.cpu_count()，输出你的cpu的核数。<br>ii  with multiprocessing.Pool(processes = num_cpu) as pool  :  创建并行池，processes = 并行个数。<br>iii  results = pool.starmap(process_chunk, tasks)： 第一个形参是你要并行使用的函数， 第二个tasks是一个列表，每个元素是一个tuple, 内含你要传入前面那个函数的所有形参。</p>
<p>3 将Counter转回dict。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_replace_pair_and_pair_counts</span>(<span class="params">self, word, count, pair, new_id</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新单词</span></span><br><span class="line">    new_word = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    has_pair = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(word):</span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(word)-<span class="number">1</span> <span class="keyword">and</span> word[i] == pair[<span class="number">0</span>] <span class="keyword">and</span> word[i+<span class="number">1</span>] == pair[<span class="number">1</span>]:</span><br><span class="line">            has_pair = <span class="literal">True</span></span><br><span class="line">            new_word.append(new_id)</span><br><span class="line">            i += <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_word.append(word[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 动态更新pair计数</span></span><br><span class="line">    <span class="keyword">if</span> has_pair == <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">zip</span>(word[:-<span class="number">1</span>], word[<span class="number">1</span>:]):</span><br><span class="line">            <span class="variable language_">self</span>.pair_counts[p] -= count</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">zip</span>(new_word[:-<span class="number">1</span>], new_word[<span class="number">1</span>:]):</span><br><span class="line">            <span class="variable language_">self</span>.pair_counts[p] += count</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">tuple</span>(new_word)</span><br></pre></td></tr></table></figure>
<p> _ replace_pair_and_pair_counts：输入一个单词（bytes形式），它对应的计数，要合并的pair，以及给合并后的新词分配的new_id 。输出新的单词，并在这个过程中更新self.pair_counts。<br> 容易理解，过程为先创建单词，再更新计数。注意此处更新应当只对有所更新的单词进行重新计数，否则pytest中的test_train_bpe_speed有1.5秒的速度要求将难以达成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pair_count</span>(<span class="params">self, word_counts</span>):</span><br><span class="line">    <span class="comment"># 第一次pair计数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对pair计数（第一次）</span></span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> word_counts.items():</span><br><span class="line">        word_byte_pair = <span class="built_in">zip</span>(word[:-<span class="number">1</span>], word[<span class="number">1</span>:])</span><br><span class="line">        <span class="comment"># 逐个地 += count</span></span><br><span class="line">        <span class="keyword">for</span> pair <span class="keyword">in</span> word_byte_pair:</span><br><span class="line">            <span class="variable language_">self</span>.pair_counts[pair] += count</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们已经有了对单词的计数，开始自然地对pair计数。<br>1 注意zip()的语法：输入两个长度相同的tuple，两两组合。<br>2 注意这里的计数方式符合我们的预分词目的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_step</span>(<span class="params">self, word_counts, new_id</span>):</span><br><span class="line">    <span class="comment"># merge迭代</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 找出应合并的pair</span></span><br><span class="line">    best_pair = <span class="built_in">max</span>(</span><br><span class="line">        <span class="variable language_">self</span>.pair_counts.items(), </span><br><span class="line">        key=<span class="keyword">lambda</span> item: (</span><br><span class="line">            item[<span class="number">1</span>],                                       </span><br><span class="line">            (<span class="variable language_">self</span>.vocab[item[<span class="number">0</span>][<span class="number">0</span>]], <span class="variable language_">self</span>.vocab[item[<span class="number">0</span>][<span class="number">1</span>]]) <span class="comment"># 合并后的实际 bytes 内容</span></span><br><span class="line">        )</span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    new_word_counts = &#123;&#125;</span><br><span class="line">    <span class="comment"># 合并pair并更新对单词和pair的计数</span></span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> word_counts.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word) &gt; <span class="number">1</span> <span class="keyword">and</span> (best_pair[<span class="number">0</span>] <span class="keyword">in</span> word <span class="keyword">and</span> best_pair[<span class="number">1</span>] <span class="keyword">in</span> word):</span><br><span class="line">            word = <span class="variable language_">self</span>._replace_pair_and_pair_counts(word, count, best_pair, new_id)</span><br><span class="line">        new_word_counts[word] = new_word_counts.get(word, <span class="number">0</span>) + count</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.vocab[new_id] = <span class="variable language_">self</span>.vocab[best_pair[<span class="number">0</span>]] + <span class="variable language_">self</span>.vocab[best_pair[<span class="number">1</span>]] </span><br><span class="line">    <span class="variable language_">self</span>.merges.append(( <span class="variable language_">self</span>.vocab[best_pair[<span class="number">0</span>]], <span class="variable language_">self</span>.vocab[best_pair[<span class="number">1</span>]] ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_word_counts</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>merge_step包含一步的step。输入：word_counts, new_id，返回new_word_counts，并在这个过程中更新self.vocab, self.merges 。  过程包括：<br>1 找出频率最高的pair<br>2 通过_replace_pair_and_pair_counts的更新，合并这些更新，产生新的单词计数new_word_counts传给下一步迭代。更新对pair的计数。<br>3 更新self.vocab和self.merges。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, input_path, vocab_size, </span></span><br><span class="line"><span class="params">          special_tokens, num_chunks = <span class="number">1000</span>, new_id = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">          </span>):</span><br><span class="line">    <span class="comment"># 迭代 merge_step</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单词计数</span></span><br><span class="line">    word_counts = <span class="variable language_">self</span>.counting_init(input_path, special_tokens, num_chunks = <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在最前面加上special_tokens </span></span><br><span class="line">    actual_merge_steps = vocab_size - <span class="number">256</span> - <span class="built_in">len</span>(special_tokens)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数pair（第一次）</span></span><br><span class="line">    <span class="variable language_">self</span>.pair_count(word_counts)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 迭代merge</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(actual_merge_steps):</span><br><span class="line">        new_id = <span class="number">256</span> + i</span><br><span class="line">        word_counts = <span class="variable language_">self</span>.merge_step(word_counts, new_id)</span><br><span class="line"></span><br><span class="line">    offset = <span class="built_in">len</span>(special_tokens)</span><br><span class="line">    new_vocab = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, st <span class="keyword">in</span> <span class="built_in">enumerate</span>(special_tokens):</span><br><span class="line">        new_vocab[i] = st.encode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> old_id, b <span class="keyword">in</span> <span class="variable language_">self</span>.vocab.items():</span><br><span class="line">        new_vocab[old_id + offset] = b</span><br><span class="line">        </span><br><span class="line">    <span class="variable language_">self</span>.vocab = new_vocab</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.vocab, <span class="variable language_">self</span>.merges</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>train：合并上述过程。包括：<br>1 先进行第一步单词计数counting_init<br>2 进行第一步pair计数pair_count<br>3 迭代刚才的merge_step。  得到完整的vocab和merges<br>4 在最终的vocab开头给special_tokens分配固定的id。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bpe</span>(<span class="params">input_path, vocab_size, special_tokens, num_chunks = <span class="number">1000</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> cProfile, pstats</span><br><span class="line">    profiler = cProfile.Profile()</span><br><span class="line">    profiler.enable() <span class="comment"># 开始分析</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    tokenizer = BPEtokenizer()</span><br><span class="line">    tokenizer.train(input_path, vocab_size, </span><br><span class="line">              special_tokens, num_chunks = <span class="number">1000</span>, new_id = <span class="literal">None</span>, </span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    profiler.disable() <span class="comment"># 结束分析</span></span><br><span class="line">    stats = pstats.Stats(profiler).sort_stats(<span class="string">&#x27;cumulative&#x27;</span>)</span><br><span class="line">    stats.print_stats(<span class="number">15</span>) <span class="comment"># 打印耗时最长的 15 个函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenizer</span><br></pre></td></tr></table></figure>
<p>train_bpe: 在类外面。用于创建新实例，和adapter.py相接。<br>1 创建一个新的tokenizer实例，进行训练。<br>2 按照课程的要求，使用cProfile进行分析。</p>
]]></content>
      <categories>
        <category>cs336</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>cs336</tag>
        <tag>code</tag>
      </tags>
  </entry>
</search>
